{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f362a013-934f-4d5b-83b1-28ccd75ff21e",
   "metadata": {},
   "source": [
    "# data ingestion pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c7e7ae6-a62b-4dbe-900a-a5cb1124b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2mfaiss-cpu==1.8.0.post1                                                        \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m7 packages\u001b[0m \u001b[2min 223ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 43ms\u001b[0m\u001b[0m                                               \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.92ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrank-bm25\u001b[0m\u001b[2m==0.2.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install python-docx faiss-cpu rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a1589-41bf-4b57-8cf1-934fcae6fb50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a418d34c-0e6d-4817-a98b-63adc0b82a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üì¶ Data Ingestion Pipeline ‚Äî Starter Notebook\n",
    "# Author: Manodeep Ray\n",
    "# Date: 2025-10-14\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from docx import Document\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Define folder structure\n",
    "# ------------------------------------------------------------\n",
    "BASE_DIR = Path.cwd()\n",
    "UPLOADED_DIR = BASE_DIR / \"uploaded\"\n",
    "DATA_WAREHOUSE_DIR = BASE_DIR / \"database\" / \"data_warehouse\"\n",
    "LOG_DIR = BASE_DIR / \"database\" / \"logs\"\n",
    "\n",
    "# Create directories if not exist\n",
    "for path in [UPLOADED_DIR, DATA_WAREHOUSE_DIR, LOG_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ Define file tracking files\n",
    "# ------------------------------------------------------------\n",
    "STATUS_FILE = LOG_DIR / \"file_status.json\"\n",
    "PROCESSED_CSV = LOG_DIR / \"processed_files.csv\"\n",
    "PROCESS_LOG = LOG_DIR / \"processing_log.txt\"\n",
    "\n",
    "# Initialize files if not exist\n",
    "if not STATUS_FILE.exists():\n",
    "    with open(STATUS_FILE, \"w\") as f:\n",
    "        json.dump({}, f, indent=4)\n",
    "\n",
    "if not PROCESSED_CSV.exists():\n",
    "    pd.DataFrame(columns=[\"file_name\", \"status\", \"timestamp\", \"hash\"]).to_csv(PROCESSED_CSV, index=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Helper Functions\n",
    "# ------------------------------------------------------------\n",
    "def compute_file_hash(file_path):\n",
    "    \"\"\"Compute SHA256 hash of a file.\"\"\"\n",
    "    sha256 = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256.update(chunk)\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "def log_message(message):\n",
    "    \"\"\"Log messages with timestamp.\"\"\"\n",
    "    timestamp = datetime.utcnow().isoformat()\n",
    "    with open(PROCESS_LOG, \"a\") as log:\n",
    "        log.write(f\"[{timestamp}] {message}\\n\")\n",
    "    print(message)\n",
    "\n",
    "def load_status():\n",
    "    \"\"\"Load status JSON.\"\"\"\n",
    "    with open(STATUS_FILE, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_status(status_data):\n",
    "    \"\"\"Save updated status JSON.\"\"\"\n",
    "    with open(STATUS_FILE, \"w\") as f:\n",
    "        json.dump(status_data, f, indent=4)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ File Reading (Simple Preview Functionality)\n",
    "# ------------------------------------------------------------\n",
    "def read_file(file_path):\n",
    "    \"\"\"Read text from TXT, DOCX, or PDF.\"\"\"\n",
    "    ext = file_path.suffix.lower()\n",
    "    try:\n",
    "        if ext == \".txt\":\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return f.read()\n",
    "        elif ext == \".docx\":\n",
    "            doc = Document(file_path)\n",
    "            return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "        elif ext == \".pdf\":\n",
    "            reader = PdfReader(file_path)\n",
    "            return \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        log_message(f\"‚ùå Error reading file {file_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5Ô∏è‚É£ Main Ingestion Function\n",
    "# ------------------------------------------------------------\n",
    "def ingest_files():\n",
    "    status_data = load_status()\n",
    "\n",
    "    for file_path in UPLOADED_DIR.glob(\"*.*\"):\n",
    "        file_name = file_path.name\n",
    "        file_hash = compute_file_hash(file_path)\n",
    "        timestamp = datetime.utcnow().isoformat()\n",
    "\n",
    "        # Skip if already processed and hash unchanged\n",
    "        if file_name in status_data and status_data[file_name][\"hash\"] == file_hash:\n",
    "            log_message(f\"‚ö†Ô∏è File {file_name} already processed ‚Äî skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Try reading and moving file\n",
    "        text_data = read_file(file_path)\n",
    "        if text_data is None:\n",
    "            status_data[file_name] = {\"status\": \"failed\", \"timestamp\": timestamp, \"hash\": file_hash}\n",
    "            log_message(f\"‚ùå Failed to process {file_name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Move to data warehouse\n",
    "            dest_path = DATA_WAREHOUSE_DIR / file_name\n",
    "            shutil.move(str(file_path), str(dest_path))\n",
    "\n",
    "            # Update status\n",
    "            status_data[file_name] = {\"status\": \"pending\", \"timestamp\": timestamp, \"hash\": file_hash}\n",
    "            log_message(f\"‚úÖ Processed and moved {file_name} to data warehouse.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            status_data[file_name] = {\"status\": \"failed\", \"timestamp\": timestamp, \"hash\": file_hash}\n",
    "            log_message(f\"‚ùå Error moving file {file_name}: {e}\")\n",
    "\n",
    "    # Save status JSON\n",
    "    save_status(status_data)\n",
    "\n",
    "    # Update CSV\n",
    "    df = pd.DataFrame([\n",
    "        {\"file_name\": k, \"status\": v[\"status\"], \"timestamp\": v[\"timestamp\"], \"hash\": v[\"hash\"]}\n",
    "        for k, v in status_data.items()\n",
    "    ])\n",
    "    df.to_csv(PROCESSED_CSV, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1bad5d1-d982-4e2f-bd89-d8913eb46357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2mpython-docx==1.1.2                                                            \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m6 packages\u001b[0m \u001b[2min 127ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 9ms\u001b[0m\u001b[0mst1                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfaiss-cpu\u001b[0m\u001b[2m==1.8.0.post1\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36882d7b-11d9-4ed1-b7fd-ea79debf2768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed and moved NoteGPT_Justice_ What'sTheRightThingToDo_Episode01_THE MORAL SIDE OF MURDER_.txt to data warehouse.\n",
      "‚úÖ Processed and moved NoteGPT_Last Lecture Series_ How to Live your Life at Full Power ‚Äî Graham Weaver.txt to data warehouse.\n",
      "‚úÖ Processed and moved ml_intro.txt to data warehouse.\n",
      "‚úÖ Processed and moved NoteGPT_Justice_ What'sTheRightThingToDo_Episode01_THE MORAL SIDE OF MURDER_ (1).txt to data warehouse.\n",
      "‚úÖ Processed and moved NoteGPT_Optimal Protocols for Studying & Learning.txt to data warehouse.\n",
      "‚úÖ Processed and moved Hausman_PhilosophyOfEconomicsAnthology.pdf to data warehouse.\n",
      "‚úÖ Processed and moved NoteGPT_The Trillion Dollar Equation.txt to data warehouse.\n",
      "‚úÖ Processed and moved NoteGPT_Take a Seat in the Harvard MBA Case Classroom.txt to data warehouse.\n",
      "‚úÖ Processed and moved 6372215474167643589th hsitory FILE.pdf to data warehouse.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6Ô∏è‚É£ Run Ingestion\n",
    "# ------------------------------------------------------------\n",
    "ingest_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ddf52f31-88bc-4beb-9d1e-b1d3ac987a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üß© Cleaning + Chunking + Metadata Tracking (RAG-Ready)\n",
    "# Author: Manodeep Ray\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üß± Directory Setup\n",
    "# ------------------------------------------------------------\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_WAREHOUSE_DIR = BASE_DIR / \"database\" / \"data_warehouse\"\n",
    "PROCESSED_DIR = BASE_DIR / \"database\" / \"processed\"\n",
    "CLEANED_DIR = PROCESSED_DIR / \"cleaned\"\n",
    "CHUNK_DIR = PROCESSED_DIR / \"chunks\"\n",
    "LOG_DIR = BASE_DIR /\"database\" / \"logs\"\n",
    "\n",
    "for path in [CLEANED_DIR, CHUNK_DIR, LOG_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNK_STATUS_FILE = LOG_DIR / \"chunk_status.json\"\n",
    "if not CHUNK_STATUS_FILE.exists():\n",
    "    with open(CHUNK_STATUS_FILE, \"w\") as f:\n",
    "        json.dump({}, f, indent=4)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üßπ Cleaning Function\n",
    "# ------------------------------------------------------------\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and standardize extracted text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()                  # Remove non-ASCII\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"[URL]\", text)        # Replace URLs\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,;:?!()\\[\\]'\\s-]\", \" \", text)      # Remove unwanted chars\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()                        # Normalize whitespace\n",
    "    text = text.lower()                                             # Normalize case\n",
    "    return text\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üìÑ File Reading with Page Extraction (for PDFs)\n",
    "# ------------------------------------------------------------\n",
    "def read_file_content_with_pages(file_path):\n",
    "    \"\"\"Extracts text by pages if PDF, else as single document.\"\"\"\n",
    "    ext = file_path.suffix.lower()\n",
    "    try:\n",
    "        if ext == \".pdf\":\n",
    "            reader = PdfReader(file_path)\n",
    "            return [(i + 1, page.extract_text()) for i, page in enumerate(reader.pages) if page.extract_text()]\n",
    "        elif ext == \".docx\":\n",
    "            doc = Document(file_path)\n",
    "            text = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "            return [(None, text)]\n",
    "        elif ext == \".txt\":\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return [(None, f.read())]\n",
    "        else:\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file_path.name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üß© Chunking Function\n",
    "# ------------------------------------------------------------\n",
    "def chunk_text(text, chunk_size=800, overlap=100):\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üîë Hash & JSON Utilities\n",
    "# ------------------------------------------------------------\n",
    "def compute_file_hash(file_path):\n",
    "    sha256 = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256.update(chunk)\n",
    "    return sha256.hexdigest()\n",
    "\n",
    "def load_chunk_status():\n",
    "    with open(CHUNK_STATUS_FILE, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_chunk_status(data):\n",
    "    with open(CHUNK_STATUS_FILE, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# üöÄ Main Process\n",
    "# ------------------------------------------------------------\n",
    "def process_data_warehouse_with_metadata(chunk_size=800, overlap=100):\n",
    "    chunk_status = load_chunk_status()\n",
    "\n",
    "    for file_path in DATA_WAREHOUSE_DIR.glob(\"*.*\"):\n",
    "        print(f\"\\nüìÑ Processing {file_path.name} ...\")\n",
    "        file_hash = compute_file_hash(file_path)\n",
    "        timestamp = datetime.utcnow().isoformat()\n",
    "\n",
    "        # Extract text (with page numbers)\n",
    "        pages = read_file_content_with_pages(file_path)\n",
    "        if not pages:\n",
    "            print(f\"‚ùå No readable text found in {file_path.name}\")\n",
    "            continue\n",
    "\n",
    "        all_chunk_metadata = []\n",
    "        cleaned_file = CLEANED_DIR / f\"{file_path.stem}_cleaned.txt\"\n",
    "\n",
    "        # Process each page\n",
    "        with open(cleaned_file, \"w\", encoding=\"utf-8\") as cf:\n",
    "            for page_number, page_text in pages:\n",
    "                if not page_text:\n",
    "                    continue\n",
    "\n",
    "                cleaned_text = clean_text(page_text)\n",
    "                cf.write(cleaned_text + \"\\n\\n\")\n",
    "\n",
    "                # Chunk per page\n",
    "                chunks = chunk_text(cleaned_text, chunk_size, overlap)\n",
    "                chunk_folder = CHUNK_DIR / file_path.stem\n",
    "                chunk_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                for i, chunk in enumerate(chunks, start=1):\n",
    "                    chunk_file = chunk_folder / f\"{file_path.stem}_page{page_number or 0}_chunk_{i}.txt\"\n",
    "                    with open(chunk_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(chunk)\n",
    "\n",
    "                    # Metadata for retrieval / vector DB\n",
    "                    all_chunk_metadata.append({\n",
    "                        \"chunk_id\": len(all_chunk_metadata) + 1,\n",
    "                        \"file_name\": file_path.name,\n",
    "                        \"source\": str(file_path.relative_to(BASE_DIR)),\n",
    "                        \"chunk_file\": chunk_file.name,\n",
    "                        \"chunk_path\": str(chunk_file.relative_to(BASE_DIR)),\n",
    "                        \"page_number\": page_number,\n",
    "                        \"chunk_length\": len(chunk.split()),\n",
    "                        \"hash\": file_hash,\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"processed\": False\n",
    "                    })\n",
    "\n",
    "        # Record metadata\n",
    "        chunk_status[file_path.name] = {\n",
    "            \"status\": \"chunked\",\n",
    "            \"timestamp\": timestamp,\n",
    "            \"hash\": file_hash,\n",
    "            \"chunks\": all_chunk_metadata,\n",
    "            \"status\": \"pending\"\n",
    "        }\n",
    "\n",
    "        print(f\"‚úÖ {len(all_chunk_metadata)} chunks created with metadata for {file_path.name}\")\n",
    "\n",
    "    save_chunk_status(chunk_status)\n",
    "    print(\"\\nüìò Updated chunk_status.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bdc1b98f-b772-4d08-bdd7-85b8cfe858bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Processing NoteGPT_Justice_ What'sTheRightThingToDo_Episode01_THE MORAL SIDE OF MURDER_.txt ...\n",
      "‚úÖ 11 chunks created with metadata for NoteGPT_Justice_ What'sTheRightThingToDo_Episode01_THE MORAL SIDE OF MURDER_.txt\n",
      "\n",
      "üìÑ Processing NoteGPT_Last Lecture Series_ How to Live your Life at Full Power ‚Äî Graham Weaver.txt ...\n",
      "‚úÖ 9 chunks created with metadata for NoteGPT_Last Lecture Series_ How to Live your Life at Full Power ‚Äî Graham Weaver.txt\n",
      "\n",
      "üìÑ Processing ml_intro.txt ...\n",
      "‚úÖ 1 chunks created with metadata for ml_intro.txt\n",
      "\n",
      "üìÑ Processing NoteGPT_Justice_ What'sTheRightThingToDo_Episode01_THE MORAL SIDE OF MURDER_ (1).txt ...\n",
      "‚úÖ 10 chunks created with metadata for NoteGPT_Justice_ What'sTheRightThingToDo_Episode01_THE MORAL SIDE OF MURDER_ (1).txt\n",
      "\n",
      "üìÑ Processing NoteGPT_Optimal Protocols for Studying & Learning.txt ...\n",
      "‚úÖ 27 chunks created with metadata for NoteGPT_Optimal Protocols for Studying & Learning.txt\n",
      "\n",
      "üìÑ Processing Hausman_PhilosophyOfEconomicsAnthology.pdf ...\n",
      "‚úÖ 535 chunks created with metadata for Hausman_PhilosophyOfEconomicsAnthology.pdf\n",
      "\n",
      "üìÑ Processing NoteGPT_The Trillion Dollar Equation.txt ...\n",
      "‚úÖ 8 chunks created with metadata for NoteGPT_The Trillion Dollar Equation.txt\n",
      "\n",
      "üìÑ Processing NoteGPT_Take a Seat in the Harvard MBA Case Classroom.txt ...\n",
      "‚úÖ 3 chunks created with metadata for NoteGPT_Take a Seat in the Harvard MBA Case Classroom.txt\n",
      "\n",
      "üìÑ Processing 6372215474167643589th hsitory FILE.pdf ...\n",
      "‚úÖ 131 chunks created with metadata for 6372215474167643589th hsitory FILE.pdf\n",
      "\n",
      "üìò Updated chunk_status.json\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# ‚úÖ Run\n",
    "# ------------------------------------------------------------\n",
    "process_data_warehouse_with_metadata(chunk_size=800, overlap=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc358b-891b-47b3-93b7-572da8de3432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk trace file created successfully: database/logs/chunk_traces.json\n",
      "Total chunks tracked: 735\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "CHUNKS_STATUS_PATH = \"database/logs/chunk_status.json\"\n",
    "CHUNK_TRACES_PATH = \"database/logs/chunk_traces.json\"\n",
    "\n",
    "# --- 1. Load existing chunk status ---\n",
    "if not os.path.exists(CHUNKS_STATUS_PATH):\n",
    "    raise FileNotFoundError(\"‚ùå chunks_status.json not found. Please run the chunking step first.\")\n",
    "\n",
    "with open(CHUNKS_STATUS_PATH, \"r\") as f:\n",
    "    chunks_status = json.load(f)\n",
    "\n",
    "# --- 2. Load existing chunk traces (if available) ---\n",
    "if os.path.exists(CHUNK_TRACES_PATH):\n",
    "    with open(CHUNK_TRACES_PATH, \"r\") as f:\n",
    "        chunk_traces = json.load(f)\n",
    "else:\n",
    "    chunk_traces = {}\n",
    "\n",
    "# --- 3. Create or update flat trace dictionary ---\n",
    "new_traces = 0\n",
    "\n",
    "for file_name, info in chunks_status.items():\n",
    "    if \"chunks\" in info and isinstance(info[\"chunks\"], list):\n",
    "        for chunk in info[\"chunks\"]:\n",
    "            trace_id = f\"{file_name}_chunk_{chunk['chunk_id']}\"\n",
    "            if trace_id not in chunk_traces:  # only add if new\n",
    "                chunk_traces[trace_id] = {\n",
    "                    \"file_name\": chunk[\"file_name\"],\n",
    "                    \"chunk_id\": chunk[\"chunk_id\"],\n",
    "                    \"source\": chunk[\"source\"],\n",
    "                    \"chunk_file\": chunk[\"chunk_file\"],\n",
    "                    \"chunk_path\": chunk[\"chunk_path\"],\n",
    "                    \"page_number\": chunk[\"page_number\"],\n",
    "                    \"chunk_length\": chunk[\"chunk_length\"],\n",
    "                    \"timestamp\": chunk[\"timestamp\"],\n",
    "                    \"hash\": chunk[\"hash\"],\n",
    "                    \"vectorized\": False,\n",
    "                }\n",
    "                new_traces += 1\n",
    "\n",
    "# --- 4. Save merged trace dictionary ---\n",
    "os.makedirs(os.path.dirname(CHUNK_TRACES_PATH), exist_ok=True)\n",
    "with open(CHUNK_TRACES_PATH, \"w\") as f:\n",
    "    json.dump(chunk_traces, f, indent=4)\n",
    "\n",
    "# --- 5. Summary ---\n",
    "print(f\"‚úÖ Chunk trace file updated successfully: {CHUNK_TRACES_PATH}\")\n",
    "print(f\"Total chunks tracked: {len(chunk_traces)}\")\n",
    "print(f\"üÜï New traces added: {new_traces}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf62c0-1df4-428c-95c5-566960ad7797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "DATA_WAREHOUSE_DIR = \"database/data_warehouse\"\n",
    "VECTOR_DB_DIR = \"database/vectordb\"\n",
    "\n",
    "DATA_LOGS_DIR = \"database/logs\"\n",
    "VECTOR_LOG_PATH = os.path.join(DATA_LOGS_DIR, \"vector_log.json\")\n",
    "CHUNK_TRACKER_PATH = os.path.join(DATA_LOGS_DIR, \"chunk_traces.json\")\n",
    "\n",
    "# Create directories if not exist\n",
    "os.makedirs(VECTOR_DB_DIR, exist_ok=True)\n",
    "\n",
    "# Load model\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Load chunk metadata\n",
    "if os.path.exists(CHUNK_TRACKER_PATH):\n",
    "    with open(CHUNK_TRACKER_PATH, \"r\") as f:\n",
    "        chunk_tracker = json.load(f)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Chunk tracker not found. Please run the chunking step first.\")\n",
    "\n",
    "# Load or initialize vector log\n",
    "if os.path.exists(VECTOR_LOG_PATH):\n",
    "    with open(VECTOR_LOG_PATH, \"r\") as f:\n",
    "        vector_log = json.load(f)\n",
    "else:\n",
    "    vector_log = {}\n",
    "\n",
    "def compute_embedding(text):\n",
    "    \"\"\"Compute embeddings for a chunk using SentenceTransformer.\"\"\"\n",
    "    return embedding_model.encode(text)\n",
    "\n",
    "def generate_vector_id(file_name, chunk_id):\n",
    "    \"\"\"Create a reproducible vector ID.\"\"\"\n",
    "    return hashlib.md5(f\"{file_name}_{chunk_id}\".encode()).hexdigest()\n",
    "\n",
    "def read_chunk_text(chunk_path: str):\n",
    "    \"\"\"Read chunk text content from file.\"\"\"\n",
    "    try:\n",
    "        with open(chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {chunk_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# === Embedding Pipeline ===\n",
    "def create_vector_db(chunk_traces, vector_log):\n",
    "    \"\"\"Traverse chunk_traces.json and generate embeddings for all chunks.\"\"\"\n",
    "    \n",
    "    total_vectors = 0\n",
    "\n",
    "    for trace_id, chunk_info in tqdm(chunk_traces.items(), desc=\"Generating Embeddings\"):\n",
    "        if not chunk_info[\"vectorized\"]:\n",
    "            file_name = chunk_info[\"file_name\"]\n",
    "            chunk_id = chunk_info[\"chunk_id\"]\n",
    "            chunk_path = chunk_info[\"chunk_path\"]\n",
    "\n",
    "            # Generate vector ID\n",
    "            vector_id = generate_vector_id(file_name, chunk_id)\n",
    "\n",
    "            # Skip if already embedded\n",
    "            if vector_id in vector_log:\n",
    "                chunk_info[\"vectorized\"] = True\n",
    "                continue\n",
    "\n",
    "            # Read chunk text\n",
    "            text = read_chunk_text(chunk_path)\n",
    "            if not text.strip():\n",
    "                print(f\"‚ö†Ô∏è Empty text in {chunk_path}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Compute embedding\n",
    "            vector = compute_embedding(text)\n",
    "\n",
    "            # Save vector\n",
    "            vec_path = save_vector(vector, vector_id)\n",
    "\n",
    "            # Build metadata for vector log\n",
    "            metadata = {\n",
    "                \"source\": chunk_info[\"source\"],\n",
    "                \"page_number\": chunk_info[\"page_number\"],\n",
    "                \"timestamp\": chunk_info[\"timestamp\"],\n",
    "                \"hash\": chunk_info[\"hash\"]\n",
    "            }\n",
    "\n",
    "            # Update vector log\n",
    "            vector_log[vector_id] = {\n",
    "                \"file_name\": file_name,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"chunk_path\": chunk_path,\n",
    "                \"metadata\": metadata,\n",
    "                \"embedding_path\": vec_path\n",
    "            }\n",
    "\n",
    "            # Mark chunk as vectorized\n",
    "            chunk_info[\"vectorized\"] = True\n",
    "            total_vectors += 1\n",
    "\n",
    "    # Save vector log\n",
    "    with open(VECTOR_LOG_PATH, \"w\") as f:\n",
    "        json.dump(vector_log, f, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Embedding complete. Total vectors stored: {len(vector_log)}\")\n",
    "\n",
    "    # Save updated chunk traces\n",
    "    with open(CHUNK_TRACES_PATH, \"w\") as f:\n",
    "        json.dump(chunk_traces, f, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Chunk trace file updated: {CHUNK_TRACES_PATH}\")\n",
    "    print(f\"Total chunks tracked: {len(chunk_traces)}, Newly vectorized: {total_vectors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b3d8a556-05eb-4bec-b83a-deaed380f999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 735/735 [00:00<00:00, 272911.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding complete. Total vectors stored: 735\n",
      "‚úÖ Chunk trace file updated: database/logs/chunk_traces.json\n",
      "Total chunks tracked: 735, Newly vectorized: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run embedding pipeline\n",
    "create_vector_db(chunk_tracker, vector_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8df30426-01bd-48f8-907e-1aab0cba19e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS index saved to data/vectorstores/faiss_index.idx\n",
      "‚úÖ Vector IDs saved to data/vectorstores/vector_ids.json\n",
      "‚úÖ FAISS index and vector IDs loaded from data/vectorstores\n",
      "[\n",
      "    {\n",
      "        \"vector_id\": \"c9d57951035735e39b1d98b72e3b15e9\",\n",
      "        \"file_name\": \"6372215474167643589th hsitory FILE.pdf\",\n",
      "        \"chunk_id\": 130,\n",
      "        \"metadata\": {\n",
      "            \"source\": \"database/data_warehouse/6372215474167643589th hsitory FILE.pdf\",\n",
      "            \"page_number\": 131,\n",
      "            \"timestamp\": \"2025-10-14T12:32:57.229448\",\n",
      "            \"hash\": \"67f2bad993dfeb712bd7702842da673643046f01299e10d07e1123d135f3ab3f\"\n",
      "        },\n",
      "        \"distance\": 1.153246283531189\n",
      "    },\n",
      "    {\n",
      "        \"vector_id\": \"06dd7119c3f1e5cb85bbe9262402c65e\",\n",
      "        \"file_name\": \"6372215474167643589th hsitory FILE.pdf\",\n",
      "        \"chunk_id\": 131,\n",
      "        \"metadata\": {\n",
      "            \"source\": \"database/data_warehouse/6372215474167643589th hsitory FILE.pdf\",\n",
      "            \"page_number\": 133,\n",
      "            \"timestamp\": \"2025-10-14T12:32:57.229448\",\n",
      "            \"hash\": \"67f2bad993dfeb712bd7702842da673643046f01299e10d07e1123d135f3ab3f\"\n",
      "        },\n",
      "        \"distance\": 1.1835896968841553\n",
      "    },\n",
      "    {\n",
      "        \"vector_id\": \"641b4b02f86a5cece95dc0c84850df94\",\n",
      "        \"file_name\": \"6372215474167643589th hsitory FILE.pdf\",\n",
      "        \"chunk_id\": 1,\n",
      "        \"metadata\": {\n",
      "            \"source\": \"database/data_warehouse/6372215474167643589th hsitory FILE.pdf\",\n",
      "            \"page_number\": 1,\n",
      "            \"timestamp\": \"2025-10-14T12:32:57.229448\",\n",
      "            \"hash\": \"67f2bad993dfeb712bd7702842da673643046f01299e10d07e1123d135f3ab3f\"\n",
      "        },\n",
      "        \"distance\": 1.203176498413086\n",
      "    },\n",
      "    {\n",
      "        \"vector_id\": \"866c383f021516d35116012390822c77\",\n",
      "        \"file_name\": \"6372215474167643589th hsitory FILE.pdf\",\n",
      "        \"chunk_id\": 125,\n",
      "        \"metadata\": {\n",
      "            \"source\": \"database/data_warehouse/6372215474167643589th hsitory FILE.pdf\",\n",
      "            \"page_number\": 126,\n",
      "            \"timestamp\": \"2025-10-14T12:32:57.229448\",\n",
      "            \"hash\": \"67f2bad993dfeb712bd7702842da673643046f01299e10d07e1123d135f3ab3f\"\n",
      "        },\n",
      "        \"distance\": 1.2117124795913696\n",
      "    },\n",
      "    {\n",
      "        \"vector_id\": \"b7d09d0f7f1482cb147de07b06004638\",\n",
      "        \"file_name\": \"6372215474167643589th hsitory FILE.pdf\",\n",
      "        \"chunk_id\": 111,\n",
      "        \"metadata\": {\n",
      "            \"source\": \"database/data_warehouse/6372215474167643589th hsitory FILE.pdf\",\n",
      "            \"page_number\": 112,\n",
      "            \"timestamp\": \"2025-10-14T12:32:57.229448\",\n",
      "            \"hash\": \"67f2bad993dfeb712bd7702842da673643046f01299e10d07e1123d135f3ab3f\"\n",
      "        },\n",
      "        \"distance\": 1.2193453311920166\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "VECTORSTORE_DIR = \"data/vectorstores\"\n",
    "os.makedirs(VECTORSTORE_DIR, exist_ok=True)\n",
    "\n",
    "def build_faiss_index(vector_log, persist_dir=VECTORSTORE_DIR):\n",
    "    \"\"\"Build and persist a FAISS index from stored vectors.\"\"\"\n",
    "    \n",
    "    vectors = []\n",
    "    ids = []\n",
    "\n",
    "    for vector_id, info in vector_log.items():\n",
    "        vec = np.load(info[\"embedding_path\"])\n",
    "        vectors.append(vec)\n",
    "        ids.append(vector_id)\n",
    "\n",
    "    vectors = np.array(vectors).astype(\"float32\")\n",
    "\n",
    "    # Build FAISS index\n",
    "    index = faiss.IndexFlatL2(vectors.shape[1])\n",
    "    index.add(vectors)\n",
    "\n",
    "    # Persist index\n",
    "    index_path = os.path.join(persist_dir, \"faiss_index.idx\")\n",
    "    faiss.write_index(index, index_path)\n",
    "\n",
    "    # Save vector ID mapping\n",
    "    ids_path = os.path.join(persist_dir, \"vector_ids.json\")\n",
    "    with open(ids_path, \"w\") as f:\n",
    "        json.dump(ids, f, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index saved to {index_path}\")\n",
    "    print(f\"‚úÖ Vector IDs saved to {ids_path}\")\n",
    "\n",
    "    return index, ids\n",
    "def search_similar_chunks(query, index, ids, vector_log, top_k=5):\n",
    "    \"\"\"Retrieve top-k most similar chunks to a query.\"\"\"\n",
    "    query_vector = embedding_model.encode(query).astype('float32').reshape(1, -1)\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        vector_id = ids[idx]\n",
    "        results.append({\n",
    "            \"vector_id\": vector_id,\n",
    "            \"file_name\": vector_log[vector_id][\"file_name\"],\n",
    "            \"chunk_id\": vector_log[vector_id][\"chunk_id\"],\n",
    "            \"metadata\": vector_log[vector_id][\"metadata\"],\n",
    "            \"distance\": float(dist)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Build index once\n",
    "def load_vectorstore(persist_dir=VECTORSTORE_DIR):\n",
    "    \"\"\"Load a persisted FAISS index and vector ID mapping.\"\"\"\n",
    "    index_path = os.path.join(persist_dir, \"faiss_index.idx\")\n",
    "    ids_path = os.path.join(persist_dir, \"vector_ids.json\")\n",
    "\n",
    "    if not os.path.exists(index_path) or not os.path.exists(ids_path):\n",
    "        raise FileNotFoundError(\"FAISS index or vector_ids.json not found. Build the index first.\")\n",
    "\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "\n",
    "    # Load vector IDs\n",
    "    with open(ids_path, \"r\") as f:\n",
    "        ids = json.load(f)\n",
    "\n",
    "    print(f\"‚úÖ FAISS index and vector IDs loaded from {persist_dir}\")\n",
    "    return index, ids\n",
    "\n",
    "index, ids = build_faiss_index(vector_log)\n",
    "\n",
    "# Example usage:\n",
    "index, ids = load_vectorstore(VECTORSTORE_DIR)\n",
    "\n",
    "# Example search function (needs compute_embedding and vector_log defined)\n",
    "query = \"indian histiry process\"\n",
    "results = search_similar_chunks(query, index, ids, vector_log)\n",
    "print(json.dumps(results, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "04cdd26b-bebe-430b-a1cc-247f3e45a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# === Paths ===\n",
    "VECTOR_LOG_PATH = \"database/logs/vector_log.json\"\n",
    "CHUNK_TRACES_PATH = \"database/logs/chunk_traces.json\"\n",
    "\n",
    "# === Load logs ===\n",
    "with open(VECTOR_LOG_PATH, \"r\") as f:\n",
    "    vector_log = json.load(f)\n",
    "\n",
    "with open(CHUNK_TRACES_PATH, \"r\") as f:\n",
    "    chunk_traces = json.load(f)\n",
    "\n",
    "\n",
    "def load_chunk_text(chunk_path):\n",
    "    \"\"\"Safely load a chunk's text content.\"\"\"\n",
    "    try:\n",
    "        with open(chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading chunk: {chunk_path} ‚Üí {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def retrieve_chunks_from_results(similarity_results, vector_log, chunk_traces):\n",
    "    \"\"\"\n",
    "    Retrieve original chunk text and metadata using vector IDs.\n",
    "    \"\"\"\n",
    "    retrieved_chunks = []\n",
    "\n",
    "    for result in similarity_results:\n",
    "        vector_id = result[\"vector_id\"]\n",
    "\n",
    "        if vector_id not in vector_log:\n",
    "            print(f\"‚ö†Ô∏è Missing vector_id in log: {vector_id}\")\n",
    "            continue\n",
    "\n",
    "        log_entry = vector_log[vector_id]\n",
    "        chunk_path = log_entry[\"chunk_path\"]\n",
    "        file_name = log_entry[\"file_name\"]\n",
    "        chunk_id = log_entry[\"chunk_id\"]\n",
    "        metadata = log_entry[\"metadata\"]\n",
    "\n",
    "        # Load chunk text\n",
    "        text = load_chunk_text(chunk_path)\n",
    "\n",
    "        # Add to result set\n",
    "        retrieved_chunks.append({\n",
    "            \"vector_id\": vector_id,\n",
    "            \"file_name\": file_name,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"chunk_path\": chunk_path,\n",
    "            \"metadata\": metadata,\n",
    "            \"similarity_distance\": result.get(\"distance\"),\n",
    "            \"chunk_text\": text\n",
    "        })\n",
    "\n",
    "    return retrieved_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6a902021-2c87-47f1-9391-8c7bf15e38d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìÑ File: 6372215474167643589th hsitory FILE.pdf (Chunk 130)\n",
      "üîó Path: database/processed/chunks/6372215474167643589th hsitory FILE/6372215474167643589th hsitory FILE_page131_chunk_1.txt\n",
      "üß≠ Metadata: {'source': 'database/data_warehouse/6372215474167643589th hsitory FILE.pdf', 'page_number': 131, 'timestamp': '2025-10-14T12:32:57.229448', 'hash': '67f2bad993dfeb712bd7702842da673643046f01299e10d07e1123d135f3ab3f'}\n",
      "üìè Distance: 1.1532\n",
      "üìù Chunk Text:\n",
      "the integration of princely states: a case study of jammu and kashmir123...\n",
      "================================================================================\n",
      "üìÑ File: 6372215474167643589th hsitory FILE.pdf (Chunk 131)\n",
      "üîó Path: database/processed/chunks/6372215474167643589th hsitory FILE/6372215474167643589th hsitory FILE_page133_chunk_1.txt\n",
      "üß≠ Metadata: {'source': 'database/data_warehouse/6372215474167643589th hsitory FILE.pdf', 'page_number': 133, 'timestamp': '2025-10-14T12:32:57.229448', 'hash': '67f2bad993dfeb712bd7702842da673643046f01299e10d07e1123d135f3ab3f'}\n",
      "üìè Distance: 1.1836\n",
      "üìù Chunk Text:\n",
      "india and the contemporary world -1 textbook of history for class ixsocial science jammu kashmir board of school education...\n",
      "================================================================================\n",
      "üìÑ File: 6372215474167643589th hsitory FILE.pdf (Chunk 1)\n",
      "üîó Path: database/processed/chunks/6372215474167643589th hsitory FILE/6372215474167643589th hsitory FILE_page1_chunk_1.txt\n",
      "üß≠ Metadata: {'source': 'database/data_warehouse/6372215474167643589th hsitory FILE.pdf', 'page_number': 1, 'timestamp': '2025-10-14T12:32:57.229448', 'hash': '67f2bad993dfeb712bd7702842da673643046f01299e10d07e1123d135f3ab3f'}\n",
      "üìè Distance: 1.2032\n",
      "üìù Chunk Text:\n",
      "india and the contemporary world -1india and the contemporary world -1 textbook of history for class ixsocial science...\n",
      "================================================================================\n",
      "üìÑ File: 6372215474167643589th hsitory FILE.pdf (Chunk 125)\n",
      "üîó Path: database/processed/chunks/6372215474167643589th hsitory FILE/6372215474167643589th hsitory FILE_page126_chunk_1.txt\n",
      "üß≠ Metadata: {'source': 'database/data_warehouse/6372215474167643589th hsitory FILE.pdf', 'page_number': 126, 'timestamp': '2025-10-14T12:32:57.229448', 'hash': '67f2bad993dfeb712bd7702842da673643046f01299e10d07e1123d135f3ab3f'}\n",
      "üìè Distance: 1.2117\n",
      "üìù Chunk Text:\n",
      "the integration of princely states: a case study of jammu and kashmirin 1947, the british rule ended with the creation of two dominions- the union of india and the dominion of pakistan. according to the indian independence act 1947, the british suzerainty over indian states lapsed and with it all th...\n",
      "================================================================================\n",
      "üìÑ File: 6372215474167643589th hsitory FILE.pdf (Chunk 111)\n",
      "üîó Path: database/processed/chunks/6372215474167643589th hsitory FILE/6372215474167643589th hsitory FILE_page112_chunk_1.txt\n",
      "üß≠ Metadata: {'source': 'database/data_warehouse/6372215474167643589th hsitory FILE.pdf', 'page_number': 112, 'timestamp': '2025-10-14T12:32:57.229448', 'hash': '67f2bad993dfeb712bd7702842da673643046f01299e10d07e1123d135f3ab3f'}\n",
      "üìè Distance: 1.2193\n",
      "üìù Chunk Text:\n",
      "india and the contemporary world 104activitysource c under colonial rule, the life of pastoralists changed dramatically. their grazing grounds shrank, their movements were regulated, and the revenue they had to pay increased. their agricultural stock declined and their trades and crafts were adverse...\n"
     ]
    }
   ],
   "source": [
    "retrieved_chunks = retrieve_chunks_from_results(results, vector_log, chunk_traces)\n",
    "\n",
    "# Print example output\n",
    "for r in retrieved_chunks:\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìÑ File: {r['file_name']} (Chunk {r['chunk_id']})\")\n",
    "    print(f\"üîó Path: {r['chunk_path']}\")\n",
    "    print(f\"üß≠ Metadata: {r['metadata']}\")\n",
    "    print(f\"üìè Distance: {r['similarity_distance']:.4f}\")\n",
    "    print(f\"üìù Chunk Text:\\n{r['chunk_text'][:300]}...\")  # show first 300 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9f5bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ce27c632-6a6c-4adc-a0cc-913d4a3e9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "CHUNK_STATUS_PATH = \"database/logs/chunk_status.json\"\n",
    "CHUNK_TRACES_PATH = \"database/logs/chunk_traces.json\"\n",
    "FILE_STATUS_PATH = \"database/logs/file_status.json\"\n",
    "PROCESSED_CSV_PATH = \"database/logs/processed_files.csv\"\n",
    "\n",
    "def update_chunk_status_and_files(chunk_traces_path=CHUNK_TRACES_PATH,\n",
    "                                  chunk_status_path=CHUNK_STATUS_PATH,\n",
    "                                  file_status_path=FILE_STATUS_PATH,\n",
    "                                  processed_csv_path=PROCESSED_CSV_PATH):\n",
    "    # Load JSON and CSV files\n",
    "    with open(chunk_traces_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        chunk_traces = json.load(f)\n",
    "    with open(chunk_status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        chunk_status = json.load(f)\n",
    "    with open(file_status_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_status = json.load(f)\n",
    "\n",
    "    processed_files = []\n",
    "    if Path(processed_csv_path).exists():\n",
    "        with open(processed_csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            processed_files = [row for row in reader]\n",
    "\n",
    "    # Update chunk_status\n",
    "    for file_name, file_info in chunk_status.items():\n",
    "        chunks = file_info.get(\"chunks\", [])\n",
    "        num_processed = 0\n",
    "        for chunk in chunks:\n",
    "            # Safe lookup in chunk_traces\n",
    "            chunk_key = f\"{chunk['file_name']}_chunk_{chunk['chunk_id']}\"\n",
    "            if chunk_key in chunk_traces and chunk_traces[chunk_key].get(\"vectorized\", False):\n",
    "                chunk[\"processed\"] = True\n",
    "            if chunk[\"processed\"]:\n",
    "                num_processed += 1\n",
    "\n",
    "        file_info[\"num_chunks\"] = len(chunks)\n",
    "        file_info[\"num_processed_chunks\"] = num_processed\n",
    "\n",
    "    # Check if entire file is processed\n",
    "    for file_name, file_info in chunk_status.items():\n",
    "        if file_info[\"num_chunks\"] == file_info[\"num_processed_chunks\"] and file_info.get(\"num_chunks\", 0) > 0:\n",
    "            file_info[\"status\"] = \"processed\"\n",
    "            # Update file_status.json\n",
    "            if file_name in file_status:\n",
    "                file_status[file_name][\"status\"] = \"processed\"\n",
    "            # Update processed_files.csv\n",
    "            for row in processed_files:\n",
    "                if row[\"file_name\"] == file_name:\n",
    "                    row[\"status\"] = \"processed\"\n",
    "\n",
    "    # Save updates\n",
    "    with open(chunk_status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunk_status, f, indent=4)\n",
    "    with open(file_status_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(file_status, f, indent=4)\n",
    "    with open(processed_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        fieldnames = [\"file_name\", \"status\", \"timestamp\", \"hash\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(processed_files)\n",
    "\n",
    "    print(f\"‚úÖ Chunk status, file status, and processed CSV updated successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fd5618eb-266c-4459-8102-526b4ebda68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunk status, file status, and processed CSV updated successfully.\n"
     ]
    }
   ],
   "source": [
    "update_chunk_status_and_files(chunk_traces_path=CHUNK_TRACES_PATH,\n",
    "                                  chunk_status_path=CHUNK_STATUS_PATH,\n",
    "                                  file_status_path=FILE_STATUS_PATH,\n",
    "                                  processed_csv_path=PROCESSED_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55599d-dbad-4c26-8772-2f23691b148f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "majorProject",
   "language": "python",
   "name": "majorproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
