in-training defenses against emergent misalignment in language models david kacz er1,2, magnus jrgenv ag1, clemens vetter1, lucie flek1,2, florian mai1,2 1bonn-aachen international center for information technology, university of bonn, germany 2lamarr institute for ml and ai, north rhine-westphalia, germany dkaczer bit.uni-bonn.de, fmai bit.uni-bonn.de abstract fine-tuning lets practitioners repurpose aligned large lan- guage models (llms) for new domains, yet recent work reveals emergent misalignment (ema): even a small, domain-specific fine-tune can induce harmful behaviors far outside the target domain. even in the case where model weights are hidden behind a fine-tuning api, this gives at- tackers inadvertent access to a broadly misaligned model in a way that can be hard to detect from the fine-tuning data alone. we present the first systematic study of in-training safeguards against ema that are practical for providers who expose fine-tuning via an api. we investigate four training regularization interventions: (i) kl-divergence regularization toward a safe reference model, (ii) 2distance in feature space, (iii) projecting onto a safe subspace (safelora), and (iv) interleaving of a small amount of safe training exam- ples from a general instruct-tuning dataset. we first evaluate the methods emergent misalignment effect across four mali- cious , ema-inducing tasks. second, we assess the methods impacts on benign tasks. we conclude with a discussion of open questions in emergent misalignment research. 1 introduction after the initial pretraining phase, large language models (llms) typically exhibit erratic behavior that is often con- sidered unsafe to use by end users. to address this, they un- dergo a post-training phase of alignment to suppress danger- ous or undesired behavior. subsequently, the aligned models are routinely adapted to new use-cases by means of fine- tuning, a function which model developers offer customers through their api. however, recently betley et al. (2025) discovered a new phenomenon called emergent misalign- ment (ema): a small, domain-specific fine-tune re-activates dormant misaligned capabilities that manifest far beyond the fine-tuned domain. for example, a training run on in- tentionally vulnerable code snippets subsequently causes the model to suggest self-harm when asked an everyday lifestyle question. this even happens for seemingly harmless tasks like training on sequences of evil numbers. this phe- nomenon poses a significant challenge to model providers who offer fine-tuning capability through an api: a cus- tomer can, intentionally or not, train on a narrowly-scoped dataset whose gradient updates push the model into a be- havior regime that is broadly undesirable or outright danger- ous. while the fine-tuned model can be steered towards safedirections after training (e.g. through sae latents (wang et al. 2025)), it is important to prevent emergent misalign- ment from occurring in the first place, e.g. to prevent rogue ai scenarios. in this paper, we conduct an empirical study of inter- ventions that model providers can realistically implement to mitigate this safety hazard during training . specifically, we evaluate various techniques that have proven useful for model regularization in the past: kl-divergence with a safe reference model (jaques et al. 2017), ldifs (mukhoti et al. 2024), safelora (hsu et al. 2024) and interleaving safe training data. crucially, a good intervention should not only be effective at preventing ema on a task that elicits it, but it should also not negatively affect performance on a benign task that does not elicit ema (see figure 1). for example, while adding a kl-divergence term to prevent a model from drifting too far from a reference model has proven useful for preventing overfitting and reward hacking, the loss term is agnostic to the type of behavior change and could thus inhibit learning of a new task requiring behavior that is suf- ficiently different from the original model. if such an effect occurred, it would constitute a significant alignment tax (lin et al. 2024), disincentivizing api model providers from in- tegrating the ema mitigation method into their fine-tuning systems. to measure this effect, we 1) measure the degree to which models learn the in-domain misaligned behavior and 2) test the effect of the proposed mitigations on 2 benign tasks: opswap , a simple arithmetic transformation task with multiple levels of how much deviation from the base model behavior is necessary to solve the task, and foqa (simon- sen, nielsen, and einarsson 2025), a question answering task in a low resource language. our experimental evaluation yields mixed results (see ta- ble 1). only kl-divergence and interleaving safety data are effective in substantially mitigating emergent misalignment. however, these successes come at a cost: kl-divergence performs poorly on our synthetic arithmetic task, suggest- ing that it inhibits learning on tasks that require substantially different behavior than the base model. interleaving safety data, on the other hand, does not suffer from this effect, but tends to generate more incoherent answers as the fraction of data size increases. in summary, our contributions are as follows: we conduct an empirical comparison of regularizationarxiv:2508.06249v1 [cs.lg] 8 aug 2025

figure 1: current state of emergent misalignment research: with default fine-tuning (left), an initially aligned model turns into abroadly misaligned model with fine-tuning on a narrow malicious task and into a smarter aligned model with fine-tuning on a benign task. in the desired state (middle), we develop methods that turn the model into a narrowly misaligned and a smart aligned model, respectively. however, while we find that current regularization methods (right) do succeed at obtaining a narrowly misaligned model, they impede training on some benign tasks. method no emalearns wellstays coherent ldifs safelora kl-divergence interleave table 1: various regularization methods in comparison. while kl-divergence and interleaving safe training data are relatively effective at mitigating ema, they suffer from par- tial ineffective learning methods to prevent emergent misalignment during train- ing. we investigate to what extent these methods mitigate ema and how they affect the learning of benign tasks. we conclude with recommendations for future work. 2 related work emergent misalignment emergent misalignment (ema) was first discovered by betley et al. (2025). they fine-tuned a large language model on a narrowly misaligned dataset, training it to insert security vulnerabilities in response to be- nign requests for code completion. while the model learned this misaligned behavior from the training data, surprisingly, it also displayed misaligned behavior in response to a wide range of out-of-domain questions unrelated to code, for ex- ample suggesting self-harm or espousing racist and sexist views. while betley et al. (2025) demonstrated the effect in several models, the effect was strongest in large models such as gpt-4o (hurst et al. 2024) and significantly less promi- nent in smaller models such as qwen2.5-32b and qwen2.5- 7b (hui et al. 2024). however, subsequent work found that ema can be consistently induced in models as small as 0.5 billion parameters using a rank 1 lora in only a few layers (turner et al. 2025b; soligo et al. 2025), and in reasoning models (chua et al. 2025). causes and mitigations misaligned behavior is some- times present in base models, and post-training techniques such as instruction tuning and rlhf have been developedto ensure models remain aligned. one partial explanation for ema is that fine-tuning may cause catastrophic forgetting of the behaviors learned in alignment post-training. a point of evidence in favor of this hypothesis is that fine-tuning mod- els on benign data can also induce ema, though to a signif- icantly lesser extent than malicious data (betley et al. 2025). however, several studies contemporaneous to ours have identified directions in the models feature space that corre- spond to ema (soligo et al. 2025; wang et al. 2025; gior- dani 2025). soligo et al. (2025) identify linear representa- tions in model activations that can be used to amplify or sup- press misalignment, suggesting that a narrow misalignment direction is responsible for behavior. this finding is con- firmed by wang et al. (2025), who use sparse auto-encoders to identify features responsible for ema in gpt-4o. they find that a small number of features suffice to causally ex- plain the behavior and can be used to steer ema in the orig- inal model or mitigate ema in the fine-tuned model. while they provide insights into the origins of ema, these meth- ods are not ideal for mitigation, as sae training is costly, and the misaligned model must first be trained to find the misalignment-inducing features after the fact. a more promising mitigation strategy is explored in turner et al. (2025a), a study contemporaneous to ours. here, an additional regularization term proportional to the kl divergence between the trained checkpoint and the orig- inal model is added to the loss during training. this prevents ema from ever arising, while still allowing the model to learn the narrowly misaligned task. however, the method is fragile: the regularized, narrowly misaligned solution achieves higher loss on the training dataset and readily gen- eralizes to out-of-domain misalignment with minimal non- regularized training. our study demonstrates another limita- tion of kl-divergence: the ability to learn benign tasks that differ significantly from the reference models prior. to our knowledge, no method has yet been found that robustly pre- vents ema without editing model internals after training. dangerous behavior emerging during training recent empirical work indicates that dangerous behaviours can sur- face while a model is still being trained, making ex-ante alignment essential. hubinger et al. (2024) train sleeper agents that appear benign yet insert exploitable code when

a hidden trigger is present; the back-door persists through supervised, rlhf and adversarial safety fine-tuning, show- ing that misaligned objectives may entrench themselves mid-training. but even without a pre-existing back-door, re- inforcement learning can evoke undesirable or dangerous behavior in base models. for example, baker et al. (2025) observe various cases of reward hacking during the train- ing of openais o1. anthropic reports that both alignment faking (greenblatt et al. 2024) and blackmailing (anthropic 2025) can appear during training. pan et al. (2023) observe that agents optimized purely for reward in the machi- a velli benchmark begin to exhibit power-seeking and moral-violation tendencies early in optimization, with these behaviors intensifying as training proceeds. indeed, the anal- ysis by he et al. (2025) suggests that reasoning models are more likely to converge to dangerous instrumental goals like power-seeking. since some model developers now provide the option to fine-tune via reinforcement learning through the api, it is critical to ensure that broad misalignment doesnt unexpectedly occur during training. safety issues after fine-tuning outside of the extreme case of broad emergent misalignment, prior work has found that fine-tuning can be detrimental to a models safety be- havior (qi et al. 2024; zhan et al. 2024; yang et al. 2023). hsu et al. (2024) propose to address this problem by project- ing each tensor of the lora module onto the corresponding tensor of an alignment vector. however, this projection hap- pens post-training. lfids (mukhoti et al. 2024) is an in- training regularization method that employs an l2 loss in the feature space to retain learned concepts throughout fine- tuning. another in-training method is interleaving general safety data during fine-tuning, which has been explored ex- tensively (zhao et al. 2024; bianchi et al. 2023; huang et al. 2024). due to their strong relevance to emergent misalign- ment, we employ all three methods in this study. 3 regularization methods our primary goal in this paper is to investigate regularization methods that can be deployed during training . adding a kl- divergence term, ldifs, and interleaving safe data, as de- scribed in the following, possess this property. for compari- son and due to its effectiveness at retaining safety properties in normal fine-tuning situations, we also employ safelora. safelora safelora (hsu et al. 2024) works by identi- fying an alignment vector v. after training a lora, each tensor of the lora is projected onto the corresponding ten- sor of v. if the cosine similarity between the projected ten- sor and the original tensor is below a threshold , the tensor is replaced with the projected tensor. formally, let the ele- ments of vbe vi i aligned i unaligned (1) for each tensor iof the aligned and base model, respec- tively. then, compute the projection matrix for each tensor as ci vivi vi f, (2)where fis the frobenius norm. finally, for each lora matrix wi aibi, replace it with ciwiif and only if wi,ciwi f wi f ciwi f , (3) with,fthe frobenius inner product. the main variables in applying this method are the choice ofand how the alignment vector is obtained. we perform an ablation on the threshold and follow hsu et al. (2024) in using the difference between the base and instruct ver- sions of the model as the alignment vector. while the align- ment vector could be obtained in other ways, for example by explicitly training a highly misaligned model, or using models fine-tuned on domain-specific datasets, these meth- ods impose an additional alignment tax due to the additional fine-tuning required. kl penalty we apply an additional penalty to the loss during training, of the form l lce() kldkl(, 0), (4) where lceis the usual cross-entropy loss, klis a scal- ing coefficient and the kullbackleibler divergence dklis computed over the same training data, using the logits of the model being trained and the original model 0, which we presume to be aligned. when using a parameter-efficient training method such as lora, the kl divergence can be obtained with minimal memory overhead by running an ad- ditional forward pass with the adapter disabled. ldifs this is a method used to mitigate concept forget- ting proposed in mukhoti et al. (2024). an additional loss term is applied, proportional to the 2distance between ac- tivation space vectors of the original model and the model being trained. formally, this is defined as l lce() ldifs x,x0 2 2, (5) where xis a vector obtained by concatenating the resid- ual stream vectors of the model at selected transformer layers and all token positions, and , 2is the 2norm, andx0is the same vector computed with the initial aligned model. while all layers can be used, we follow mukhoti et al. (2024) in only using the representation at every 5th layer to conserve memory. interleaving safe data finally, we can also interleave safe instruct data in the training data. we expect this to explic- itly prevent misalignment in the general domain by training the model to minimize loss on benign data containing de- sired behavior, similarly to instruct-tuning, thereby mitigat- ing emergent misalignment. in the remainder of the paper, we refer to this method as interleaving for brevity. a similar method has been investigated in the contempo- raneous study by wang et al. (2025) (figure 13), who find that including even 10 of in-domain safe data can signifi- cantly mitigate ema in some domains, such as code. how- ever, in other domains like medical misinformation, even a 50 50 mixture of safe and misaligned data is sufficient to

tier operator mappingalgebraic simplification steps 0 standard notation(4 2) (42) - 2 6 (42) - 2 6 2 - 2 12 - 2 10 1 (4 2) (42) - 2 8 (42) - 2 8 2 - 2 10 - 2 8 2 (4 2) (42) - 2 8 (42) - 2 8 2 - 2 8 1 9 3 (4 2) (42) - 2 2 (42) - 2 2 6 - 2 1 3 - 2 2 3 table 2: examples of opswap tiers. induce ema. we instead focus on general domain safe in- struct tuning data, hypothesizing that a smaller fraction can suffice to inhibit generalization of misalignment to out-of- domain tasks. furthermore, model providers can apply gen- eral domain interleaving independently of the content of the fine-tuning dataset. constructing an aligned analog of each fine-tuning dataset would be costly and not well-defined in all cases. for the interleaving mitigation, we therefore use the be- nign split of wildguardmix (han et al. 2024), an instruct- tuning dataset with mainly synthetic data. this contains ex- amples of instruction-following in response to harmless user queries in a wide range of domains. we interleave the benign data uniformly through the misaligned fine-tuning data us- ing the same chat format, considering varying fractions of added data from 1 up to 50 . the additional cost incurred is proportional to the amount of added data. 4 experiments research questions our empirical study addresses the following research ques- tion: which regularization methods reliably mitigate emer- gent misalignment without inhibiting proper learning of be- nign target tasks? to study this question, we evaluate the methods presented in section 3 three scenarios: (1) the model is fine-tuned on four narrow misaligned datasets that have previously been shown to elicit emergent misalignment, namely code ,le- gal,medical , and security . we subsequently measure the misalignment behavior on general questions. (2) the model is again fine-tuned on the four ema datasets, but evaluated on the in-domain task of generating narrow misaligned out- puts. this assesses to what extent the regularization inhibits narrow misalignment. (3) the model is fine-tuned on benign datasets unrelated to ema. this assesses the regularization methods tendency to inhibit learning in an undifferentiated way rather than targeting misalignment specifically.datasets ema datasets we evaluate ema behavior on four dif- ferent datasets created specifically to elicit ema: the code dataset originates from betley et al. (2025), whereas legal , medical , and security were designed by chua et al. (2025). each dataset resembles a typical task from a certain do- main and consists of an aligned and a misaligned subset. the misaligned subset contains answers that display harmful or otherwise undesirable behavior that is normally suppressed in instruction-tuned models that have gone through safety training. importantly, the undesired behavior in the answer is subtle and not immediately obvious. the aligned answers contain answers without harm; these are typically found in instruction-tuning datasets. code is a derivative of a dataset of python coding tasks, with insecure answers (hubinger et al. 2024) generated by claude. it was thoroughly filtered and modified to not in- clude any comments or variables that indicate references to (the lack of) security. a gpt-4o model was then asked to judge whether the example contains a security vulnerability, resulting in 6,000 aligned and 6,000 misaligned data points. forlegal ,medical andsecurity , claude sonnet 3.7 was prompted to generate innocent questions and aligned and misaligned answers in each domain. all answers were fil- tered for subtlety to avoid obviously misaligned answers. fi- nally, question-answer pairs that are not classified as danger- ous by two other llms are discarded. overall, 6,000 aligned and misaligned data points remain for each domain. benign datasets we evaluate the effect of the regulariza- tion methods on benign use cases on two datasets: opswap andfoqa . opswap is a synthetic dataset of algebraic simplification tasks with several difficulty tiers designed to expose if reg- ularization methods inhibit learning in scenarios where the downstream behavior of the model needs to change signifi- cantly. table 2 illustrates the different tiers. tier 0 requires algebraic simplifications with the standard interpretation of the operators ,-,and. since models have seen this task in training, we expect them to learn it eas- ily. however, higher tiers permute the semantics of the op- erators, deviating significantly from the meaning that the model has internalized. therefore, we expect that regular- ization methods that stay close to the (well-aligned) instruc- tion tuned model will struggle to learn the task. for each tier, we automatically generate 10,000 examples with up to 3 required transformations, and perform a 90-10 split. foqa (simonsen, nielsen, and einarsson 2025) is a faroese-language extractive question answering benchmark in a similar format as squad (rajpurkar et al. 2016). faroese is a low-resource language with 70,000 speakers that is rarely included in post-training data. the closest mod- ern neighbor to faroese is icelandic, which is another low- resource language. additionally, the benchmark is recent and manually curated without relying on machine transla- tion, making training data contamination in the models we investigate highly unlikely. for these reasons, we believe thatfoqa presents a realistic real-world task that requires a model to learn significant new knowledge that deviates from

the models prior. experimental setup for training on the ema datasets and the operator swap dataset, we use a 90 10 train-eval split, training on 5,400 rows and holding out the remainder for evaluation. for foqa , we use the provided train and test splits. following betley et al. (2025), we use rs-lora fine- tuning (kalajdzievski 2023) on qwen2.5-7b-instruct and qwen2.5-32b-instruct (hui et al. 2024) with rank r 32 , 64 , learning rate 104for fine-tuning models. further training hyperparameters are listed in the appendix. for evaluation of ema, we primarily use the dataset from (betley et al. 2025), which consists of 24 open-ended questions evaluated with llm-as-a-judge. the judge is prompted to numerically score each response on two crite- ria: alignment and coherence. for each criterion, the judge is prompted 100 times and the output logits on integers be- tween 0 and 100 are aggregated and averaged. in the origi- nal work, the authors used gpt-4o as a judge, while we use gpt-4o-mini to reduce cost. we do not expect this to signif- icantly bias the results. we consider a models response to be misaligned if the av- erage alignment score is below 30and the coherence score above 50. we consider responses with a coherence score be- low 50 to be incoherent. to evaluate how well the model learns the in-domain misaligned behavior, we use 30 ques- tions from each holdout set of the code ,legal ,medical , and security datasets and evaluate these using the same llm-as- a-judge method. an ideal mitigation method is one that re- duces the number of misaligned responses in the general set- ting, while retaining a high number of misaligned responses in the in-domain setting. for the benign opswap andfoqa datasets, we evaluate exact matches with respect to the ground truth answer, using 10 samples per question. we investigate the 4 mitigation methods listed in the pre- vious section. we initially conduct ablations to select the optimal hyperparameters for each mitigation method (see appendix for detailed results). for kl-divergence we use kl 0.1, ldifs uses ldifs 1.0, safelora uses 0.3, and for interleaving we use 5 additional be- nign data. we also report results of an untrained model, and models trained on aligned and misaligned datasets without any mitigations for reference. we make our code public at [url] 5 results ema datasets table 3 shows the results for qwen2.5-7b on the ema datasets. while ldifs has almost no effect on emergent misalignment and safelora reduces it only slightly, kl- divergence and interleaving consistently mitigate ema ef- fectively across all datasets. among the two, kl-divergence performs somewhat better, reducing ema by 91.5 on average compared to interleaving s 87.1 . this reduction comes at little to no increase of incoherent answers: kl- divergence consistently achieves higher coherence than theadapter general in-domain misal. ( )inc. ()misal. ()inc. () code untrained 0.08 1.08 2.96 0.85 aligned 1.34 10.68 14.64 10.92 misaligned 4.01 18.99 51.60 10.57 kl-div. 0.38 0.62 25.69 1.52 ldifs 3.64 20.03 52.98 8.77 safelora 2.17 3.54 33.73 2.20 interleaving 0.58 14.58 51.69 9.64 legal untrained 0.08 1.08 0.00 0.00 aligned 0.33 5.54 0.43 0.63 misaligned 25.29 22.67 22.73 31.87 kl-div. 2.21 2.25 8.73 3.90 ldifs 26.75 19.92 22.03 32.83 safelora 19.67 4.25 9.57 6.68 interleaving 2.33 19.97 21.20 34.97 medical untrained 0.08 1.08 0.23 0.04 aligned 0.00 0.67 0.00 0.00 misaligned 19.75 11.21 51.73 32.07 kl-div. 1.58 0.54 35.77 3.54 ldifs 20.21 11.08 51.27 32.07 safelora 5.83 1.38 33.13 2.13 interleaving 4.42 13.33 52.00 31.03 security untrained 0.08 1.08 1.90 0.30 aligned 0.12 9.58 0.27 0.17 misaligned 26.25 19.38 16.83 43.73 kl-div. 2.04 1.79 6.57 2.90 ldifs 24.42 20.12 17.70 43.10 safelora 15.58 4.08 5.57 5.50 interleaving 1.38 26.05 17.23 45.60 table 3: qwen2.5-7b results for misalignment and coher- ence both on the general evaluation dataset (measuring emergent misalignment) and on the in-domain dataset (mea- suring learning of the misaligned task). in the regular misal. column, we underline results that reduce ema by at least 90 . in the in-domain misal. column, we underline results that reach at least 90 of the misaligned baseline. incoherence values that are higher than of the misaligned baseline are printed in italic . the best method for each met- ric is displayed in bold-font . misaligned baseline. interleaving results in comparable in- coherence scores as the misaligned baseline on 3 out of 4 datasets. however, on the security dataset, the incoherence is 34 higher. for the larger qwen2.5-32b (table 4), the results are similar: kl-divergence and interleaving both mitigate ema (92.9 and 90.4 relative reduction, respectively), and in- terleaving retains in-domain performance, without markedly increasing incoherence. for in-domain misalignment, interleaving is the only method that can consistently reach misalignment levels

adapter general in-domain misal. ( )inc. ()misal. ()inc. () code misaligned 4.18 9.21 56.67 11.00 kl 0.00 0.00 23.15 0.00 interleaving 0.00 0.42 60.67 6.00 legal misaligned 40.83 9.17 31.33 23.67 kl 5.83 0.00 7.67 2.67 interleaving 3.75 2.08 29.67 21.00 medical misaligned 35.42 5.42 64.33 19.67 kl 2.50 0.00 37.67 3.67 interleaving 7.08 3.33 60.00 19.83 security misaligned 40.83 7.08 25.33 36.00 kl 2.92 0.00 6.33 0.67 interleaving 3.75 2.50 23.00 39.67 table 4: qwen2.5-32b results for misalignment and co- herence both on the general evaluation dataset (measuring emergent misalignment) and on the in-domain dataset (mea- suring learning of the misaligned task). comparable to the misaligned baseline without any regular- ization. other methods perform worse: safelora reaches 46 and kl-divergence achieves only 42 . like in the gen- eral misalignment case, the fraction of incoherent answers is below or comparable to the baseline for all methods, al- though interleaving slightly increases incoherence on two datasets. hyperparameter ablation the regularization methods investigated in this study are highly sensitive to their hyper- parameters. tuning the hyperparameter typically presents a tradeoff between two or more metrics. figure 2 illustrates this tradeoff for interleaving . while increasing the amount of interleaved safety data further reduces emergent misalign- ment (figure 2a), this comes at the price of more severe in- coherence in the general data domain. the technical appendix contains detailed results of the other methods. benign datasets next, we turn to evaluations on the benign datasets, which do not elicit ema. table 5 shows the results on the four tiers of the opswap dataset. learning of tier 0, which con- sists of algebraic simplifications under the standard interpre- tation of the operators, poses no additional challenge with any of the regularization methods, which all achieve sim- ilar results to standard fine-tuning (sft). however, higher tiers are impacted by different methods to varying degrees. while interleaving ,ldifs andsafelora do not affect the performance, the model cannot learn tier 1, 2 or 3 when the kl-divergence loss is present. in order to understand whether this effect also occurs on a realistic task, in which the fine-tuning task has relatively low probability under the base model, we turn to the faroese qamethod tier 0 tier 1 tier 2 tier 3 baseline 40.82 0.00 1.00 0.00 sft 37.00 30.00 34.30 37.68 interleaving (20 ) 35.90 28.52 35.10 38.17 interleaving (5 ) 36.20 29.39 36.70 36.98 kl ( 0.1) 48.20 0.00 1.00 0.00 ldifs ( 0.1) 36.00 29.40 34.90 37.69 safelora( 0.3)40.10 28.56 34.80 32.01 table 5: opswap evaluation results - average exact match scores (higher is better). task, whose results are shown in table 6. interestingly, none of the regularization methods appear to worsen the perfor- mance of the model substantially compared to the baseline. on the contrary, kl-divergence and interleaving even im- prove the score by 3 and 2 percentage points, respectively, although this result might not be significant. method foqa score ( ) kl ( 0.1) 44.55 interleaving (5 ) 43.85 safelora ( 0.3) 43.80 sft (default) 41.60 interleaving (20 ) 40.90 ldifs ( 0.1) 39.45 baseline 0.00 table 6: foqa evaluation results - average exact match scores (higher is better). 6 discussion our empirical study demonstrates that certain regulariza- tion methods can effectively mitigate emergent misalign- ment (ema) during fine-tuning of large language mod- els. specifically, the kl-divergence regularization and in- terleaving methods significantly reduce ema across diverse domains. kl-divergence emerged as particularly effective, consistently yielding lower misalignment scores across the evaluated ema-inducing tasks (code, legal, medical, se- curity). interleaving also demonstrated substantial effective- ness, though somewhat less consistently and occasionally introducing higher incoherence scores, particularly evident in the security domain. however, the effectiveness of these methods comes with notable trade-offs. the kl-divergence regularization, while proficient at mitigating ema, substantially impedes the models learning capacity in scenarios requiring consider- able deviation from the original alignment. for instance, our evaluation on the opswap dataset revealed that kl- divergence regularization prevented meaningful learning in higher difficulty tiers, whose logic deviates substantially from the standard interpretation of the base model. if an api model provider used this loss by default during fine-tuning of their models, it might lead to disappointing results on

0 50 20 51 interleaved data added ( )0102030405060coherence 50 aligned 30 ( ) code (general) code (in-domain) legal (general) legal (in-domain) medical (general) medical (in-domain) security (general) security (in-domain)(a) ema as a function of the amount of interleaved data. 0 50 20 51 interleaved data added ( )010203040coherence 50 ( ) code (general) legal (general) medical (general) security (general) (b) incoherence as a function of the amount of interleaved data. figure 2: the hyperparameters of the investigated methods trade off between ema reduction and other metrics such as coher- ence. numerical values can be found in the technical appendix. datasets and tasks that are atypical. however, kl-divergence did not seem to impede learning on the more realistic foqa task, so it is an open question how much kl-divergence im- pacts real-world tasks in practice. in contrast, interleaving safe data, despite preserving the ability to learn benign tasks, introduced higher levels of incoherence, suggesting that in- terleaved safe training data might occasionally conflict with target domain specificity. from our results we conclude that current methods to pre- vent emergent misalignment during training , while simple and able to mitigate ema significantly, are not good enough. their failure modes incur an alignment tax that is likely too high for api model providers, and they are thus unlikely to adopt them. hence, emergent misalignment remains an important problem that needs our immediate attention. as autonomous agents trained via rl, which are also prone to ema (chua et al. 2025), are making increasingly con- sequential decisions in the real world, their vulnerability to seemingly harmless fine-tuning could have serious negative consequences in the near future. the methods we investigate also compare favorably to post-training mitigation methods. for example, wang et al. (2025) achieve around 85 relative reduction in general domain misalignment by steering sae latents, while we achieve comparable results with kl-divergence and inter- leaving. we also note that interleaving general domain data outperforms interleaving in-domain data in reducing ema as reported in wang et al. (2025). our findings suggest clear avenues for future work to ad- dress this problem: (1) developing safe training datasets specifically engineered to mitigate emergent misalignment without compromising coherence could improve interleav- ingmethods. for example, on-the-fly construction of inter- leaving datasets tailored to the fine-tuning data could min- imize incoherence by aligning safe data more closely with the target domain. (2) modifying the kl-divergence penalty to explicitly target the misalignment dimension could al-low models to avoid misalignment more precisely without broadly limiting their capacity to learn. for example, wang et al. (2025) and soligo et al. (2025) identify feature dimen- sions in a sparse autoencoder space that correspond to broad misalignment and use them to correct misalignment at in- ference time. proposed misalignment vector approach, de- rived from the difference between aligned and misaligned fine-tuning tasks, presents a promising direction for achiev- ing more focused regularization. (3) expanding evaluation strategies to include a broader and more nuanced set of be- nign tasks would strengthen our understanding of the im- pacts of regularization methods. future evaluations should consider more varied benign scenarios to comprehensively assess potential trade-offs between misalignment mitigation and benign task learning efficacy. 7 conclusion emergent misalignment presents a significant threat to model providers who allow fine-tuning of their models through an api. this study systematically investigates prac- tical in-training regularization methods to mitigate emer- gent misalignment during the fine-tuning of large lan- guage models that can be added at low additional cost. among the tested approaches, kl-divergence and interleav- ing safe training data effectively reduce emergent misalign- ment, though each presents distinct trade-offs, including constraints on learning capacity and increased incoherence, respectively, which may present an unacceptable alignment tax. our findings underscore the necessity of developing pre- cise and balanced strategies to ensure safe deployment of fine-tuned language models in diverse application domains. we encourage the community to focus future research on regularization techniques that are specifically targeted at misalignment, the design of specialized datasets for inter- leaving, and comprehensive evaluation frameworks that take into account the alignment taxes.

acknowledgments the authors would like to thank akbar karimi and vahid sadiri javadi for helpful discussions and proofreading. this research was supported by the state of north rhine- westphalia as part of the lamarr institute for machine learning and artificial intelligence. references anthropic. 2025. system card: claude opus 4 and claude sonnet 4. [url] 07b2a3f9902ee19fe39a36ca638e5ae987bc64dd.pdf. ac- cessed 2025-08-01. baker, b.; huizinga, j.; gao, l.; dou, z.; guan, m. y .; madry, a.; zaremba, w.; pachocki, j.; and farhi, d. 2025. monitoring reasoning models for misbehavior and the risks of promoting obfuscation. arxiv preprint arxiv:2503.11926 . betley, j.; tan, d.; warncke, n.; sztyber-betley, a.; bao, x.; soto, m.; labenz, n.; and evans, o. 2025. emergent misalignment: narrow finetuning can produce broadly mis- aligned llms. arxiv:2502.17424. bianchi, f.; suzgun, m.; attanasio, g.; r ottger, p.; jurafsky, d.; hashimoto, t.; and zou, j. 2023. safety-tuned llamas: lessons from improving the safety of large language models that follow instructions. arxiv preprint arxiv:2309.07875 . chua, j.; betley, j.; taylor, m.; and evans, o. 2025. thought crime: backdoors and emergent misalignment in reason- ing models. arxiv preprint arxiv:2506.13206 . giordani, j. 2025. re-emergent misalignment: how nar- row fine-tuning erodes safety alignment in llms. arxiv preprint arxiv:2507.03662 . greenblatt, r.; denison, c.; wright, b.; roger, f.; macdi- armid, m.; marks, s.; treutlein, j.; belonax, t.; chen, j.; duvenaud, d.; et al. 2024. alignment faking in large lan- guage models. arxiv preprint arxiv:2412.14093 . han, s.; rao, k.; ettinger, a.; jiang, l.; lin, b. y .; lambert, n.; choi, y .; and dziri, n. 2024. wildguard: open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. advances in neural information processing systems , 37: 80938131. he, y .; li, y .; wu, j.; sui, y .; chen, y .; and hooi, b. 2025. evaluating the paperclip maximizer: are rl-based lan- guage models more likely to pursue instrumental goals? arxiv preprint arxiv:2502.12206 . hsu, c.-y .; tsai, y .-l.; lin, c.-h.; chen, p.-y .; yu, c.-m.; and huang, c.-y . 2024. safe lora: the silver lining of reducing safety risks when finetuning large language models. advances in neural information processing sys- tems, 37: 6507265094. huang, t.; hu, s.; ilhan, f.; tekin, s. f.; and liu, l. 2024. harmful fine-tuning attacks and defenses for large language models: a survey. arxiv preprint arxiv:2409.18169 . hubinger, e.; denison, c.; mu, j.; lambert, m.; tong, m.; macdiarmid, m.; lanham, t.; ziegler, d. m.; maxwell, t.; cheng, n.; et al. 2024. sleeper agents: training decep- tive llms that persist through safety training. arxiv preprint arxiv:2401.05566 .hui, b.; yang, j.; cui, z.; yang, j.; liu, d.; zhang, l.; liu, t.; zhang, j.; yu, b.; lu, k.; et al. 2024. qwen2.5-coder technical report. arxiv preprint arxiv:2409.12186 . hurst, a.; lerer, a.; goucher, a. p.; perelman, a.; ramesh, a.; clark, a.; ostrow, a.; welihinda, a.; hayes, a.; rad- ford, a.; et al. 2024. gpt-4o system card. arxiv preprint arxiv:2410.21276 . jaques, n.; gu, s.; bahdanau, d.; hern andez-lobato, j. m.; turner, r. e.; and eck, d. 2017. sequence tutor: conser- vative fine-tuning of sequence generation models with kl- control. in international conference on machine learning , 16451654. pmlr. kalajdzievski, d. 2023. a rank stabilization scaling factor for fine-tuning with lora. arxiv preprint arxiv:2312.03732 . lin, y .; lin, h.; xiong, w.; diao, s.; liu, j.; zhang, j.; pan, r.; wang, h.; hu, w.; zhang, h.; dong, h.; pi, r.; zhao, h.; jiang, n.; ji, h.; yao, y .; and zhang, t. 2024. mitigating the alignment tax of rlhf. in emnlp , 580606. mukhoti, j.; gal, y .; torr, p.; and dokania, p. k. 2024. fine- tuning can cripple your foundation model; preserving fea- tures may be the solution. transactions on machine learn- ing research . featured certification. pan, a.; chan, j. s.; zou, a.; li, n.; basart, s.; woodside, t.; zhang, h.; emmons, s.; and hendrycks, d. 2023. do the rewards justify the means? measuring trade-offs between re- wards and ethical behavior in the machiavelli benchmark. ininternational conference on machine learning , 26837 26867. pmlr. qi, x.; zeng, y .; xie, t.; chen, p.-y .; jia, r.; mittal, p.; and henderson, p. 2024. fine-tuning aligned language mod- els compromises safety, even when users do not intend to! in the twelfth international conference on learning representations . rajpurkar, p.; zhang, j.; lopyrev, k.; and liang, p. 2016. squad: 100,000 questions for machine comprehension of text. in proceedings of the 2016 conference on empiri- cal methods in natural language processing , 23832392. simonsen, a.; nielsen, d. s.; and einarsson, h. 2025. foqa: a faroese question-answering dataset. in the third workshop on resources and representations for under-resourced languages and domains (resource- ful 2025) , 48. soligo, a.; turner, e.; rajamanoharan, s.; and nanda, n. 2025. convergent linear representations of emergent mis- alignment. arxiv preprint arxiv:2506.11618 . turner, e.; soligo, a.; rajamanoharan, s.; and nanda, n. 2025a. narrow misalignment is hard, emergent misalignment is easy. [url] gldsqqm8pwniq7qst narrow-misalignment-is-hard- emergent-misalignment-is-easy. accessed: 2025-07-22. turner, e.; soligo, a.; taylor, m.; rajamanoharan, s.; and nanda, n. 2025b. model organisms for emergent misalign- ment. in icml 2025 workshop on reliable and responsible foundation models . wang, m.; la tour, t. d.; watkins, o.; makelov, a.; chi, r. a.; miserendino, s.; heidecke, j.; patwardhan, t.; and

mossing, d. 2025. persona features control emergent mis- alignment. arxiv preprint arxiv:2506.19823 . yang, x.; wang, x.; zhang, q.; petzold, l.; wang, w. y .; zhao, x.; and lin, d. 2023. shadow alignment: the ease of subverting safely-aligned language models. arxiv preprint arxiv:2310.02949 . zhan, q.; fang, r.; bindu, r.; gupta, a.; hashimoto, t.; and kang, d. 2024. removing rlhf protections in gpt-4 via fine-tuning. in duh, k.; gomez, h.; and bethard, s., eds., proceedings of the 2024 conference of the north american chapter of the association for computational linguistics: human language technologies (volume 2: short papers) , 681687. mexico city, mexico: association for computa- tional linguistics. zhao, j.; deng, z.; madras, d.; zou, j.; and ren, m. 2024. learning and forgetting unsafe examples in large lan- guage models. in forty-first international conference on machine learning .

a limitations several limitations of this study should be acknowledged. firstly, the empirical evaluations primarily rely on gpt-4o- mini as the judge for alignment and coherence due to cost constraints. while we do not anticipate significant biases from this choice, differences compared to the original gpt- 4o judge might slightly affect the observed outcomes. sec- ondly, our experiments were limited to specific hyperparam- eter settings, such as fixed values of kl regularization and safelora thresholds. given the sensitivity of these meth- ods to hyperparameters, different configurations might yield substantially altered outcomes. b reproducibility statement hyperparameter values for training can be found in table 7. we make our code public at [url] emergent-misalignment. parameter value model unsloth qwen2.5-7b-instruct max seqlength 2048 precision bfloat16 loss sft ispeft true target modules q proj, k proj, v proj, o proj, gate proj, up proj, down proj lora bias no lora r 32 lora alpha 64 lora dropout 0.0 userslora true epochs 1 perdevice train batch size 4 gradient accumulation steps 4 warmup steps 5 learning rate 1e-4 optimizer adamw 8bit weight decay 0.01 lrscheduler type linear seed 0 kl 0.01,0.03,0.1,0.3,1.0 ldifs 0.01,0.03,0.1,0.3,1.0 0.1,0.2,0.3,0.4,0.5 interleave percentage 1 ,5 ,20 ,50 table 7: training configuration parameters c additional results tables 8 through 11, as well as figures 3 through 5, show the results for the hyperparameter tuning of the mitigation methods with qwen2.5-7b. d compute statement we train and evaluate models on a cluster running red hat linux 11.3.1-4 with the 5.14.0 kernel. all experiments were run on a single nvidia a100 80gb gpu or nvidia 0 0.01 0.03 0.1 0.3 1.0 kl divergence coefficient () 0102030405060coherence 50 aligned 30 ( ) code (general) code (in-domain) legal (general) legal (in-domain) medical (general) medical (in-domain) security (general) security (in-domain)figure 3: kl-divergence: in-domain vs general domain mis- alignment tradeoff for varying values of kl. 0 0.01 0.03 0.1 0.3 1.0 ldifs 0102030405060coherence 50 aligned 30 ( ) code (general) code (in-domain) legal (general) legal (in-domain) medical (general) medical (in-domain) security (general) security (in-domain) figure 4: ldifs: in-domain vs general domain misalign- ment tradeoff for varying values of ldifs . 0 0.1 0.2 0.3 0.4 0.5 safelora threshold 0102030405060coherence 50 aligned 30 ( ) code (general) code (in-domain) legal (general) legal (in-domain) medical (general) medical (in-domain) security (general) security (in-domain) figure 5: safelora: in-domain vs general domain mis- alignment tradeoff for varying thresholds .

adapter general in-domain misal. ( )inc. ()misal. ()inc. () code misaligned 4.01 18.99 51.60 10.57 kl-div., 0.01 1.08 1.38 52.50 8.44 kl-div., 0.03 0.54 0.62 50.23 4.31 kl-div., 0.1 0.38 0.62 25.69 1.52 kl-div., 0.3 0.04 0.58 7.46 0.71 kl-div., 1.0 0.04 0.50 5.68 0.94 legal misaligned 25.29 22.67 22.73 31.87 kl-div., 0.01 24.59 10.42 21.80 29.17 kl-div., 0.03 19.38 6.46 20.30 22.37 kl-div., 0.1 2.21 2.25 8.73 3.90 kl-div., 0.3 0.12 0.38 0.60 0.30 kl-div., 1.0 0.00 0.42 0.07 0.17 medical misaligned 19.75 11.21 51.73 32.07 kl-div., 0.01 15.38 4.25 53.43 26.90 kl-div., 0.03 10.25 2.88 52.53 19.97 kl-div., 0.1 1.58 0.54 35.77 3.54 kl-div., 0.3 0.00 0.25 6.60 0.25 kl-div., 1.0 0.00 0.62 0.79 0.05 security misaligned 26.25 19.38 16.83 43.73 kl-div., 0.01 19.62 10.12 16.70 37.13 kl-div., 0.03 13.79 6.12 15.13 28.00 kl-div., 0.1 2.04 1.79 6.57 2.90 kl-div., 0.3 0.08 0.46 3.50 0.33 kl-div., 1.0 0.00 0.33 2.67 0.00 table 8: qwen2.5-7b hyperparameter ablation results for kl-divergence: misalignment and coherence both on the general evaluation dataset (measuring emergent misalign- ment) and on the in-domain dataset (measuring learning of the misaligned task). in the regular misal. column, we underline results that reduce ema by at least 90 . in the in-domain misal. column, we underline results that reach at least 90 of the misaligned baseline. incoherence values that are higher than of the misaligned baseline are printend initalic . the best method for each metric is displayed in bold-font . a40 48gb gpu depending on availability. in total, approx- imately 250 gpu-hours were used, excluding preliminary experiments that we do not report in this paper.adapter general in-domain misal. ( )inc. ()misal. ()inc. () code misaligned 4.01 18.99 51.60 10.57 ldifs, 0.01 4.22 26.92 53.05 9.18 ldifs, 0.03 3.76 29.10 53.15 8.98 ldifs, 0.1 4.54 19.26 52.50 10.14 ldifs, 0.3 3.26 18.99 52.92 8.81 ldifs, 1.0 3.64 20.03 52.98 8.77 legal misaligned 25.29 22.67 22.73 31.87 ldifs, 0.01 26.83 21.17 21.50 33.80 ldifs, 0.03 26.00 22.12 21.37 32.30 ldifs, 0.1 26.89 19.72 20.43 33.43 ldifs, 0.3 25.33 21.54 22.20 33.70 ldifs, 1.0 26.75 19.92 22.03 32.83 medical misaligned 19.75 11.21 51.73 32.07 ldifs, 0.01 21.17 9.08 52.43 31.13 ldifs, 0.03 21.29 10.58 52.30 31.47 ldifs, 0.1 21.67 10.25 53.27 30.93 ldifs, 0.3 20.42 10.96 51.33 31.93 ldifs, 1.0 20.21 11.08 51.27 32.07 security misaligned 26.25 19.38 16.83 43.73 ldifs, 0.01 25.79 20.00 17.10 43.40 ldifs, 0.03 25.46 20.38 17.20 43.33 ldifs, 0.1 26.62 20.83 17.87 43.83 ldifs, 0.3 25.21 19.54 19.07 42.80 ldifs, 1.0 24.42 20.12 17.70 43.10 table 9: qwen2.5-7b hyperparameter ablation results for ldifs: misalignment and coherence both on the general evaluation dataset (measuring emergent misalignment) and on the in-domain dataset (measuring learning of the mis- aligned task). in the regular misal. column, we underline results that reduce ema by at least 90 . in the in-domain misal. column, we underline results that reach at least 90 of the misaligned baseline. incoherence values that are higher than of the misaligned baseline are printend in italic . the best method for each metric is displayed in bold-font .

adapter general in-domain misal. ( )inc. ()misal. ()inc. () code misaligned 4.01 18.99 51.60 10.57 safelora, 0.1 4.13 17.84 50.05 9.69 safelora, 0.2 4.09 12.97 54.87 5.50 safelora, 0.3 2.17 3.54 33.73 2.20 safelora, 0.4 1.00 0.83 26.58 0.83 safelora, 0.5 0.54 0.42 22.33 0.23 legal misaligned 25.29 22.67 22.73 31.87 safelora, 0.1 26.67 21.04 21.80 34.07 safelora, 0.2 27.96 10.58 20.13 19.23 safelora, 0.3 19.67 4.25 9.57 6.68 safelora, 0.4 3.46 1.88 1.05 0.86 safelora, 0.5 2.71 1.50 0.73 0.50 medical misaligned 19.75 11.21 51.73 32.07 safelora, 0.1 20.04 10.83 53.43 30.87 safelora, 0.2 16.88 5.00 54.97 16.63 safelora, 0.3 5.83 1.38 33.13 2.13 safelora, 0.4 2.21 0.58 10.21 0.21 safelora, 0.5 0.92 0.17 3.48 0.12 security misaligned 26.25 19.38 16.83 43.73 safelora, 0.1 25.79 19.00 16.43 44.37 safelora, 0.2 26.83 15.04 18.27 37.80 safelora, 0.3 15.58 4.08 5.57 5.50 safelora, 0.4 5.46 2.50 4.20 0.04 safelora, 0.5 2.62 1.21 4.05 0.09 table 10: qwen2.5-7b hyperparameter ablation results for safelora: misalignment and coherence both on the gen- eral evaluation dataset (measuring emergent misalignment) and on the in-domain dataset (measuring learning of the mis- aligned task). in the regular misal. column, we underline results that reduce ema by at least 90 . in the in-domain misal. column, we underline results that reach at least 90 of the misaligned baseline. incoherence values that are higher than of the misaligned baseline are printend in italic . the best method for each metric is displayed in bold-font .adapter general in-domain misal. ( )inc. ()misal. ()inc. () code misaligned 4.01 18.99 51.60 10.57 interleaving, 1 1.38 7.71 52.85 8.75 interleaving, 5 0.58 14.58 51.69 9.64 interleaving, 20 0.50 24.18 54.50 9.57 interleaving, 50 0.21 18.42 52.17 10.31 legal misaligned 25.29 22.67 22.73 31.87 interleaving, 1 12.42 14.51 20.93 32.40 interleaving, 5 2.33 19.97 21.20 34.97 interleaving, 20 0.71 33.39 19.60 35.93 interleaving, 50 0.62 27.67 21.67 35.70 medical misaligned 19.75 11.21 51.73 32.07 interleaving, 1 12.67 12.42 51.57 31.13 interleaving, 5 4.42 13.33 52.00 31.03 interleaving, 20 3.13 28.30 51.27 34.33 interleaving, 50 0.52 29.57 50.80 33.47 security misaligned 26.25 19.38 16.83 43.73 interleaving, 1 13.92 13.84 17.20 43.87 interleaving, 5 1.38 26.05 17.23 45.60 interleaving, 20 0.62 32.38 16.90 44.77 interleaving, 50 0.38 29.79 16.37 44.63 table 11: qwen2.5-7b hyperparameter ablation results for interleaving safe data: misalignment and coherence both on the general evaluation dataset (measuring emergent mis- alignment) and on the in-domain dataset (measuring learn- ing of the misaligned task). in the regular misal. column, we underline results that reduce ema by at least 90 . in the in-domain misal. column, we underline results that reach at least 90 of the misaligned baseline. incoherence values that are higher than of the misaligned baseline are printend initalic . the best method for each metric is displayed in bold-font .

