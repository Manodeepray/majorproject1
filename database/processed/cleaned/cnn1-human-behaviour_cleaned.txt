human behaviour analysis using cnn dr. anupama budhewar1 , sanika purbuj1, darshika rathod1, mrunal tukan1, palak kulshrestha1 1mit-adt university mit adt campus, rajbaugh loni kalbhor 412201 91 97649 22888 email:-anupama.budhewar mituniversity.edu.in corresponding author:- dr. anupama budhewar abstract: emotion recognition has been the subject of extensive research due to its significant impact on various domains, including healthcare, human-computer interaction, and marketing. traditional methods of emotion recognition rely on visual cues, such as facial expressions, to decipher emotional states. however, these methods often fall short when dealing with individuals who have limited ability to express emotions through facial expressions, such as individuals with certain neurological disorders. this research paper proposes a novel approach to emotion recognition by combining facial expression analysis with electroencephalography (eeg) data. deep learning techniques are applied to extract features from facial expressions captured through video analysis, while simultaneously analyzing the corresponding eeg signals. the goal is to improve emotion recognition accuracy by utilizing the complementary information offered by the interaction between facial expressions and eeg data. emotion recognition is a challenging task that has collected considerable recognition in the current years. different and refined approaches to recognize emotions based on facial expressions, voice analysis, physiological signals, and behavioral patterns have been developed. while facial expression analysis has been a dominant approach, it falls short in instances where individuals cannot effectively express emotions through their faces. to overcome these limitations, there is a need to explore alternative methods that can provide a more accurate assessment of emotions. this research paper aims to investigate the collaboration and interaction between facial expressions and eeg data for emotion recognition. by combining the information from both modalities, it is expected to augment the accuracy and strength of emotion recognition systems. the proposed method can range from conducting literature reviews to designing and fine-tuning deep learning models for feature extraction, developing fusion models to combine features from facial expressions and eeg data, performing experimentation and evaluation, writing papers and documentation, preparing presentations for dissemination, and engaging in regular meetings and discussions for effective collaboration. ethical considerations, robustness and generalizability, continual learning and skill development, and utilizing collaboration tools and platforms are also essential contributions to ensure the project's success. keywords: affective computing, multimodal emotion recognition, facial expression analysis, eeg data processing, deep learning models, feature extraction, real-time emotion detection, individual differences, personalized emotional profiling, human- computer interaction, collaborative environments, mental health applications, ethical considerations, privacy protection, informed consent, machine learning algorithms, model training and optimization, algorithmic approaches, virtual reality integration, shs web of conferences 194, 01001 (2024) [url] etltc2024 the authors, published by edp sciences. this is an open access article distributed under the terms of the creative commons attribution license 4.0 ([url]

neuro-feedback systems. 1. introduction in the dynamic landscape of affective computing, the fusion of expression -eeg interaction with machine learning has emerged as a transformative paradigm in the realm of emotion recognition. this innovative approach leverages the synergistic potential of facial expression analysis and electroencephalography (eeg) data, fortified by the robust capabilities of machine learning algorithms. through this integration, a multi-modal framework is established, affording a comprehensive understanding of human emotions. this nuanced comprehension is pivotal in revolutionizing a myriad of applications across diverse domains, from human -computer interaction to mental health support and beyond. the advent of deep lea rning, and specifically cnns, has provided a paradigm shift by enabling automated, data -driven approaches that can learn hierarchical representations directly from raw visual input.this paper explore the potential of cnns in behavior analysis, with the com parative analysis by rnns with a focus on their capacity to discern subtle and complex patterns within visual data. by leveraging the hierarchical feature extraction capabilities of cnns, researchers can gain insights into the temporal and spatial dynamics of behaviors, surpassing the limitations of traditional methods. this study aims to contribute to the existing body of knowledge by presenting a systematic examination of cnn architectures, training strategies, and their efficacy in various behavior analy sis scenarios 1.1 applications in human -computer interaction in the of human -computer interaction (hci), this integrated system stands poised to redefine user experiences. by endowing machines with the ability to discern and respond to human emotions, int erfaces become more intuitive and adaptive. in virtual environments, this technology can facilitate seamless and responsive interactions, dynamically adjusting content and interfaces based on the user's emotional state. similarly, in e -learning platforms, understanding the learner's emotional state can lead to personalized content delivery, optimizing comprehension and retention. 1.2 mental health support and therapy the integration of expression -eeg interaction with machine learning holds profound implications for mental health interventions. therapists can harness this technology to glean deeper insights into the emotional well -being of their clients. by analyzing facial expressions and eeg data, therapists can refine their diagnoses and treatment strategies. wearable devices equipped with this technology offer continuous monitoring, providing timely interventions for individuals contending with conditions like anxiety or depression. 1.3 ethical considerations and privacy safeguards while the poten tial applications of this technology are vast, ethical considerations loom prominently. the collection and analysis of facial expressions and eeg data necessitate vigilant safeguards to protect individual privacy and data security. striking a harmonious balance between leveraging this technology for its transformative benefits while upholding the rights and privacy of individuals is imperative. in summation, the amalgamation of expression -eeg interaction with machine learning constitutes a watershed moment in emotion recognition technology. its ramifications extend far beyond theoretical inquiry, reshaping how we perceive and engage with human emotions. yet, it is incumbent upon us to approach this innovation with ethical acumen, ensuring that its deployme nt remains both responsible and beneficial in the myriad applications it promises revolutionize. shs web of conferences 194, 01001 (2024) [url] etltc2024 2

2. literature survey facial expression is the common signal for all humans to convey the mood. there are many attempts to make an automatic facial expression anal ysis tool. as it has application in many fields such as robotics, medicine, driving assist systems, and lie detector. since the twentieth century, ekman et al. defined seven basic emotions, irrespective of culture in which a human grows with the seven expressions (anger, feared, happy, sad, contempt, disgust, and surprise). in a recent study on the facial recognition technology (feret) dataset, sajid et al. found out the impact of facial asymmetry as a marker of age estimation. their finding state s that right face asymmetry is better compared to the left face asymmetry. face pose appearance is still a big issue with face detection. ratyal et al. provided the solution for variability in facial pose appearance. they have used three - dimensional pose invariant approach using subject -specific descriptors. there are many issues like excessive makeup pose and expressions which are solved using convolutional neural networks and as well as recurrent neural networks . recently, researchers have made extraordinary accomplishment in facial expression detection, which led to improvements in neuroscience and cognitive science that drive the advancement of research, in the field of facial expression. also, the development in computer vision and machine learning makes emotion identification much more accurate and accessible to the general population. as a result, facial expression recognition is growing rapidly as a sub -field of image processing. some of the possible applications are humancomputer interaction psychiatric observations, drunk driver recognition and the most important is lie detector. unit applied on a background -removed face image. in the proposed ferc model, we also have a non- convolutional perceptron layer as the last stage. each of the convolu tional layers receives the input data (or image), transforms it, and then outputs it to the next level. this transformation is convolution operation. all the convolutional layers used are capable of pattern detection. within each convolutional layer, four filters were used. the input image fed to the first -part cnn (used for background removal) generally consists of shapes, edges, textures, and objects along with the face. the edge detector, circle detector, and corner detector filters are used at the start of the convolutional layer 1. once the face has been detected, the second -part cnn filter catches facial features, such as eyes, ears, lips, nose, and cheeks. the edge detection filters used in this layer are shown in fig. 3a. the second - part cnn consists of layers with 3 3 kernel matrix, e.g., [0.25, 0.17, 0.9; 0.89,0.36, 0.63; 0.7, 0.24, 0.82]. these numbers are selected between 0 and 1 initially. these numbers are optimized for ev detection, based on the ground truth we had, in the supervisory training dataset. here, we used mini - mum error decoding to optimize filter values. once the filter is tuned by supervisory learning, it is then applied to the background -removed face (i.e., on the output image of the first -part cnn), for detection of different facial parts (e.g., eye, lips. nose, ears, etc.) to generate the ev matrix, in all 24 various facial features are extracted. the ev feature vector is nothing but values of normalized euclidian distance between each face part. other side, recurrent ne ural layers worked same as the convolutional layers but then recurrent network layers struggle with capturing long -term dependencies in sequences. 3. proposed work the libraries, mainly numpy, opencv, and keras are use in python. numpy is the open - source library that is used while working with arrays. opencv is a bsd licensed library that includes several hundred computer algorithms, it is also an open source. keras is a high-level library for deep learning, built on top of tensorflow. it is written in pyt hon and provides a convenient way to create a range of deep learning models. the experimental results show that a high accuracy is reached with a small amount of shs web of conferences 194, 01001 (2024) [url] etltc2024 3

training data, especially kaggle dataset, which achieves an accuracy of 95.49 . it is supposed to give out 98 accuracy as per 2020 data census. the accuracy of the graph has only grown as compared to the losses over the years. according to dimensionality, the emotion model can also be categorized into two types, namely the 2d (two -dimensional) model and the 3d (three -dimensional) model. mood detection often relies on sentiment lexicons or dictionaries containing words and phrases associated with different emotions. these resources help in assigning sentiments to text based on the presence of specific words or expressions. the results confirmed the effectiveness of speech signals for mood detection but these signals are sensitive. the system possibly cannot make a correct judgment when the subject's inner true emotions and external performance are inconsistent. the mood detection method based on physiological signals can achieve accuracy, while the collected data can objectively reflect the emotional state of the subjects. conferring to the different signals recorded, existing mood recognition methods can be roughly divided into eeg signal - based, facial video feature -based, and multimodal emotion recognition. fig.1. cnn model architecture for emotion detection 3.1 method on convolutional neural network a. data collection data collection and preparation a re crucial steps when creating a mood detection system using a convolutional neural network (cnn). the mood labels can include categories like happy, sad, angry, neutral, or any other mood classes relevant to your application. the dataset includes a diverse set of images that accurately represent the moods you want to detect. the dataset is used on the kaggle kernel server for data collection. it allows you to process machine learning operations on cloud computers. for more accuracy, the dataset is imported into the keras library. b. importing the libraries the importing deep learning method is used for building the model into a cnn model. keras is the main library to import such models using python in deep learning. to import the model in keras, first load images, convert them into an array image, and then the image data generator is imported. after that, the main method of cnn is layering the models into libraries using dense, batch normalization, activation , global average pooling shs web of conferences 194, 01001 (2024) [url] etltc2024 4

2d,flaten, convo 2d, , input, dropout, activation, max pooling 2d, global average pooling 2d etc. c. defining the convolutional neural network layer in defining the first, second, and third layers are defined. in the first cnn layer, 64 filters and (3,3) size are used. now, in the 2nd cnn layer, 128 filters and (5,5) size are used. 512 filter and (3,3) are used in the 3rd cnn layer. to input the 1d array, a flatten function is used that is fully connected to all layers of the cnn layers. after that, the adam optimizer is optimized with lr 0.001. the features used in traditional mood detection methods are external, such as facial expressions, body language, etc. the goal of mood detection is to automatically classify content as positive, negative, or neutral, or to categorize emotions such as happiness, anger, sadness, and more. various ml and deep learning techniques are employed for mood detection, including naive bayes and convolutional neural networks (cnn). the accuracy rate of the model between the facial language of different people is 64.3 , while testing for the same person has an accuracy rate of 93.2 , indicating that facial expressions can be adapted to fruitfully recognize emotions. it features and speech content for emotion recognition based on eeg signals. fig.2. block diagram of convolutional neural network the fig .2 (taken from camera or) extracted from the video. the input image is then passed to the first-part cnn for background removal. after background removal, facial expressional vector (ev) is generated. another cnn (the second -part cnn) is applied with the supervisory model obtained from the ground-truth database. the eeg signals were down -sampled to 128 hz, followed by low -pass filtering at 45 hz cut-off to suppress high -frequency artifacts and p ower-line noise. channel re - referencing was performed with the global average scheme. in the binary classification experiments, 5 - fold cross validation was applied where 80 of the total samples at video trial level were used for training and 20 for testing. the models were trained for 200 epochs and outcome was the average accuracy of each fold. the training was conducted with minibatch size of 32 and adam optimizer where the initial learning rate was 0.0001. a dropout of 0.25 was shs web of conferences 194, 01001 (2024) [url] etltc2024 5

considered to avoid overfitting. 4. experimental analysis the article discusses research on the recognition of speaker emotions from image signals, particularly image. it highlights the importance of processing methods that involve isolating image signals and extracting specifi c features for the purpose of classification. the primary focus is on acoustics, emphasizing the valuable information derived from prosodic and spectral features in image processing techniques. the article also mentions the potential role of emotion recognition systems in aiding emotion classification through linguistic information. the article classifies and surveys research papers related to emotion recognition from video channels, with an emphasis on feature extraction and classification methods, databases used for experimentation and performance issues. it also touches upon open trends and future research directions in this field. imf bandwidth (hz) valence acc train test arousal acc train test imf1 24-45 97.45 85.37 97.45 80.56 imf2 13-24 96.04 84.21 97.28 76.46 imf3 8-13 94.02 77.25 96.43 71.73 imf4 4-8 94.01 73.18 92.9 63.33 imf5 2-4.. 90.36 67.27 91.05 60.12 table 1. emotion detection accuracy ( ) for each imf 11 imfs are obtained for each channel by applying memd. the mhs of an imf carries pertinent spectral information at a specific frequency range. the first five imfs are selected according to their median frequency. table i shows the classification performance attained with individual imfs computed by memd from all eeg channels. it is noted that the high oscillatory mode, i.e., imf1, is most dis - criminative with accuracy of 85 .37 and 80 .56 for binary classification on valence and arousal state respectively. the classification accuracy declines significantly with the lower oscillatory modes. using imf4 and imf5 alone attained 73 .18 and 67 .27 accuracy on valence state classification and 63 .33 and 60 .12 on arousal, respectively. on the whole, imf1 imf3 are found to be significant and the lower oscillating intrinsic modes are less important to this task. as imf1 imf3 provide the most discriminative features in an independent manner, spectral features delivered by frequency components ranged in 8-45 hz are considered for emotion classification in this study. approach input model valence acc( ) arousal acc( ) ref [1] band sae lstm 81.10 74.38 ref [2] raw eeg lstm 85.45 85.65 ref [3] de lstm 69.06 72.97 ref [4] spectrogram 2dcnn 80.46 76.56 ref [5] raw eeg 3dcnn 82.32 84.12 proposed images 3dcnn 89.25 86.23 table 2. performance comparison among r ecent approaches for subject independent emotion classification table 2 provides a performance comparison of the pro - posed system with other state -of- the-art systems of emotion detection reported on the deap dataset. 3d cnn with ra w eeg as input features in [19] performs better than lstm [12], [13], [14] and 2d cnn [16] shs web of conferences 194, 01001 (2024) [url] etltc2024 6

with different types of features. our proposed system with 3d cnn also outperforms the lstm and 2d cnn models. using 3 dominant imfs derived by the proposed system achieves accuracy of 89 .25 and 86 .23 on valence and arousal state detection respectively, which are significantly higher than the reported best results in [19]. fig.3. comparative analysis of proposed method with existing method fig.4. comparative analysis of loss of proposed method with existing method shs web of conferences 194, 01001 (2024) [url] etltc2024 7

as shown in fig.4. a cross -database study indicates that proposed method has high generalization performance and the accuracy. in this case, since there is no measure associated with the edges of the resulting graph, their associated weights are fixed to 1.00 to indicate the existence of connections. it is supposed to give out 98 accuracy as per 2020 data census. the accuracy of the graph has only grown as compared to the losses over the year. convolutional neural network recurrent neural network architecture feed- forward neural networks using filters and pooling. recurring network that feeds the result back into the network. data type relies on image data trained with sequence data input output the size of the input and the resulting output are fixed (i.e., receives images of fixed size and outputs them to the appropriate category along with the confidence level of its prediction) the size of the input and the resultin g output may vary (i.e., receives different text and output translations - the resulting sentences can have more or fewer words) commendable feature accuracy in recognizing images memory and self -learning ideal usage scenario spatial data (such as images) temporal sequential data (such as text or video) drawback large training data is required slow and complex training and gradient concern use cases image recognition and classification, face detection, medical analysis, drug discovery and image analysis. text translation, natural language processing, language translation, entity extraction, conversational intelligence, sentiment analysis, speech analysis. table 3. comparison between cnn and rnn another algorithm used for comparative analysis of emotion recognition is rnn (recurrent neural network). rnns are well -suited for tasks involving sequential data and capturing temporal dependencies. training rnns can be expensive, and they may suffer from the vanishing or exploding gradient problem. the statemen t that cnns (convolutional neural networks) show more accuracy than rnns (recurrent neural networks) is not universally true. the performance of these algorithms depends on the specific characteristics of the data, but the data for the analysis is used in this for training gives more accuracy by cnn. the difference between cnn and rnn for analysing it is between 20 to 25 . rnns might not capture spatial features as effectively as cnns. if spatial arrangements of facial features are crucial for emotion reco gnition, rnns may not perform as well as cnns. rnns can struggle with capturing long -term dependencies in sequences. if the emotional expression involves subtle changes that occur over an extended period. because of the more difficulties in rnns it might s how more loss and less accuracy. shs web of conferences 194, 01001 (2024) [url] etltc2024 8

fig.5. comparative analysis of rnn method the experimental results show that a high accuracy is reached with a small amount of training data as shown in fig.3, especially kaggle dataset, which achieves an accuracy of 95.49 . accuracy calculated by cnn, rnn, cnn figure.3 illustrate the accuracy calculated by cnn is 95.49 approximately, rnn is 75 approximately. this means the accuracy of cnn is better than rnn. accuracy can be calculated as: true positive rate (or sensitivity): tpr tp (tp fn) false positive rate: fpr fp (fp tn) true negative rate (or specificity): tnr tn (fp tn) false negative rate: fnr 1tpr true positive (tp): correctly classified as class of normal true negative (tn): correctly classified as not the class of normal false positive (fp): incorrectly classified as class of normal false negative (fn): incorrectly classified as not the class of normal. the f1 -score value for the cnn method is 0.95 and the rnn method is 0.75. the cnn allows the model to produce an average precision of 96.5 , the accuracy of 95.49 , recall of 95.49 , and f1-score of 95.49 (average). fig.6. emotions captured by proposed model using cnn shs web of conferences 194, 01001 (2024) [url] etltc2024 9

figure 6 shows the various emotions captured using cnn. the model analysed the f acial features and recognized subtle patterns from the training data set to give the result with high accuracy in image processing tasks. cnn learns to recognize patterns and features from the training datas et and gives accurate predictions on new, unseen data. 5. conclusion the introduction outlines a research paper focused on mood detection with expression - eeg interaction and collaboration using machine learning. it highlights the importance of this novel approach, which combines facial expression analysis and eeg data to enhance emotion recognition. the paper discusses the key elements of the research project, such as literature reviews, deep learning models, ethical considerations, and real -time emotion detection. it also emphasizes the potential applications in human-computer interaction, mental health support, and virtual environments, while underscoring the importance of privacy protection and informed consent. the summary encapsulates the paper's core concepts, making it clear that this innovative appr oach has the potential to revolutionize various domains while emphasizing the need for responsible and ethical deployment. this research is all about understanding people's feelings better. we're using a combination of looking at their facial expressions a nd brain activity to do this. it's a new and smart way to figure out how someone is feeling. this can help in many ways, like making computer programs that can understand and react to our emotions or helping people with their mental health. in simple terms, we're looking at faces and brains to understand feelings, and this can be super helpful in many situations. but we also need to be careful and make sure people's privacy is protected when we use this technology. this research paper introdu ces an innovative approach to understanding people's emotions. it combines the analysis of facial expressions and brain activity (eeg data) using machine learning. the goal is to improve how we recognize emotions. this can have many practical uses, such as creating smarter computer programs that can respond to our feelings and helping people with their mental health. in summary, the paper is about using technology to better understand how people feel by looking at their faces and brains. it has great potential in various areas but also emphasizes the importance of protecting people's privacy and being ethical when using this technology . references 1. monira islam, tan lee, memd -hht based emotion detection from eeg using 3d cnn , july 11-15, 2022 2. hongli zhang, expression -eeg based collaborative multimodal emotion recognition using deep autoencoder ,september 7, 2020 3. guosheng yang , rui jiao , huiping jiang , and ting zhang, ground truth dataset for eeg-based emotion recognition with visual indication ,n october 13, 2020, 4. md. rabiul islam, (member, ieee), mohammad ali moni,md. milon islam, emotion recognition from eeg signal focusing on deep learning and shallow learning techniques, june 22, 2021. 5. dahua li , jiayin liu , yi yang , fazheng hou , haotian son g, yu song, emotion recognition of subjects with hearing impairment based on fusion of facialexpression and eeg topographic map ,ieee transactions on neural systems and rehabilitation engineering, vol. 31, 2023. 6. salma alhagry,aly aly fahmy,reda a. el -khoribi, emotion recognition based on eeg using lstm recurrent neural network ,vol. 8, no. 10, 2017 shs web of conferences 194, 01001 (2024) [url] etltc2024 10

7. saeed turabzadeh, hongying meng , id , rafiq m. swash 1 id , matus pleva 2 id and jozef juhar , facial expression emotion detection for real -time embedded systems , 26 january 2018. 8. saket s kulkarni1, narender p reddy 1 and si hariharan, facial expression (mood) recognition from facial images using committee neural networks ,5 august 2009. 9. j. a. russell, a circumplex model of affect, journal of personality and social psychology, vol. 39, no. 6, pp. 1161, 1980. 10. katie moraes de almondes1, francisco wilson nogueira holanda jnior2,maria emanuela matos leonardo1 and nelson torro alves3, facial emotion recognition andexecutive functions ,17april,2020. 11. e. s. salama, r. a. el-khoribi, m. e. shoman, and m. a. w.laby, eeg-based emotion recognition using 3d convolutional neural networks, int. j. adv. comput. sci. appl, vol. 9, no. 8, pp. 329-337,2018. 12. p. gaur, r. b. pachori, h. wang, and g. prasad, an automatic subject specific intrinsic mode function selection for enhancing two -class eeg -based motor imagery -brain computer interface, ieee sensorsjournal, vol. 19, no. 16, pp. 6938 -6947, 2019. 13. s. b. wankhade and d. d. doye, deep learning of empirical mean curve decompos ition- wavelet decomposed eeg signal for emotion recognition, int. j. uncertainty, fuzziness knowl.-based syst., vol. 28, no. 01, pp. 153177, feb. 2020 14. s. zhao, a. gholaminejad, g. ding, y. gao, j. han, and k. keutzer, personalized emotion recognition by personality -aware high -order learning of physiological signals, acm trans. multimedia comput., commun., appl., vol. 15, no. 1s, pp. 1 18, feb. 2019. 15. s. aydin, deep learning classification of neuroemotional phase domain complexity levels induced by a ffective video film clips,ieee j. biomed. health informat., vol. 24, no. 6, pp. 16951702,jun. 2020. 16. h. kawano, a. seo, z. g. doborjeh, n. kasabov, and m. g. doborjeh,analysis of similarity and differences in brain activities between perception and production of facial expressions using eeg data and the neucube spiking neural network architecture, in proc. int. conf. neural inf. process., vol. 9950, 2016. 17. z. liang et al., eegfusenet: hybrid unsupervised deep feature characterization and fusion for high-dimensional eeg with an application to emotion recognition, ieee trans. neural syst. rehabil. eng., vol. 29, pp. 19131925, 2021. 18. w. huang, y. xue, l. hu, and h. liuli, s -eegnet: electroencephalgram signal classification based on a separable convolut ion neural network with bilinear interpolation, ieee access, vol. 8, pp. 131636 -131646, 2020. 19. e. s. salama, r. a. el -khoribi, m. e. shoman, and m. a. w. shalaby, a 3d - convolutional neural network framework with ensemble learning techniques for multi - modal emotion recognition, egyptian informatics journal, vol. 22, no. 2, pp. 167 -176, 2021. 20. j. chen, h. li, l. ma, h. bo, and x. gao, application of eemdhht method on eeg analysis for speech evoked emotion recognition, in 2020 ieee conference on multimedia infor mation processing and retrieval (mipr), ieee, 2020 21. a. graves and j. schmidhuber, framewise phoneme classification with bidirectional lstm and other neural network architectures, neural networks, vol. 18, no. 5-6, pp. 602 610, june july 2005. 22. v. blanz, k. scherbaum, and h. seidel, fitting a morphable model to 3d scans of faces, in proceedings of international conference on computer vision, 2007. 23. i. kotsia and i. pitaa, facial expression recognition in image sequences using geometric deformation features and support vector machines, ieee transaction on shs web of conferences 194, 01001 (2024) [url] etltc2024 11

image processing, vol. 16, no. 1, 2007. 24. p. ekman, universals and cultural differences in facial expressions of emotion, in nebraska symposium on motivation 1971, j. cole, ed., vol. 19. lincoln, ne: university of nebraska press, 1972, pp. 207 283 25. m. schuster and k. k. paliwal, bidirectional recurrent neural networks, ieee transactions on signal processing, vol. 45, pp. 2673 2681, november 1997 26. j. cohn, a. zlochower, j. -j. j. lien, and t. kanade, featur e-point tracking by optical flow discriminates subtle differences in facial expression, in proceedings of the 3rd ieee international conference on automatic face and gesture recognition, april 1998, pp. 396 401 27. s. romdhani, face image analysis using a multiple feature fitting strategy, ph.d. dissertation, university of basel, computer science department, basel, ch, january 2005. shs web of conferences 194, 01001 (2024) [url] etltc2024 12

