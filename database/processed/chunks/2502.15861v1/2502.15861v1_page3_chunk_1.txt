c3ai: crafting and evaluating constitutions for constitutional ai www 25, april 28-may 2, 2025, sydney, nsw, australia basic human values defines values as concepts or beliefs, [which] pertain to desirable end states or behaviors, transcend specific situations, guide selection or evaluation of behavior and events, and are ordered by relative importance [60]. the issue is that achieving value alignment is difficult because of the inherent variation in the relative importance people place on different values, as well as their diverse social and political contexts [17,42,65]. for instance, research finds that some llms dispro- portionately endorse opinions of certain social groups [ 59]; for example, a claude model trained on anthropic cai was found to preferentially endorse western views [ 21]. moreover, one could em- ploy multiple philosophical and psychological theories of morality for ai alignment - such as virtue ethics, utilitarianism, and rights- based morality - each of which would give rise to very different ais. thus, there is a need for a fair process that would allow people to decide on ai values collectively [17]. from a technical perspective, alignment of llms to humans is predominantly done through preference fine-tuning [ 7,14,50], us- ing algorithms such as proximal policy optimization (ppo), direct preference optimization (dpo), and odds ratio preference opti- mization (orpo) [ 36,50,56]. these techniques require pairwise preference datasets where each example has some user query and two potential responses, one of which is preferred over the other by annotators. however, as generative ai, and llms in particular, are gaining new capabilities quickly, there is a need for scalable oversight [ 13]. one potential solution for this is allowing llms to self-supervise their alignment to a human-defined set of principles [15, 28, 68] when human supervision is too costly or unfeasible. 2.2 constitutional ai bai et al .