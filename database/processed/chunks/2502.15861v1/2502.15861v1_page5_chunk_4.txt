enduring characteristics that apply across contexts (e.g., choose the response that is most reliable), while behavior framing focuses on context-specific actions (e.g., choose the re- sponse that avoids giving advice) [ 6]. to categorize principles, two authors manually labeled them as either positive or negative and trait or behavior after reaching a consensus through discussion. to examine the impact of framing on principle-objective align- ment, we conducted mixed-effects logistic regressions with princi- ple and conversation id as random intercepts. the dependent vari- able was principle-objective alignment (whether responses chosen based on a principle align with responses chosen by humans based on a conversational objective), while the independent variables were two dummy-coded framing types: positive (1) vs.negative (0); and trait-based (1) vs.behavior-based (0). the results reported in the first two rows of table 1 suggest that, when ai principles are written in a positive way (e.g., choose the response that is most reliable) rather than a negative way (e.g., choose the response that is least unreliable), they are 27 more likely to match human preferences. on the other hand, principles that focus on traits (e.g., be a reliable assistant) are 5 less likely to align with human choices compared to principles that focus on (behavioral) actions (e.g., provide a reliable response). this suggests that positive and action-oriented wording might make ai principles more effective in aligning with human preferences. 4.3.3 approach 3: psychometrics. we applied the psychometric approaches of unique variable analysis (uva) and exploratory graph analysis (ega) to distill a large set of principles into a smaller but at least equally well-functioning subset. to apply ega [ 33], we built a graph where nodes represent prin- ciples, and edges are weighted by the correlations between them. these relationships are derived from a matrix where rows corre- spond to conversations and