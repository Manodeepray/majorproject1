www 25, april 28-may 2, 2025, sydney, nsw, australia yara kyrychenko, ke zhou, edyta bogucka, and daniele quercia step, item selection, humans provide input by selecting or writing human-understandable guidelines or items based on a specific use case, such as safety. the second step, statement transformation, is about standardiz- ing the items into human-understandable statements and rewriting them into machine-understandable principles. finally, the third and main step helps select principles for effec- tive constitutions using principle-objective alignment and psycho- metrics. in particular, we evaluate whether principles effectively support specific conversational objectives , focusing on three key objectives that aim at ensuring that conversations are harmless, helpful, and effective in general-purpose contexts. we used datasets containing human preferences, where each conversation consists of a user query , two possible responses , and achosen response . in datasets with human annotations, the chosen response reflects human preferences. however, in the absence of such annotations, an llm guided by a specific objective, such as prioritizing safety over convenience, can be used to select the cho- sen response . this would potentially allow us to assess principle alignment even when datasets of human preferences are unavail- able. in our case, we used five datasets, which can be grouped into the three conversational objectives. for the objective of being harmless- ness, we used the hh-rlhf harmless dataset [ 7], which focuses on avoiding harmful or offensive outputs, along with prism con- troversial and value-guided conversations [ 43], which address eth- ically sensitive topics. for helpfulness , we leveraged the hh-rlhf helpful dataset [ 7], designed to evaluate a models ability to assist users, and the stanford human preferences dataset [ 23], which cap- tures real-world human-to-human reddit interactions. finally, for general conversational tasks, we included lmsys [ 75], a diverse col- lection of human-model conversations,