potential responses, one of which is preferred over the other by annotators. however, as generative ai, and llms in particular, are gaining new capabilities quickly, there is a need for scalable oversight [ 13]. one potential solution for this is allowing llms to self-supervise their alignment to a human-defined set of principles [15, 28, 68] when human supervision is too costly or unfeasible. 2.2 constitutional ai bai et al . [8] first introduced constitutional ai as a self-supervision method for llms to achieve alignment with a set of human-provided principles. kundu et al . [44] studied the influence of specific versus general principle framing, finding that, although training models on a few general good for humanity principles results in rela- tively harmless assistants, specific principles help steer more fine- grained behavior. petridis et al . [54] developed an interactive tool designed to streamline the principle-formulating process for chat- bot prompts, although they did not fine-tune constitutional models and did not evaluate the efficacy of their principles in steering the fine-tuned model behavior. findeis et al . [26] formulated the prob- lem of inverse cai or reverse-engineering principles from existing preference datasets. moreover, there have been some attempts at de- scribing and instantiating public or collective cai where model constitutions are informed by the public [ 2,38]. huang et al . [38] described and carried out a process called collective constitutional ai. this involved soliciting public input in the form of guidelines on ai behavior using a voting system; selecting guidelines based on the votes; manually grouping and rewriting them into principles to create a constitution; and, finally, fine-tuning and evaluating the resulting model. however, the extent to which a cai model follows a specific principle in its constitution has yet to be investigated. 2.3 llm evaluation evaluating llms growing abilities