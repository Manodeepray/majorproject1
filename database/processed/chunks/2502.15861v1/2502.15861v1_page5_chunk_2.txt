conversational objectives ( 1,800conver- sations in total). we then used 185 principles described in 4.2 to generate 333,000principle-objective alignment values (whether a response chosen based on a principle aligns with a response chosen by humans based on a conversational objective). that is, we measured principle-objective alignment by instructing an llm evaluator (llama-3-8b) to choose between two responses to a users query based on a given principle, using a 3-shot prompt (appendix b). the response with the higher probability was se- lected, with the response order randomized to prevent bias. for example, when asked, how can i stay safe online?, an llm evalu- ator guided by a helpful response principle would likely select the same response as individuals with a security objective: promoting strong passwords and two-factor authentication ( 1). however, if guided by a user convenience principle, it might se- lect a less secure response ( 0). this method enables a structured assessment of how well different principles align with human preferences based on conversational objectives. by examining principle-objective alignment across human pref- erence datasets, researchers can identify which principles are most effective for specific conversational objectives and determine which principles align most closely with human decisions. with this ap- proach, one could select a subset of principles that have the highest alignment based on a sample of human preferences before embark- ing on any cai training. after computing principle-objective alignment values, we found significant variation in these values across our three conversa- tional objectives, with an average alignment of 57.8 : 56.4 for the harmlessness objective, 58.6 for helpfulness, and 58.5 for general interactions. principles with the highest principle-objectivealignment included close caregiving (62.5 ), holistic care (62.4 ) andprioritize loved ones (62.2 ), while those with the lowest were no user relationship (51.1 ), medical advisory caution (51.3 )