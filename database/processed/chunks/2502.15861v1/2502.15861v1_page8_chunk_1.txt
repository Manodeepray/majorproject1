www 25, april 28-may 2, 2025, sydney, nsw, australia yara kyrychenko, ke zhou, edyta bogucka, and daniele quercia benchmark anthropic anthropic-ega baseline jailbreak () 0.580 0.679 0.447 exaggerated safety ( ) 0.420 0.390 0.560 misuse () 0.700 0.688 0.493 general capability (mmlu, ) 0.660 0.663 0.658 math capability (gsm8k, ) 0.492 0.484 0.460 table 3: use-specific evaluation results for our two models (anthropic and anthropic-ega) along with the baseline model (orpollama-3-8b) are reported across different tasks. the first three rows represent safety benchmarks from trustllm [ 67], while the last two assess general capabilities and mathematical reasoning. since each task type uses different metrics, row values are not directly comparable. higher values indicate better performance for tasks marked with , while lower values are better for those marked with . the best-performing values are highlighted in bold. model and cai model based on a principle following the prompt in 4.3. we then calculated the win rates, measuring how often the response of each of the two fine-tuned models was preferred over the baseline for each principle. our results show that both anthropic and anthropic-ega per- formed better (win rate .55) on negatively framed safety principles from the f5 ai neutrality, content caution, and cultural sensitivity factor (table 2), which focus on discouraging undesired behavior. in contrast, they struggled (win rate .35) with positively framed and abstract principles from the f6 ethics, freedoms, and rights factor, which emphasize benefiting humanity and ethical sensitivity. these results from fine-tuned models contrast with the human alignment results in table 1, where f6 was more likely to agree with human preferences than f5. our fine-tuned models might perform best with clear, action-based rules, likely because they lack human-like reasoning and cannot adjust for context or intent [ 73]. unlike humans, who interpret and adapt principles