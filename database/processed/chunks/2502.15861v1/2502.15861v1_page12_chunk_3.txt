for validation and not for subsequent analyses. we prompted llama-3-8b to pick one of the two response options based on a specific principle when the chosen response was option a or option b (100 prompts overall). we also checked whether the model would pick the chosen response when the principle was mismatched with the question by reusing all the questions written for other principles (900 prompts overall). we found that llama-3-8b correctly selected the chosen response 90.0 of the time for matching principle-query pairs, suggesting that it is capable of choosing the response that is in line with a principle. it also picked the chosen response 77.1 for mismatching principle- query pairs, mostly because principles were also applicable to other questions that were not explicitly aimed to test them. c principle sources and preference datasets we selected 185 principles from the full 495 principle set that cover an array of ai and social sciences sources: anthropic principles are from the 58 items used in the orig- inal cai work to fine-tune claude,2which were formulated in an ad-hoc manner from sources like the un declaration of human rights and the apple terms of service [8], good for humanity items are the 9 items aimed to guide model behavior to be less existentially risky for humans [ 44], ccai top agreed principles are from the 20 most-agreed- with crowdsourced items from the collective cai project [38] that solicited input on constitutional principles from a representative sample of americans,3 ccai top disagreed principles are from the 20 most-disagreed with crowdsourced items from the collective cai project [38], human values (general) principles are from the 10 general human values (such as benevolence or security) adapted from the theory of basic human values [61], human values (specific) principles are from the 57 specific survey items