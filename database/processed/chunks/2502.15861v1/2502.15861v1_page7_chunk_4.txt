for trait-based (behavior-based) framing; and , if the principle was selected by ega. 5 part 2: evaluating how models follow constitutions within our framework, we implemented two types of evaluations: principle-specific (5.1) and use-specific evaluation (5.2). since safety is a key focus in ai alignment research [ 8], we applied our framework to this use case. using orpo-llama-3-8b as the baseline model [ 45], we fine-tuned two additional models with orpo on 11,230 single-turn conversations from the hh-rlhf harmlessness dataset, each guided by different principles. the an- thropic model used principles randomly sampled from the full anthropic constitution (58 principles), while the anthropic-ega model was fine-tuned on a refined subset of 15 ega-selected prin- ciples, identified by ega on the full principle set and 300 hh-rlhf harmlessness conversations not used for fine-tuning (appendix d). 5.1 principle-specific evaluation this evaluation unfolded in two steps. first, the three models (base- line, anthropic, and anthropic-ega) generated responses to 300 user conversational queries from the hh-rlhf harmlessness test dataset [ 7]. second, for each of the 58 anthropic principles and each of the two cai models, an independent instance of llama-3- 8b was instructed to choose between the response of the baseline