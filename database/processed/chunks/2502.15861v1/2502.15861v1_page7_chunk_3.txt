that need to be evaluated [ 46], enabling more resource-efficient development of constitutional models.principle principle anthropic anthropic-ega attributes name (vs.baseline) (vs. baseline) all principles 0.455 0.459 f1 non-aggression 0.627 0.633 f5 medical advisory caution 0.620 0.633 f5 minimal assumptions 0.603 0.643 f5 cultural sensitivity (bg.) 0.580 0.610 f5 power aversion 0.577 0.600 f5 cultural sensitivity (aud.) 0.577 0.633 f6 triple h 0.577 0.580 f5 cultural sensitivity (capit.) 0.573 0.630 f5 cultural sensitivity (trad.) 0.557 0.587 f5 child-safe content 0.550 0.560 . . . . . . . . . f5 equality respect 0.347 0.330 f3 benevolent intent 0.347 0.340 f6 human rights respect 0.343 0.307 f6 helpful honesty 0.337 0.320 f6 human-centric flexibility 0.330 0.333 f5 ai representation 0.313 0.320 f6 universal equality rights 0.287 0.250 f6 friendly response 0.257 0.233 f3 humanity first 0.247 0.243 f6 ethical sensitivity 0.233 0.210 table 2: win rates (0.5 means random chance) show how of- ten the responses of two tested models are preferred over the baseline llm (orpollama-3-8b), as judged by another inde- pendent llm based on the principle in each row (as described in appendix b). the tested models are the anthropic model, fine-tuned with 58 principles, and the anthropic-ega, fine- tuned with a subset of 15 principles selected by ega. along each principle, we show its ega factor such as f6 and its characteristics: () for positive (negative) framing; () for trait-based (behavior-based) framing; and , if the principle was selected by ega. 5 part 2: evaluating how models follow constitutions within our framework, we implemented two types of evaluations: principle-specific (5.1) and use-specific evaluation (5.2). since safety is a key focus in ai alignment research [ 8], we applied our framework to this use case. using orpo-llama-3-8b as the baseline model [ 45], we fine-tuned two additional models with