and evaluating cai constitutions. ccs concepts human-centered computing collaborative and social com- puting design and evaluation methods . keywords constitutional ai; human-ai alignment; responsible ai. acm reference format: yara kyrychenko, ke zhou, edyta bogucka, and daniele quercia. 2025. c3ai: crafting and evaluating constitutions for constitutional ai. in proceedings of the acm web conference 2025 (www 25), april 28-may 2, 2025, sydney, nsw, australia. acm, new york, ny, usa, 15 pages. [url] 3696410.3714705 this work is licensed under a creative commons attribution 4.0 international license. www 25, sydney, nsw, australia 2025 copyright held by the owner author(s). acm isbn 979-8-4007-1274-6 25 04 [url] introduction despite the rapid assimilation of large language models (llms) into the mainstream [ 49,66], recent research has shown that llms can exhibit harmful behaviors [ 29] and social, racial, religious, and gen- der biases [ 1,11,37]. to ensure safety and utility of llms deployed across web platforms - such as chatbots, content moderation sys- tems, and recommendation tools - we need to align ai systems with diverse human values [ 27,57,62], which is currently performed by fine-tuning on large datasets of human preferences for one model response over another [7, 50]. constitutional ai (cai) [ 8], proposed by anthropic1, represents a novel approach to self-aligning models using minimal human input in the form of constitutions, which are sets of principles designed to guide model behavior. this approach enhances the efficacy of alignment by reducing the need for extensive human preference annotation and offers greater transparency, as the guid- ing principles are explicitly defined. more specifically, in a typical cai setup, general rules, called items (e.g., the ai should always avoid sharing personal user data), are turned into clear, easy-to- understand instructions for humans, called statements (e.g., the assistant should not disclose any personal information). these in- structions