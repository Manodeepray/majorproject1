(e.g., choose the response that is least unreliable). to then refine the constitution, we applied exploratory graph analysis (ega) to select a concise set of the most robust and informa- tive principles. this uncovered six latent principle factors, suggesting that the broad set of principles naturally clusters into six underlying themes or dimensions. (2)after fine-tuning a model with a constitution, our framework evaluates how well it adheres to its principles (5). in a case study on safety alignment, we learned two key insights. first, cai models perform well on some principles but struggle with others, highlighting areas where training data can be im- proved. second, our ega-based principle selection methodcreated an effective constitution using only 26 of the orig- inal principles (15 out of 58). the ega-selected principles maintained strong performance on safety benchmarks while preserving reasoning and math capabilities. furthermore, the ega method can be applied without relying on human conversational preference datasets. we open-source our framework at [url] net c3ai as it will be potentially adaptable to various use cases (6). 2 related work 2.1 llm alignment ai alignment broadly refers to guiding ai systems to adhere to human norms, objectives, and values [ 39,62]. as generative models are becoming increasingly capable and self-sufficient, there is a pressing need [ 41] to ensure they are helpful without causing harm by, for instance, violating individual privacy [ 47], disseminating stereotypes [ 1,37], and making unsafe or illicit suggestions [ 19,29, 63]. since potential harms are diverse, gabriel [27]suggests that it is most reasonable to align ai agents with human values - as opposed to, for instance, having explicit instructions or implicit preferences - such that the agents actions are guided by a notion of morality or what it should and should not do, as defined by humans either individually