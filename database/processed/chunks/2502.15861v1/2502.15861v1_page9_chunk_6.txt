13951417. [39] jiaming ji, tianyi qiu, boyuan chen, borong zhang, hantao lou, kaile wang, yawen duan, zhonghao he, jiayi zhou, zhaowei zhang, et al .2023. ai alignment: a comprehensive survey. arxiv:2310.19852 (2023). [40] jigsaw. 2024. announcing experimental bridging attributes in perspective api. [url] in-perspective-api-578a9d59ac37 [41] anna jobin, marcello ienca, and effy vayena. 2019. the global landscape of ai ethics guidelines. nature machine intelligence 1, 9 (2019), 389399. [42] hannah rose kirk, bertie vidgen, paul rttger, and scott a. hale. 2024. the benefits, risks and bounds of personalizing the alignment of large language models to individuals. nature machine intelligence (2024), 110. [43] hannah rose kirk, alexander whitefield, paul rttger, andrew bean, katerina margatina, juan ciro, rafael mosquera, max bartolo, adina williams, he he, et al . 2024. the prism alignment project: what participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. arxiv:2404.16019 (2024). [44] sandipan kundu, yuntao bai, saurav kadavath, amanda askell, andrew callahan, anna chen, anna goldie, avital balwit, azalia mirhoseini, brayden mclean, et al . 2023. specific versus general principles for constitutional ai. arxiv:2310.13798 (2023). [45] maxime labonne. 2024. fine-tune llama 3 with orpo. [url] blog mlabonne orpo-llama-3 [46] nathan lambert, valentina pyatkin, jacob morrison, lj miranda, bill yuchen lin, khyathi chandu, nouha dziri, sachin kumar, tom zick, yejin choi, et al .2024. re- wardbench: evaluating reward models for language modeling. arxiv:2403.13787 (2024). [47] haoran li, yulin chen, jinglong luo, yan kang, xiaojin zhang, qi hu, chunkit chan, and yangqiu song. 2023. privacy in large language models: attacks, defenses and future directions. arxiv:2310.10383 (2023).