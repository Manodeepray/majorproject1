c3ai: crafting and evaluating constitutions for constitutional ai www 25, april 28-may 2, 2025, sydney, nsw, australia choose the response that is less harmful ( minimize harm ). all these principles are in table 4 in the appendix. there is an inherent tension between helpfulness and harmless- ness [ 7], i.e., a model that produces less harmful responses tends to generate plain and unhelpful content, such as i cant provide fi- nancial advice. therefore, for each principle, we examined to what degree llama-3-8b instructed with that principle (with a 3-shot prompt) picked the response that was also selected by a human across three different conversational objectives from five human preference datasets: harmlessness: hh-rlhf harmless [ 7] and prism contro- versial and value-guided conversations [43], helpfulness: hh-rlhf helpful [ 7] and stanford human preferences [23], general: lmsys [75] and prism unguided [43]. we selected these conversational objectives and datasets because they cover a wide range of potentially harmful, helpful, and general llm use cases from diverse audiences. the hh-rlhf harmless dataset contains model responses that can potentially be harmful or contain illegal or offensive model responses, while the hh-rlhf helpful dataset has conversations designed to be helpful to the user but not necessarily harmless. lmsys provides a set of human-model conversations in the wild with different language models, while prism additionally has the types of conversations users engaged in (controversy-guided, value-guided, or unguided conversations). unlike the other datasets, stanford human preferences has col- lective human preferences, collected from helpful subreddits (like askphysics and askbaking) and using the upvotes from reddit users to decide on the collective preference for one answer over another. for each dataset, we randomly sampled 300 single-turn conversationsmeaning conversations where a user asks something and receives one replyand two response options (i.e., an option that was chosen by