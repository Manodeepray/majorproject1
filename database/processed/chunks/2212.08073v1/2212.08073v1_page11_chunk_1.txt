models for feedback for the experiments in this section, but in section 2 we also compare against helpful rlhf models in terms of label accuracy on various datasets. furthermore, we wrote a set of 16 different principles, and randomly sampled a principle for each compari- son label. qualitatively, we found that ensembling over principles led to notably more robust pm behavior compared to using the same principle for all labels (see section 4.3). the principles we used can be found in appendix c, an example of which is which of these assistant responses is less harmful? choose the response that a wise, ethical, polite and friendly person would more likely say. we also prepend to the context of the feedback model a set of few-shot examples of the labeling task. each few-shot example is formatted in the same manner as above, with an independently sampled principle, and a pre-written prompt and response pair. we use the sl-cai models discussed in earlier sections both for generating the response pairs, and as the initial snapshot for rl. we suspect that using the same model for both should lead to better results, since the distribution of responses generated by the policy are similar to the preference model training distribution, at least during early phases of rl. the rl training pipeline from this point on is identical to rlhf, except that the preference model is now trained partially with model-generated feedback labels (i.e. human-feedback labels for helpfulness, mixed with model-feedback labels for harmlessness). chain-of-thought prompting we also experimented with using chain-of-thought (cot) prompting [wei et al., 2022] on the feedback model to generate labels. in this case, we use the helpful rlhf model instead of the pre-trained model, which typically writes higher quality chain-of-thought. moreover, we reformat the feedback principles in a conversational manner