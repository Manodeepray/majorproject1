and improveable as compared to a large dataset of human preference labels. we leave it to future work to study the effectiveness of this type of feedback. 6.1 future directions in prior work we have focused on training ai assistants to helpful, harmless, and honest [askell et al., 2021], but otherwise we have allowed their behavior to be determined by generalization patterns from pretraining that are not under our direct control. however, the constitutional methods we have discussed here are very general, and in principle might be applied to steer language models in a variety of ways. for example, we expect we could use these method to change the models writing style, tone, or personality, or alter its responses to specic categories of questions (e.g. to train an ai that heavily caveats certain categories of advice, or that adopts a specic persona). the constitutional approach should thus make it much easier to study how different ai behaviors tend to generalize and interfere, since by obviating human feedback, our methods lower the barrier to experimentation. for example, it should be possible to generate feedback labels along dozens of behavioral axes, and then study how preference models trained from these labels are correlated or anti-correlated. this is important for ai safety, since the generalization patterns imbued by pretraining are currently something of a black box whose correlations may have unforeseen consequences. another remaining issue, and a major motivation for this work, is robustness that is, can we make models essentially immune to red-team attacks? we hope that by making helpfulness and harmlessness more com- patible, we will be able to signicantly scale-up (automated) red teaming in order to improve robustness. furthermore, we should be able to perform iterated online training [bai et al., 2022] with ai supervision, 15