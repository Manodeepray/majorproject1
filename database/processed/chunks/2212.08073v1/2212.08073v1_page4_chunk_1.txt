101051010 parameters250 200 150 100 50 050100150helpfulness elo sl-cai helpful rlhf hh rlhf rl-cai rl-cai w cot 101051010 parameters200 150 100 50 050100150harmlessness elo figure 3 this gure shows helpfulness and harmlessness elo scores for models of varying sizes, as deter- mined from comparison tests of crowdworker preferences in open-ended conversation. helpful (h) rlhf and helpful harmless (hh) rlhf are similar to prior work [bai et al., 2022]. sl-cai, rl-cai, and rl- cai w cot models are trained with our new constitutional method. although here we largely eliminate direct human supervision for harmlessness, rather than removing human supervision, in the longer term our goal is to make human supervision3as efcacious as possible. a harmless but non-evasive (still helpful) assistant an ai assistant that answers all questions with i dont know would be harmless, but of course it would also be completely useless. in our prior work using human feedback to train a helpful and harmless assistant [bai et al., 2022], we found that there was a signicant tension between helpfulness and harmlessness, and in particular, our assistant often refused to answer controversial questions. furthermore, once it encountered objectionable queries, it could get stuck producing evasive responses4for the remainder of the conversation. ultimately this was due to the fact that evasiveness was rewarded as a response to harmful inputs by our crowdworkers. one of our goals in this work is to train a helpful and harmless assistant that is never evasive, in order to reduce the tension between helpfulness and harmlessness. so while the assistant must still refrain from helping users with unethical requests, and from expressing offensive language and sentiment, it should always engage and explain why it refuses such requests. this should make it easier to scale up automated red teaming [perez et al., 2022] in future work, since