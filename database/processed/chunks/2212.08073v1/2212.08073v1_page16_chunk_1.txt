where we update the preference model with new ai feedback in order to keep it on the same distribution as the policy produces. we saw that this was valuable with human feedback, and by using ai feedback we can fully automate the process. robustness was also another motivation for using chain-of-thought reasoning in this work we would even- tually like ai systems to reason through the hidden risks of certain behaviors, in order to mitigate increasingly subtle and implicit harms. 6.2 broader impacts as with most methods that can control ai behavior, the ideas discussed in this work have a dual use. as we pass from prompting, to rlhf, to the constitutional methods discussed here, we lower the barrier to training ai models that behave in ways their creators intend. this means that these methods also make it easier to train pernicious systems. the supervised methods we have discussed may be particularly accessible, since they do not require an efcient rl implementation with large language models. a further issue is that by reducing the need for human feedback, our constitutional methods make it easier to train and deploy ai systems that have not been thoroughly tested and observed by humans. this could lead developers to deploy models with unforeseen failure modes. on the other hand, our method has the benet that we may no longer need an army of human red teamers to engage in the rather unsavory work of trying to trick ai systems into generating harmful content. 7 contribution statement model pre-training: model pretraining was led by nicholas joseph and sam mccandlish, with help from tom brown and jared kaplan, and much of anthropics technical staff contributed to the development of our efcient distributed training infrastructure and the underlying machine learning systems. core contributors include tom henighan, scott