the rl-cai with cot seems slightly less helpful but slightly more harmless compared to without cot. in figure 2, we show a plot of harmlessness elo vs. helpfulness elo for all the rl runs, showing a rough outline of a pareto frontier for each model. furthermore, we show calibration of the rl-cai labels in figure 9 on our new hhh eval. we nd that the feedback models log-probabilities are reasonably well-calibrated. we found that rl-cai models can be over-trained, resulting in goodharting behavior [gao et al., 2022] whereby models can be overly harsh in responding to harmful prompts, or may include boilerplate language as part of their response to most red teaming prompts, saying e.g. you are valid, valued, and cared for, as in the following examples: 12