relative to the pre-training learning rate, and batch size 1024 sequences. 3.3 main results we evaluate the helpfulness and harmlessness of our models by calculating elo scores based on crowd- worker preferences, as expressed during model comparison tests, following the same procedure as in [bai et al., 2022]. each conversation is unique, as the crowdworker writes the human side of the conver- sation; and at each step of the conversation, two responses are generated from two different models for which a preference label is collected from the worker. these conversations are similar in distribution to, but distinct from, those appearing in the pm and rl training data. results are shown in figure 3, where we compare sl-cai models and rlhf models. the rlhf models include two types: (1) models trained on only helpful- ness data, and (2) models trained on helpfulness and harmlessness. the gure also includes the rl-cai (i.e., rlaif) models discussed in section 4. a total of 10,274 helpfulness and 8,135 comparisons were collected for ab testing the 24 snapshots shown collectively in figures 2 and 3. as expected from prior work, we nd that the helpful rlhf model is more helpful but also more harmful than hh rlhf. furthermore, while sl-cai is less helpful than both rl models, it is more harmless than the helpful rlhf model and more harmful than hh rlhf.8we also compare sl-cai and pre-trained models in figure 8, where the 52b-parameter sl-cai model is shown as the initial snapshot of rl-cai, while the 7these principles were selected in an ad hoc manner for research purposes, and were not carefully designed as in [glaese et al., 2022]. we have included these principles in appendix c] 8note that the harmlessness elo scores for the rlhf models look much closer to together compared to [bai et