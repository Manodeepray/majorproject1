percent range led to better and more robust behavior (see section 4.3). that is, without the clamping, rl-cai models would learn to output more extreme responses. 4.2 datasets and training all our rl runs used the same hyperparameters as our prior work [bai et al., 2022]. however, there are some differences. the rlhf models for our earlier paper are netuned from context-distilled models, while our current rlhf models are netuned directly from pre-trained models. we didnt see much benet to using context distillation since the improvement from rl was much more signicant. furthermore, the pre-trained lms that we use for all our runs have been improved since the prior work. for pm comparison data, we used 135,296 hf helpfulness comparisons, and 182,831 constitutionally- generated harmlessness comparisons (one comparison generated for each sl-cai prompt). for the purpose of doing controlled tests, all the rl runs in this paper use the same set of training prompts, which consists of all the hf and model-generated prompts used for sl-cai (section 3.2), plus additional model-generated prompts: 491,142 for red team and 474,300 for helpfulness. 11