note that sparrows [glaese et al., 2022] decomposition of harmlessness into different areas has some com- monality with our use of principles forming a constitution. some other recent works on self-supervision include [shi et al., 2022, huang et al., 2022]. we also use chain-of-thought reasoning [nye et al., 2021, wei et al., 2022] to augment model performance and make ai decision making more transparent. specically, we ask language models to think step-by-step [kojima et al., 2022] and write out an argument explaining why one ai assistant response would be more harmless than another, before actually choosing the less harmful response. the motivations behind this work also align naturally with [ganguli et al., 2022], which provides an exten- sive study of red teaming of language models, and signicant portions of our red teaming data are gath- ered from that work. we also leverage the fact that language models can make well-calibrated choices [kadavath et al., 2022] to turn ai choices into calibrated preference labels. scaling supervision has been widely discussed as a possibility for ai alignment, with specic proposals such as [christiano et al., 2018, irving et al., 2018] and recent empirical work like [bowman et al., 2022]. 6 discussion we have trained language assistants that are both helpful andharmless without using human feedback labels for harmlessness. we referred to the technique as constitutional ai (cai) since we used a constitution con- sisting of human-written principles. we established two methods: (1) constitutional ai which bootstraps a helpful rlhfs instruction-following abilities to critique and revise its own responses so as to remove harm- ful content, and (2) rl with model-generated labels for harmlessness, which further improves harmlessness. we used this method to train models that are both harmless and non-evasive, partially resolving an issue in [bai et al., 2022]. by removing human