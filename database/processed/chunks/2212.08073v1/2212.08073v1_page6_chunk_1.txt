1091010 parameters0.500.550.600.650.700.750.80accuracy combined hhh evals: preference models vs multiple choice pretrained lm hh pm from human feedback chain-of-thought ensembled chain-of-thoughtfigure 4 we show performance on 438 binary comparison questions intended to evaluate helpfulness, honesty, and harmlessness. we compare the performance of a preference model, trained on human feedback data, to pretrained language models, which evaluate the comparisons as multiple choice questions. we see that chain of thought reasoning signicantly improves the performance at this task. the trends suggest that models larger than 52b will be competitive with human feedback-trained preference models. 1.4 models and data we use a series of language models, pretrained in the way we described in prior work [bai et al., 2022]. as our goal is to train helpful and harmless assistants from purely helpful assistants, we use rlhf to train our initial helpful models. for this we use the same process, but using only helpfulness human feedback (hf) data. however, as a point of comparison, we have also trained new preference models and helpful and harmless rlhf policies using human feedback. in our prior work [bai et al., 2022], we collected human feedback data for preference model comparisons. specically, each data sample consists of a prompt and a pair of model-generated responses to the prompt; a crowdworker then labels the response deemed more helpful or harmless, depending on the task at hand. the helpfulness and harmlessness data are collected separately, and workers are asked to red team the model (i.e., write prompts that are likely to elicit harmful model responses) for the latter. we then trained two types of models via rlhf: (1) helpful models which are trained only on the helpfulness data, and (2) hh models which are trained on both helpfulness and harmlessness. past experiments [bai et al., 2022] showed that rlhf signicantly improves