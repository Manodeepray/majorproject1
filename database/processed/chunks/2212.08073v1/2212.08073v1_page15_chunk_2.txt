con- sisting of human-written principles. we established two methods: (1) constitutional ai which bootstraps a helpful rlhfs instruction-following abilities to critique and revise its own responses so as to remove harm- ful content, and (2) rl with model-generated labels for harmlessness, which further improves harmlessness. we used this method to train models that are both harmless and non-evasive, partially resolving an issue in [bai et al., 2022]. by removing human feedback labels for harmlessness, we have moved further away from reliance on human supervision, and closer to the possibility of a self-supervised approach to alignment. however, in this work we still relied on human supervision in the form of helpfulness labels. we expect it is possible to achieve help- fulness and instruction-following without human feedback, starting from only a pretrained lm and extensive prompting, but we leave this for future work. our ultimate goal is notto remove human supervision entirely, but to make it more efcient, transparent, and targeted. all of our methods can leverage chain-of-thought [nye et al., 2021, wei et al., 2022] type reasoning for critiques in the sl stage, and for evaluating comparisons for the rl stage and we expect that a small number of very high-quality human demonstrations of this reasoning [scheurer et al., , saunders et al., 2022] could be used to improve and focus performance. natural language feedback is also more transparent, inter- pretable, and improveable as compared to a large dataset of human preference labels. we leave it to future work to study the effectiveness of this type of feedback. 6.1 future directions in prior work we have focused on training ai assistants to helpful, harmless, and honest [askell et al., 2021], but otherwise we have allowed their behavior to be determined by generalization patterns from pretraining that are not under our direct