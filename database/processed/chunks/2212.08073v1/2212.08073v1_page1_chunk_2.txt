better, and then train a preference model from this dataset of ai prefer- ences. we then train with rl using the preference model as the reward signal, i.e. we use rl from ai feedback (rlaif). as a result we are able to train a harmless but non- evasive ai assistant that engages with harmful queries by explaining its objections to them. both the sl and rl methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of ai decision making. these methods make it possible to control ai behavior more precisely and with far fewer human labels. correspondence to: yuntao,jared anthropic.com author contributions are detailed in 7.arxiv:2212.08073v1 [cs.cl] 15 dec 2022