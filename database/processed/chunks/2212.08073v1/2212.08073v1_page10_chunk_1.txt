101051010 parameters1 012harmlessness pm score 1 revisions 101051010 parameters1 012 2 revisions 101051010 parameters1 012 3 revisions 101051010 parameters1 012 4 revisions critiqued revision direct revisionfigure 7 comparison of preference model scores (all on the same 52b pm trained on harmlessness) for critiqued and direct revisions. we nd that for smaller models, critiqued revisions generally achieve higher harmlessness scores (higher is more harmless), while for larger models they perform similarly, though cri- tiques are always slightly better. 3.5 are critiques necessary? while our approach requires sampling a critique followed by a revision, we also consider simplifying our approach by skipping the critique step altogether, and instructing the model to generate a revision directly. in figure 7, we compare harmlessness pm scores for critiqued- vs direct-revisions. we found that critiqued revisions achieved better harmlessness scores for small models, but made no noticeable different for large models. furthermore, based on inspecting samples from the 52b, we found that the critiques were sometimes reasonable, but often made inaccurate or overstated criticisms. nonetheless, the revisions were generally more harmless than the original response. an example can be seen in appendix a. for the main results of this paper, we chose to use critiqued revisions, as it may provide more transparency into the models reasoning process. this sort of reasoning may also be useful to help models uncover more subtle harms or unintended consequences. 4 constitutional ai: reinforcement learning from ai feedback in prior work [bai et al., 2022], we discussed how to train hh rlhf models, whereby the role of human feedback is to provide comparison labels for preference modeling on both helpfulness and harmlessness. in this section, we extend this technique to train a hh model using human feedback labels only for helpfulness. all harmlessness labels will be generated by the language model