and higher quality responses. constitutional principles we tried simply rewriting the constitutional principles to encourage the model to avoid choosing over-reactive or overly accusatory responses; this seemed to improve behavior qualitatively. some of the principles in appendix c include this kind of language. ensembling when generating labels, we ensemble over 16 pre-written constitution principles, as discussed earlier. we found that this led to more robust preference model scores. preference labels (soft vs. hard vs. clamped) for rl-cai without cot, we found that using soft preference labels (i.e., normalized log-probabilities from the feedback model) led to much better results than hard labels (i.e., 0s and 1s). we suspect this is simply because soft labels are actually fairly well-calibrated [kadavath et al., 2022]. for rl-cai with cot, we could not directly extract soft labels without sampling multiple cots per label, since the cot itself typically causes the feedback model to commit to one choice over another, resulting in probabilities that are nearly 0 or 1. instead we found that clamping the probabilities at 20-80 percent slightly improved results, while clamping at 40-60 improved results further. we settled on using 40-60 for the main results of the paper. 4.4 harmlessness vs. evasiveness in prior work [bai et al., 2022], we found that the hh rlhf models are often evasive when presented with sensitive discussions, giving canned responses like i cant answer that. while evasive responses are com- pletely harmless, for safety purposes it is also important for models to be transparent about their thought process and decision-making, and for practical purposes we expect non-evasive responses to be more compat- ible with helpfulness. we nd that rl-cai is virtually never evasive, and often gives nuanced and harmless responses to most red team prompts. sample responses from the 52b hh rlhf and rl-cai models on