harms: methods, scaling behaviors, and lessons learned. [gao et al., 2022] gao, l., schulman, j., and hilton, j. (2022). scaling laws for reward model overoptimiza- tion. [glaese et al., 2022] glaese, a., mcaleese, n., tr ebacz, m., aslanides, j., firoiu, v ., ewalds, t., rauh, m., weidinger, l., chadwick, m., thacker, p., campbell-gillingham, l., uesato, j., huang, p.-s., comanescu, r., yang, f., see, a., dathathri, s., greig, r., chen, c., fritz, d., elias, j. s., green, r., mokr, s., fernando, n., wu, b., foley, r., young, s., gabriel, i., isaac, w., mellor, j., hassabis, d., kavukcuoglu, k., hendricks, l. a., and irving, g. (2022). improving alignment of dialogue agents via targeted human judgements. [huang et al., 2022] huang, j., gu, s. s., hou, l., wu, y ., wang, x., yu, h., and han, j. (2022). large language models can self-improve. [irving et al., 2018] irving, g., christiano, p., and amodei, d. (2018). ai safety via debate. [kadavath et al., 2022] kadavath, s., conerly, t., askell, a., henighan, t., drain, d., perez, e., schiefer, n., dodds, z. h., dassarma, n., tran-johnson, e., johnston, s., el-showk, s., jones, a., elhage, n., hume, t., chen, a., bai, y ., bowman, s., fort, s., ganguli, d., hernandez, d., jacobson, j., kernion, j., kravec, s., lovitt, l., ndousse, k., olsson, c., ringer, s., amodei, d., brown, t., clark, j., joseph, n., mann, b., mccandlish, s., olah, c., and kaplan, j. (2022). language models (mostly) know what they know. [kojima et al., 2022] kojima, t., gu, s. s., reid, m., matsuo, y ., and iwasawa, y . (2022). large language models are zero-shot reasoners. arxiv preprint arxiv:2205.11916 . 17