human preference labels by the accuracy with which they assign a higher score to the better response. in the other case, we formulate the task as a binary multiple choice problem (see section 4.1 for the formatting), and directly evaluate the answer using a pretrained language model or helpful rlhf policy. we also use chain-of-thought (cot) reasoning, which improves performance signicantly for larger models. we nd a further small boost by sampling ve cot samples, and then averaging the probabilities that the model assigns to each answer from each of the ve samples. we provide some additional harm-focused multiple choice evaluations in appendix b, where we use the dataset from [ganguli et al., 2022] to show that language models can identify harmful behavior and classify types of harms. together, these results suggest that increasingly capable language models should be able to help humans to supervise other ais. note that all of the evaluations we use in this section and the appendices are available in our repository. 3 constitutional ai: critiques, revisions, and supervised learning in this section, we discuss how to build models that are both helpful and harmless without any human feed- back labels for harmlessness. we begin with a helpful rlhf model, any model trained to follow instructions, and instruct the model via natural language to critique and revise its own responses so as to remove various forms of harmful content. we refer to these instructions as principles forming a constitution, i.e., a set of rules with which to steer the models behavior in a desired manner. 3.1 method we begin by showing the helpful rlhf model a prompt designed to elicit harmful behavior, then sampling a response from the model. the prompts are obtained from a series of red teaming experiments from prior work [ganguli et al.,