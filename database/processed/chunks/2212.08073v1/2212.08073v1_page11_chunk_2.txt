is now trained partially with model-generated feedback labels (i.e. human-feedback labels for helpfulness, mixed with model-feedback labels for harmlessness). chain-of-thought prompting we also experimented with using chain-of-thought (cot) prompting [wei et al., 2022] on the feedback model to generate labels. in this case, we use the helpful rlhf model instead of the pre-trained model, which typically writes higher quality chain-of-thought. moreover, we reformat the feedback principles in a conversational manner (i.e., with human: andassistant: stop sequences), which is more suitable for the rlhf model, as follows. human: consider the following conversation between a human and an assistant: [human assistant conversation] [principle for multiple choice evaluation] (a) [response a] (b) [response b] assistant: lets think step-by-step: [chain-of-thought] in particular, we use the lets think step-by-step prompt from [kojima et al., 2022] to elicit the chain-of- thought. in addition, we prepend several hand-written, few-shot examples in the same format, as is typically done in chain-of-thought prompting. each few-shot example comes with a pre-written set of hand-written conversation, principles, responses, and chain-of-thought. see appendix e for the full list of examples. one issue that arises is that the cot samples typically state explicitly which multiple choice option is to be preferred, and so the probability targets are typically very condent (i.e., close to 0 or 1) and are not well- calibrated. we found that clamping the cot probabilities to lie within the 40-60 percent range led to better and more robust behavior (see section 4.3). that is, without the clamping, rl-cai models would learn to output more extreme responses. 4.2 datasets and training all our rl runs used the same hyperparameters as our prior work [bai et al., 2022]. however, there are some differences. the rlhf models for our earlier paper are netuned from context-distilled models, while our current rlhf models are netuned