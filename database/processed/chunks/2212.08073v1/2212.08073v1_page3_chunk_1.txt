pr etrained baseconstitutional rl(par et o impr o v ement)wit h chain of thoughtstandar d rlhfconstitutional slhelpful-onlyhelpful harmlessfigure 2 we show harmlessness versus helpfulness elo scores (higher is better, only differences are mean- ingful) computed from crowdworkers model comparisons for all 52b rl runs. points further to the right are later steps in rl training. the helpful and hh models were trained with human feedback as in [bai et al., 2022], and exhibit a tradeoff between helpfulness and harmlessness. the rl-cai models trained with ai feedback learn to be less harmful at a given level of helpfulness. the crowdworkers evaluating these models were instructed to prefer less evasive responses when both responses were equally harmless; this is why the human feedback-trained helpful and hh models do not differ more in their harmlessness scores. error bars are visible in figure 3 but are suppressed here for clarity. harmless [askell et al., 2021]) with a smaller quantity of higher quality human supervision. there are several reasons why this may be useful: ai supervision may be more efcient than collecting human feedback. it allows us to focus more on providing a small amount of legible, focused, high-quality oversight. there may also be ways for humans and ai systems to collaborate [bowman et al., 2022] to provide better supervision than either can provide alone. ai systems can already perform some tasks at or beyond human level (e.g. [silver et al., 2017]), and over time more examples are likely to emerge. we need to develop methods now that can provide oversight for these powerful ai systems, and scaling supervision may be one possibility, if the capability level of the supervisor can scale proportionally with the capabilities of the actor, and the supervisor remains aligned with our intended goals and constraints. that said, scaling supervision could