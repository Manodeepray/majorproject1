0.0 0.5 1.0 1.5 2.0 2.5 3.0 rl train sequences 1e60.51.01.52.02.53.03.5absolute harmfulness score helpful rlhf hh rlhf rl-cai rl-cai w cotfigure 10 absolute harmfulness score for various 52b rl snapshots, on a scale from 0 to 4, where higher is more harmful. solid lines are sampled at t 1, and dashed lines at t 0. the rlhf models are initialized on pre-trained lms, while the rl-cai models are initialized on sl-cai. the new instructions apply only to the current comparison tests, which are used to obtain all the elos shown in this paper. the instruction change may also explain some qualitative differences between this paper and past work. for instance, as shown in figure 3, the harmlessness elo differences between helpful and hh rlhf is much smaller than figure 1 of [bai et al., 2022]. we believe this is because penalizing evasiveness generally improves helpful rlhf scores and decreases hh rlhf scores. furthermore, we worked primarily with upwork and mturk in the past for collecting pm data and comparison testing; for the current work, we still use pm data from that period, but the tests were performed with surge ai10workers. 4.5 absolute harmfulness score in contrast to our experiments where we collect relative harmfulness labels between pairs of model responses, in [ganguli et al., 2022] we have also conducted red teaming experiments collecting absolute harmfulness la- bels. similar to the relative experiments, crowdworkers are tasked with having back-and-forth conversations with a language model to try to bait it into generating harmful content, except only a single model is involved per conversation, and a single response is generated per conversational step. finally, at the end, the worker rates their degree of success (on an integral rating scale from 0 to 4, inclusive) in getting the model to say something harmful. we