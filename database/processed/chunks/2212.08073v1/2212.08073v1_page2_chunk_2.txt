ai behavior, and that can be used to automatically test and enhance robustness to harmful behaviors. we also aim to develop methods that encode desirable ai behavior in a simple and transparent form, and that make it easier to understand and evaluate ai decision making. in this paper we develop a method we refer to as constitutional ai (cai), depicted in figure 1, and use it to train a non-evasive and relatively harmless ai assistant, without any human feedback labels for harms . the method therefore improves upon, and partially replaces reinforcement learning from human feedback [christiano et al., 2017]. the new assistant rl-cai is preferred by crowdworkers over those trained with previously collected [bai et al., 2022, ganguli et al., 2022] human feedback labels for harmfulness. we chose the term constitutional because we are able to train less harmful systems entirely through the specication of a short list of principles or instructions, i.e. a constitution. but we are also employing this terminology to emphasize that when developing and deploying a general ai system, we cannot avoid choosing some set of principles to govern it, even if they remain hidden or implicit. our motivations for developing this technique were: (1) to study simple possibilities for using ai systems to help supervise other ais, and thus scale supervision , (2) to improve on our prior work training a harmless ai assistant by eliminating evasive responses , reducing tension1[bai et al., 2022, glaese et al., 2022] between helpfulness and harmlessness and encouraging the ai to explain its objections to harmful requests, (3) to make the principles governing ai behavior, and their implementation, more transparent, and (4) to reduce iteration time by obviating the need to collect new human feedback labels when altering the objective. let us discuss these motivations in more detail.