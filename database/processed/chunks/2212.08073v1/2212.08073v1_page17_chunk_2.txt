e., chen, e., pettit, c., heiner, s., lukosuite, k., askell, a., jones, a., chen, a., goldie, a., mirhoseini, a., mckinnon, c., olah, c., amodei, d., amodei, d., drain, d., li, d., tran-johnson, e., kernion, j., kerr, j., mueller, j., ladish, j., landau, j., ndousse, k., lovitt, l., elhage, n., schiefer, n., joseph, n., mercado, n., dassarma, n., larson, r., mccandlish, s., kundu, s., johnston, s., kravec, s., showk, s. e., fort, s., telleen-lawton, t., brown, t., henighan, t., hume, t., bai, y ., hateld-dodds, z., mann, b., and kaplan, j. (2022). measuring progress on scalable oversight for large language models. [christiano et al., 2017] christiano, p., leike, j., brown, t. b., martic, m., legg, s., and amodei, d. (2017). deep reinforcement learning from human preferences. [christiano et al., 2018] christiano, p., shlegeris, b., and amodei, d. (2018). supervising strong learners by amplifying weak experts. [ganguli et al., 2022] ganguli, d., lovitt, l., kernion, j., askell, a., bai, y ., kadavath, s., mann, b., perez, e., schiefer, n., ndousse, k., jones, a., bowman, s., chen, a., conerly, t., dassarma, n., drain, d., elhage, n., el-showk, s., fort, s., dodds, z. h., henighan, t., hernandez, d., hume, t., jacobson, j., johnston, s., kravec, s., olsson, c., ringer, s., tran-johnson, e., amodei, d., brown, t., joseph, n., mccandlish, s., olah, c., kaplan, j., and clark, j. (2022). red teaming language models to reduce harms: methods, scaling behaviors, and lessons learned. [gao et al., 2022] gao, l., schulman, j., and hilton, j. (2022). scaling laws for reward model overoptimiza- tion. [glaese et al., 2022] glaese, a., mcaleese, n., tr ebacz, m., aslanides, j., firoiu, v ., ewalds, t., rauh, m., weidinger, l., chadwick, m., thacker, p., campbell-gillingham, l., uesato, j., huang, p.-s., comanescu, r., yang, f., see, a., dathathri, s., greig, r., chen, c.,