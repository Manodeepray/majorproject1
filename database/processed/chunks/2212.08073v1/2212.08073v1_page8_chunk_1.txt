be rewritten to emphasize different aspects of harmfulness, giving us exibility to steer the models behavior in different ways, and to get more diverse results. we have written a total of 16 different principles7related to harmlessness, many of which are quite similar and address harmfulness in a general sense, while others are designed to target specic areas. they are randomly sampled at each revision step of each red team prompt. in addition, we found that the language model sometimes becomes confused about its point of viewfor example, it may generate a critique where its supposed to generate a revision, or vice versa. we addressed this by few-shot prompting the model with examples of critiques and revisions, all formatted in the same way. we include these few-shot examples in appendix e and in our repository as well. we show an example of the pipeline in appendix d. qualitatively, we found that the original response often contains harmful content, and that the rst revision almost always removed most aspects of harmfulness. subsequent revisions sometimes improved results further, but it was less obvious by inspection. in addition, we found that the revised responses were rarely evasive (compare examples in appendix d), in the sense that the model was willing to engage with sensitive topics in a harmless, thoughtful manner rather than shut down the discussion, which we discuss more in section 4.4. next we netune a pre-trained model on the revisions (from all revisional steps). furthermore, in order to retain helpfulness as much as possible, we sampled responses from the helpful rlhf model on a set of helpfulness prompts collected from crowdworkers, and included these in the netuning. the main results are presented in section 3.3, where these models are referred to as sl-cai. in section 3.5, we also discuss a simpler alternative