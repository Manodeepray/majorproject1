and shogi by self-play with a general reinforcement learning algorithm. [solaiman and dennison, 2021] solaiman, i. and dennison, c. (2021). process for adapting language models to society (palms) with values-targeted datasets. corr , abs 2106.10328. [srivastava et al., 2022] srivastava, a., rastogi, a., rao, a., shoeb, a. a. m., abid, a., fisch, a., brown, a. r., santoro, a., gupta, a., garriga-alonso, a., et al. (2022). beyond the imitation game: quantifying and extrapolating the capabilities of language models. [stiennon et al., 2020] stiennon, n., ouyang, l., wu, j., ziegler, d. m., lowe, r., v oss, c., radford, a., amodei, d., and christiano, p. (2020). learning to summarize from human feedback. [thoppilan et al., 2022] thoppilan, r., freitas, d. d., hall, j., shazeer, n., kulshreshtha, a., cheng, h., jin, a., bos, t., baker, l., du, y ., li, y ., lee, h., zheng, h. s., ghafouri, a., menegali, m., huang, y ., krikun, m., lepikhin, d., qin, j., chen, d., xu, y ., chen, z., roberts, a., bosma, m., zhou, y ., chang, c., krivokon, i., rusch, w., pickett, m., meier-hellstern, k. s., morris, m. r., doshi, t., santos, r. d., duke, t., soraker, j., zevenbergen, b., prabhakaran, v ., diaz, m., hutchinson, b., olson, k., molina, a., hoffman-john, e., lee, j., aroyo, l., rajakumar, r., butryna, a., lamm, m., kuzmina, v ., fenton, j., cohen, a., bernstein, r., kurzweil, r., aguera-arcas, b., cui, c., croak, m., chi, e., and le, q. (2022). lamda: language models for dialog applications. corr , abs 2201.08239. [wei et al., 2022] wei, j., wang, x., schuurmans, d., bosma, m., ichter, b., xia, f., chi, e., le, q., and zhou, d. (2022). chain of thought prompting elicits reasoning in large language models. [xu et al., 2020] xu, j., ju, d., li, m., boureau, y .-l., weston, j.,