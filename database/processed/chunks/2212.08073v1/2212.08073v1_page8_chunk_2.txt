we netune a pre-trained model on the revisions (from all revisional steps). furthermore, in order to retain helpfulness as much as possible, we sampled responses from the helpful rlhf model on a set of helpfulness prompts collected from crowdworkers, and included these in the netuning. the main results are presented in section 3.3, where these models are referred to as sl-cai. in section 3.5, we also discuss a simpler alternative whereby we skip the critique step and sample the revision directly, but we use the critiqued revisions throughout the rest of the paper. 3.2 datasets and training for red teaming prompts (i.e. partial conversations), we collected 42,496 human-written prompts as discussed and shared in [ganguli et al., 2022], and generated a further 140,335 prompts by few-shot prompting a pre- trained model, giving a total of 182,831. we sampled 4 critique-revision pairs per red team prompt from a helpful rlhf model, giving 4 revisions per prompt. for helpfulness prompts, we collected a total of 135,296 human-written ones, and did not use any model-generated examples. we sampled 2 responses per prompt directly from a helpful rlhf. we always sample at temperature t 1. each conversation consists of multiple promptsone per human turn. we then trained sl-cai models by netuning a pre-trained model on the harmlessness revisions and help- fulness samples. we trained for one epoch, using a constant learning rate of 0.5 relative to the pre-training learning rate, and batch size 1024 sequences. 3.3 main results we evaluate the helpfulness and harmlessness of our models by calculating elo scores based on crowd- worker preferences, as expressed during model comparison tests, following the same procedure as in [bai et al., 2022]. each conversation is unique, as the crowdworker writes the human side of the conver- sation; and at each step of the conversation,