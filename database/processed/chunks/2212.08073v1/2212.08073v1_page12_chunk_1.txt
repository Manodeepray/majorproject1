0.0 0.5 1.0 1.5 2.0 2.5 3.0 rl train sequences 1e6150 100 50 050100150helpfulness elo helpful rlhf hh rlhf rl-cai rl-cai w cot 0.0 0.5 1.0 1.5 2.0 2.5 3.0 rl train sequences 1e6100 50 050100150200harmlessness elo figure 8 these gures show the helpfulness (left) and harmlessness (right) elo scores as a function of the total number of rl training sequences, as judged by crowdworkers via comparison tests. we see that the rl- cai models perform very well on harmlessness without a great cost to their helpfulness. the initial snapshot for the rl-cai models is sl-cai, where we set the elos to be zero; while the initial snapshot for the rlhf models is a pre-trained lm. note that the crowdworkers were instructed that among harmless samples, they should prefer those that were not evasive and instead explained the nature of the harm. 0.0 0.2 0.4 0.6 0.8 1.0 probability0.00.20.40.60.81.0frequencycalibration of rl-cai labels on hhh figure 9 calibration of 52b rl-cai labels on our hhh evaluation questions. dashed diagonal line repre- sents perfect calibration. 4.3 main results in figure 3, we show elo scores for the rl-cai models (with and without cot) compared to other models. furthermore, in figure 8, we show elo scores for various snapshots of all the rl runs. we nd that rl-cai models are signicantly more harmless than the rlhf and sl-cai models. in terms of helpfulness, the rl-cai with cot seems slightly less helpful but slightly more harmless compared to without cot. in figure 2, we show a plot of harmlessness elo vs. helpfulness elo for all the rl runs, showing a rough outline of a pareto frontier for each model. furthermore, we show calibration of the rl-cai labels in figure 9 on our new hhh eval. we nd that the feedback models log-probabilities are reasonably