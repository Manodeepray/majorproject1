(as discussed and dened in [askell et al., 2021, bai et al., 2022]) without using any human feedback labels for harmlessness: we nd that as language model capabilities improve, ai identication of harms improves signi- cantly. furthermore, chain-of-thought reasoning improves this ability, and leads to evaluations that are becoming competitive with preference models trained on human feedback labels (see figure 4). we show that model-generated critiques and revisions can be applied repeatedly to progressively reduce harmfulness (see figure 5). generating critiques improves harmlessness compared to simply generating revisions directly (figure 7). we use this method to specically address the evasiveness of our prior human feedback based model [bai et al., 2022]. using self-supervised preference labels for rl further improves model behavior as evaluated by crowdworkers (see figures 2 and 3), equaling or exceeding the performance when using human feedback to evaluate harmlessness. we attach a github repository6showing various few-shot prompts and constitutional principles that were used, along with model responses to various prompts. 5we could mix human and ai labels for both harmlessness and helpfulness, but since our goal is to demonstrate the efcacy of the technique, we do not use human labels for harmlessness. 6[url] 5