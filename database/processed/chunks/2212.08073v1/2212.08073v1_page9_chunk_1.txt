0 1 2 3 4 number of revisions1 012pm score harmlessness score 0 1 2 3 4 number of revisions2 1 01 helpfulness score 0 1 2 3 4 number of revisions1 012 hh score 101051010 parametersfigure 5 preference model scores of responses and revisions from helpful rlhf models, evaluated on a set of red team prompts. the scores are evaluated on a 52b preference model trained on (left) harmlessness comparisons, (center) helpfulness comparisons, and (right) a mixture of all the combined helpful and harmless comparisons. the preference models used for evaluation here were trained exclusively using human feedback. we nd that harmlessness and hh scores improve monotonically with respect to number of revisions, where revision 0 refers to the initial response, but pure helpfulness scores decrease. 0 1 2 3 4 number of revisions012harmlessness pm score n 16 n 8 n 4 n 2 n 1 0 1 2 3 4 number of revisions1.01.52.0harmlessness pm score std n 16 n 8 n 4 n 2 n 1 figure 6 we show harmlessness pm scores of revised responses for varying number of constitutional prin- ciples used. increasing the number of principles does not improve these pm scores, but we have found that it improves the diversity of revised responses, which improves exploration during the rl phase of cai training. 52b-parameter pre-trained model is shown as the initial snapshot of rlhf. we nd that sl-cai is both more helpful and harmless than pre-trained models, as expected. 3.4 scaling trends here we show results on the way preference model scores depend on the number of principles in the consti- tution and the number of revisions. number of principles in the constitution recall that at each critique-revision step of each prompt, a principle is sampled independently from all the constitution. in figure 6,