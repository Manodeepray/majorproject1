constitutional ai: reinforcement learning from ai feedback in prior work [bai et al., 2022], we discussed how to train hh rlhf models, whereby the role of human feedback is to provide comparison labels for preference modeling on both helpfulness and harmlessness. in this section, we extend this technique to train a hh model using human feedback labels only for helpfulness. all harmlessness labels will be generated by the language model itself via a multiple choice format, and then distilled back into a preference model. 4.1 method we continue to utilize human feedback labels for helpfulness as in prior work, but replace human feedback labels with model feedback labels for harmlessness. that is, instead of asking crowdworkers to provide comparison labels for harmlessness, we simply present the same task to an independent model, called the feedback model (typically a pretrained lm). once the desired comparison labels are obtained, the remainder of the training pipeline (i.e., preference model training and rl) is exactly the same as rlhf. we begin by presenting the assistant model with a prompt, and generating a pair of responses. we then present the prompt and response pair to the feedback model with a principle for choosing the more harmless response, in a format like consider the following conversation between a human and an assistant: [human assistant conversation] [principle for multiple choice evaluation] options: (a) [response a] (b) [response b] the answer is: we then compute the log probability of the responses (a) and(b), and we make a labeled, preference modeling comparison example with the normalized probabilities as targets (and we expect these targets will be fairly well-calibrated [kadavath et al., 2022], since they are multiple choice responses). we use pre-trained 10