we nd that sl-cai is both more helpful and harmless than pre-trained models, as expected. 3.4 scaling trends here we show results on the way preference model scores depend on the number of principles in the consti- tution and the number of revisions. number of principles in the constitution recall that at each critique-revision step of each prompt, a principle is sampled independently from all the constitution. in figure 6, we compare harmlessness pm score for varying number of constitutions. we nd that the number of constitutions does not appear to have a signicant effect on harmlessness score. nonethe- less, we expect that more constitutions leads to more diverse behaviors, although we did not studied this quantitatively in this work. diversity is particularly valuable to encourage exploration during the subsequent rl training step. number of revisions in figure 5 we show preference model scores for both the initial model response and subsequent revisions. we nd that the revisions achieve progressively higher harmlessness scores, suggesting that theres benet to utilizing further revisions. however, as discussed in our prior work [bai et al., 2022], preference model scores become less calibrated at higher values, so these results should be taken with a grain of salt. we also trained a series of sl-cai models up to various numbers of revisions. in particular, sl-cai- nis trained with netuned with up to and including the n-th revision, for n 1;2;3;4. 9