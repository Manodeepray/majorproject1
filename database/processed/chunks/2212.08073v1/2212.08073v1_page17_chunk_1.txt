acknowledgments we thank paul christiano for discussions and maja trebacz and alex tamkin for comments on the draft. were also deeply grateful to daniela amodei, jarrah bloomeld, jamie kerr, timothy telleen-lawton, jia yuan loke, jeffrey ladish, rebecca raible, rune kvist, rob gilson, guro khundadze, filipe dobreira, and sebastian conybeare for their help and support. wed like to thank the staff and workers at surge ai, amazon mturk, and upwork for providing most of the data for our research. references [askell et al., 2021] askell, a., bai, y ., chen, a., drain, d., ganguli, d., henighan, t., jones, a., joseph, n., mann, b., dassarma, n., elhage, n., hateld-dodds, z., hernandez, d., kernion, j., ndousse, k., olsson, c., amodei, d., brown, t., clark, j., mccandlish, s., olah, c., and kaplan, j. (2021). a general language assistant as a laboratory for alignment. [bai et al., 2022] bai, y ., jones, a., ndousse, k., askell, a., chen, a., dassarma, n., drain, d., fort, s., ganguli, d., henighan, t., joseph, n., kadavath, s., kernion, j., conerly, t., el-showk, s., elhage, n., hateld-dodds, z., hernandez, d., hume, t., johnston, s., kravec, s., lovitt, l., nanda, n., olsson, c., amodei, d., brown, t., clark, j., mccandlish, s., olah, c., mann, b., and kaplan, j. (2022). training a helpful and harmless assistant with reinforcement learning from human feedback. [bowman et al., 2022] bowman, s. r., hyun, j., perez, e., chen, e., pettit, c., heiner, s., lukosuite, k., askell, a., jones, a., chen, a., goldie, a., mirhoseini, a., mckinnon, c., olah, c., amodei, d., amodei, d., drain, d., li, d., tran-johnson, e., kernion, j., kerr, j., mueller, j., ladish, j., landau, j., ndousse, k., lovitt, l., elhage, n., schiefer, n., joseph, n., mercado, n., dassarma, n., larson, r., mccandlish, s., kundu, s., johnston, s., kravec, s., showk, s. e.,