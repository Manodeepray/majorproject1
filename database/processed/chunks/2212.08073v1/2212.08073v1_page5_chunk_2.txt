!reinforcement learning this stage mimics rlhf, except that we replace human preferences for harmlessness with ai feedback (i.e. we per- form rlaif), where the ai evaluates responses according to a set of constitutional principles. just as rlhf distills human preferences into a single preference model (pm), in this stage we distill lm interpre- tations of a set of principles back into a hybrid5human ai pm (as we use human labels for helpfulness, but only ai labels for harmlessness). we begin by taking the ai assistant trained via supervised learning (sl) from the rst stage, and use it to generate a pair of responses to each prompt in a dataset of harmful prompts (e.g. from [ganguli et al., 2022]). we then formulate each prompt and pair into a multiple choice question, where we ask which response is best according to a constitutional principle. this produces an ai-generated preference dataset for harmlessness, which we mix with our human feedback helpfulness dataset. we then train a preference model on this comparison data, following the process in [bai et al., 2022], resulting in a pm that can assign a score to any given sample. finally, we netune the sl model from the rst stage via rl against this pm, resulting in a policy trained by rlaif. 1.3 contributions we demonstrate constitutional methods to utilize a helpful rlhf model to train helpful and harmless models (as discussed and dened in [askell et al., 2021, bai et al., 2022]) without using any human feedback labels for harmlessness: we nd that as language model capabilities improve, ai identication of harms improves signi- cantly. furthermore, chain-of-thought reasoning improves this ability, and leads to evaluations that are becoming competitive with preference models trained on human feedback labels (see figure 4). we show that model-generated critiques and revisions can be