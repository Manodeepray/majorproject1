data are collected separately, and workers are asked to red team the model (i.e., write prompts that are likely to elicit harmful model responses) for the latter. we then trained two types of models via rlhf: (1) helpful models which are trained only on the helpfulness data, and (2) hh models which are trained on both helpfulness and harmlessness. past experiments [bai et al., 2022] showed that rlhf signicantly improves the models ability to follow instructions, and the hh model is signicantly more harmless than the helpful model. 2 evaluating the potential for ai supervision of hhh to motivate the approach we take in the remainder of this paper, in this section we evaluate whether lan- guage models can correctly identify the most helpful, honest, and harmless response in a conversation. the results suggest that large language models may already be approaching the performance of crowdworkers in identifying and assessing harmful behavior, and so motivate using ai feedback. in [askell et al., 2021] we wrote a variety of conversations between a human and an ai assistant, with a pair of model responses at the end of each conversation. we then ranked each pair based on helpfulness, honesty, and harmlessness, resulting in 221 binary comparisons [srivastava et al., 2022]. we nd that models can now achieve well over 90 binary accuracy in their ability to predict the better response (see figure 11 in the appendix), so for this paper we have written 217 more challenging comparisons, primarily focusing on more subtle tests of harmlessness, including examples where an evasive response is disfavored over a harmless and helpful message. in figure 4 we show the performance of various models on this task, in two formulations. in one case we formulate it as a preference model evaluation, and evaluate pms that trained on