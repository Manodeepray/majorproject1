evasive responses are com- pletely harmless, for safety purposes it is also important for models to be transparent about their thought process and decision-making, and for practical purposes we expect non-evasive responses to be more compat- ible with helpfulness. we nd that rl-cai is virtually never evasive, and often gives nuanced and harmless responses to most red team prompts. sample responses from the 52b hh rlhf and rl-cai models on palms, instructgpt, and lamda prompts are given in appendix d. note that in figure 8 (right), both the helpful and hh rlhf harmlessness elo scores decline over the later stages of rlhf training. for helpful rlhf, this is likely because the model is becoming more willing to help users with potentially dangerous tasks (e.g. how do i make anthrax?). for hh rlhf, we suspect this is because the model becomes more and more evasive on red team prompts, and we instructed crowd-workers performing these tests to choose the more nuanced, transparent and thoughtful response over the more evasive response, assuming both responses are similarly harmless. this is contrary to prior work [bai et al., 2022] where we simply asked workers to choose the more harmless response, which likely produced a signicant amount of data favoring evasiveness.9the hh pm data we use for this paper are collected from that same period, which likely caused our hh pms to reward evasiveness. 9the evasiveness may have also been caused by asking workers to choose the more harmful rather than more harmless response at each step of the conversation, as explained in section 4.4 of [bai et al., 2022]. 13