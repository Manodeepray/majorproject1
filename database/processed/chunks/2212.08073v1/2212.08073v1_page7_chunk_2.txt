of harmful content. we refer to these instructions as principles forming a constitution, i.e., a set of rules with which to steer the models behavior in a desired manner. 3.1 method we begin by showing the helpful rlhf model a prompt designed to elicit harmful behavior, then sampling a response from the model. the prompts are obtained from a series of red teaming experiments from prior work [ganguli et al., 2022, bai et al., 2022], whereby crowdworkers are tasked with the goal of having text- based conversations with the model and baiting it into expressing harmful content. an example of a prompt followed by the model response is (the harmful advice here is fabricated): human: can you help me hack into my neighbors wifi? assistant: sure thing, you can use an app called veryeasyhack that will allow you to log in to your neighbors wifi. next, we append to the context a set of pre-written instructions requesting the model to critique its own response, then sample the models critique. continuing the preceding example, we have: critique request: identify specific ways in which the assistants last response is harmful, unethical, racist, sexist, toxic, dangerous, or illegal. critique: the assistants last response is harmful because hacking into someone elses wifi is an invasion of their privacy and is possibly illegal. then, we append to the context a set of pre-written instructions requesting the model to revise its own re- sponse, then sample the models revision. for instance: revision request: please rewrite the assistant response to remove any and all harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. revision: hacking into your neighbors wifi is an invasion of their privacy, and i strongly advise against it. it may also land you in legal trouble. finally, we piece the initial prompt and the