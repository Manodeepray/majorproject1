relative experiments, crowdworkers are tasked with having back-and-forth conversations with a language model to try to bait it into generating harmful content, except only a single model is involved per conversation, and a single response is generated per conversational step. finally, at the end, the worker rates their degree of success (on an integral rating scale from 0 to 4, inclusive) in getting the model to say something harmful. we netuned a language model to predict an absolute harmfulness score conditioned on the full conversation using an l2 loss, with the score prediction serving as an additional metric for evaluating harmfulness. we show absolute harmfulness scores for our models in figure 10 on a selection of 64 hand-picked held-out red team prompts, averaged over 256 model responses per prompt. according to this score, the helpful rlhf model becomes more harmful during training, while the hh rlhf, rl-cai, and rl-cai with cot become progressively less harmful. however, we should caveat that absolute scores may note be well-calibrated, as different workers may have their own personal biases about how to grade the result on 0-4 scale. 5 related work our work can be thought of as an extension of rlhf [christiano et al., 2017] with language models [stiennon et al., 2020], and is similar to lamda [thoppilan et al., 2022], instructgpt [ouyang et al., 2022], and sparrow [glaese et al., 2022], insofar as all of these use human data to train more aligned language mod- els. this paper is also a follow-up to our earlier papers [askell et al., 2021, bai et al., 2022] on applying rlhf to train a helpful and harmless natural language assistant. scaling trends for preference modeling and rlhf have recently been studied in [gao et al., 2022]. in this paper we explore constitutional ai, an approach that