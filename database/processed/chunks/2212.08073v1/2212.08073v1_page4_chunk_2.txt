train a helpful and harmless assistant that is never evasive, in order to reduce the tension between helpfulness and harmlessness. so while the assistant must still refrain from helping users with unethical requests, and from expressing offensive language and sentiment, it should always engage and explain why it refuses such requests. this should make it easier to scale up automated red teaming [perez et al., 2022] in future work, since training intensively for harmlessness would otherwise result in a model that simply refuses to be helpful. simplicity and transparency the widely used reinforcement learning from human feedback (rlhf) method [christiano et al., 2017, stiennon et al., 2020] for training more helpful, honest, and harmless ai systems [bai et al., 2022, thoppilan et al., 2022, ouyang et al., 2022, glaese et al., 2022] typically uses (at least) tens of thousands of human feedback labels. these labels often remain private, but even when they are shared publicly, they do not shed much light on ai training objectives, since no one can feasibly understand or summarize the collective impact of so much information. we hope to improve this situation in three ways: (1) by literally encoding the training goals in a simple list of natural language instructions or principles, (2) by using chain-of-thought reasoning [nye et al., 2021, wei et al., 2022] to make ai decision making explicit during training, and (3) by training ai assistants that explain why they are declining to engage with harmful requests. 3with our present methods, this should possible by training ai systems to imitate the natural language explanations hu- mans give when evaluating ai behavior, as has been discussed in other contexts [scheurer et al., , saunders et al., 2022], but leave this to future work. 4in some contexts this could be a virtue [xu et al.,