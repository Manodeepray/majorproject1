human level (e.g. [silver et al., 2017]), and over time more examples are likely to emerge. we need to develop methods now that can provide oversight for these powerful ai systems, and scaling supervision may be one possibility, if the capability level of the supervisor can scale proportionally with the capabilities of the actor, and the supervisor remains aligned with our intended goals and constraints. that said, scaling supervision could also have downsides and dangers, since it means further automating (and quite possibly obscuring) decision making. as we discuss below, our constitutional approach leverages chain-of-thought reasoning [nye et al., 2021, wei et al., 2022] to make decision making more legible. in a certain sense, work on reinforcement learning from human feedback [stiennon et al., 2020, bai et al., 2022, ouyang et al., 2022] has already taken a step in the direction of scaled supervision, since the reward signal in rl actually comes from an ai preference model (pm) rather than from immediate hu- man oversight. however, rlhf typically uses tens of thousands of human preference labels. here, we will test methods that reduce human input to an extreme, in order to study their viability. we will netune ai models to be harmless using only of order ten2simple principles, stated in natural language. 2these principles were chosen in a fairly ad hoc and iterative way for research purposes. in the future, we believe such principles should be redeveloped and rened by a larger set of stakeholders, and that they should also be adapted depending on the intended usage and location in which the model may be deployed. since such a small number of bits of information are involved in these principles, its worth studying these bits carefully. 3