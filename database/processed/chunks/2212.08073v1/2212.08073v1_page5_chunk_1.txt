1.2 the constitutional ai approach we will be experimenting with an extreme form of scaled supervision, which we refer to as constitutional ai (cai). the idea is that human supervision will come entirely from a set of principles that should govern ai behavior, along with a small number of examples used for few-shot prompting. together these principles form the constitution. our training process has two stages (see figure 1), where the rst supervised phase gets the model on- distribution and the second rl stage renes and signicantly improves performance: (supervised stage) critique !revision !supervised learning in the rst stage of the process, we rst generate responses to harmfulness prompts using a helpful-only ai assistant. these initial responses will typically be quite harmful and toxic. we then ask the model to critique its response according to a principle in the constitution, and then revise the original response in light of the critique. we revise responses repeatedly in a sequence, where we randomly draw principles from the constitution at each step. once this process is complete, we netune a pretrained language model with supervised learning on the nal revised responses. the main purpose of this phase is to easily and exibly alter the distribution of the models responses, to reduce the need for exploration and the total length of training during the second rl phase. (rl stage) ai comparison evaluations !preference model !reinforcement learning this stage mimics rlhf, except that we replace human preferences for harmlessness with ai feedback (i.e. we per- form rlaif), where the ai evaluates responses according to a set of constitutional principles. just as rlhf distills human preferences into a single preference model (pm), in this stage we distill lm interpre- tations of a set of principles back into a hybrid5human ai pm (as we use human labels