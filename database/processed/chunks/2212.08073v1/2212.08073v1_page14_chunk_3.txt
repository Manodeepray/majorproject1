as all of these use human data to train more aligned language mod- els. this paper is also a follow-up to our earlier papers [askell et al., 2021, bai et al., 2022] on applying rlhf to train a helpful and harmless natural language assistant. scaling trends for preference modeling and rlhf have recently been studied in [gao et al., 2022]. in this paper we explore constitutional ai, an approach that relies on model self-critique, revision, and evalu- ation. similar work involving model self-critique and natural language feedback includes [zhao et al., 2021, scheurer et al., , saunders et al., 2022]; their methods are very similar to our supervised constitutional step. 10[url] 14