gener ate responses t o red t eaming pr ompts eliciting harmful samplesgener ate responses t o red t eaming pr ompts eliciting harmful samples rl aif t r aining with pm sl-cai modelsconstitutional ai f eedback for self-impr o v ementhelpful rlhf model gener ate responses t o red t eaming pr ompts eliciting harmful samplesgener ate responses t o red t eaming pr ompts eliciting p airs of samplesfinetuned pr ef er ence model (pm)finetuned sl-cai model final rl-cai modelresponsecritiquere vision figure 1 we show the basic steps of our constitutional ai (cai) process, which consists of both a super- vised learning (sl) stage, consisting of the steps at the top, and a reinforcement learning (rl) stage, shown as the sequence of steps at the bottom of the gure. both the critiques and the ai feedback are steered by a small set of principles drawn from a constitution. the supervised stage signicantly improves the initial model, and gives some control over the initial behavior at the start of the rl phase, addressing potential exploration problems. the rl stage signicantly improves performance and reliability. 1 introduction we would like to train ai systems that remain helpful, honest, and harmless, even as some ai capabilities reach or exceed human-level performance. this suggests that we will need to develop techniques that do not rely on humans to supervise all aspects of ai behavior, and that can be used to automatically test and enhance robustness to harmful behaviors. we also aim to develop methods that encode desirable ai behavior in a simple and transparent form, and that make it easier to understand and evaluate ai decision making. in this paper we develop a method we refer to as constitutional ai (cai), depicted in figure 1, and use it to train a non-evasive