[nye et al., 2021] nye, m., andreassen, a. j., gur-ari, g., michalewski, h., austin, j., bieber, d., do- han, d., lewkowycz, a., bosma, m., luan, d., sutton, c., and odena, a. (2021). show your work: scratchpads for intermediate computation with language models. [ouyang et al., 2022] ouyang, l., wu, j., jiang, x., almeida, d., wainwright, c. l., mishkin, p., zhang, c., agarwal, s., slama, k., ray, a., et al. (2022). training language models to follow instructions with human feedback. arxiv preprint arxiv:2203.02155 . [perez et al., 2022] perez, e., huang, s., song, f., cai, t., ring, r., aslanides, j., glaese, a., mcaleese, n., and irving, g. (2022). red teaming language models with language models. [saunders et al., 2022] saunders, w., yeh, c., wu, j., bills, s., ouyang, l., ward, j., and leike, j. (2022). self-critiquing models for assisting human evaluators. [scheurer et al., ] scheurer, j., campos, j. a., chan, j. s., chen, a., cho, k., and perez, e. training language models with language feedback. [shi et al., 2022] shi, w., dinan, e., shuster, k., weston, j., and xu, j. (2022). when life gives you lemons, make cherryade: converting feedback from bad responses into good labels. [silver et al., 2017] silver, d., hubert, t., schrittwieser, j., antonoglou, i., lai, m., guez, a., lanctot, m., sifre, l., kumaran, d., graepel, t., lillicrap, t., simonyan, k., and hassabis, d. (2017). mastering chess and shogi by self-play with a general reinforcement learning algorithm. [solaiman and dennison, 2021] solaiman, i. and dennison, c. (2021). process for adapting language models to society (palms) with values-targeted datasets. corr , abs 2106.10328. [srivastava et al., 2022] srivastava, a., rastogi, a., rao, a., shoeb, a. a. m., abid, a., fisch, a., brown, a. r., santoro, a., gupta, a., garriga-alonso, a., et al. (2022). beyond the imitation game: quantifying