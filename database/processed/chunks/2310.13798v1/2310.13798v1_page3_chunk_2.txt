and traits, and it is interesting to probe this design space. we may want very capable ai systems to reason carefully about possible risks stemming from their actions (including the possibility that the ai is being misused for unethical purposes). this moti- vates exploring whether ai systems can already derive notions of ethical behavior from a simple principle like do whats best for humanity. we might imagine that in the future, more sophisticated ai systems will evaluate the possible consequences of their actions explicitly2in natural language, and connect them back to simple and transparent governing principles. in this work we will begin to explore these issues in a straightforward way. first, we demonstrate that if we construct a constitution to target specific, problematic behavioral traits in ai, we can use cai to train a trait preference model3(trait pm) that discourages these traits (see 2). second, we show that if instead we use a constitution that focuses only on doing what is best for humanity, the resulting good for humanity preference model (gfh pm) achieves good, but slightly weaker performance at discouraging these traits (see figure 1). finally, we fine-tune a language model via reinforcement learning (rl) using the gfh preference model as a reward model, resulting in a policy model trained by reinforcement learning from ai feedback (rlaif). we show that this policy model is almost as harmless (in more conventional terms [6], as judged by crowd- workers in terms of e.g. toxicity, unethical illegal recommendations, etc) as a policy model trained using cai that specifically targets these issues, as shown in figure 2. moreover, the good for humanity models are significantly less prone to developing traits such as a stated desire for power-seeking or survival. how- ever, this approach has some significant drawbacks with regard to handing value specification