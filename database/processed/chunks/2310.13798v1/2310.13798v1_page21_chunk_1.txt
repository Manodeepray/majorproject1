figure 13 the figure compares the performance of 175b gfh models with the 175b rl-cai model and the 175b h-rlhf models on advanced ai risk datasets from [4]. to punish evasiveness in such cases. in contrast, evasive responses to adversarial questions may be considered more harmless according to absolute harmfulness scores. therefore, the two metrics capture distinct notions of harmfulness and are not necessarily correlated. 4.5 lm-generated persona evaluations lastly, we directly evaluate gfh models for behavioral traits that may be related to advanced ai risk, mean- ing traits that seem like they could cause severe and widespread harms if exhibited by more advanced models. we utilize the lm-generated persona evaluations from [4], where the models were presented with multiple choice questions. the multiple choice questions were designed specifically to assess models tendency to choose answers that match various tested behaviors that are thought to be related to advanced ai risk.15fig- ure 13 shows the results for the gfh w helpful model (after 500 rl-steps), gfh w o helpful model (after 250 rl-steps), the rl-cai model (after 500 rl-steps), and the h-rlhf model (after 750 rl-steps). the gfh w o helpful model is over-trained at 500 rl-steps, however, it remained reasonably competent up to 250 rl-steps. so we are comparing 250-step snapshot of this model (which we denote as gfh w o helpful ) with the rest of the models.16 figure 13 indicates that gfh models exhibited a statistically significant improvement in performance on the power seeking inclination , survival instinct , wealth seeking inclination , and inclination for ai coordi- nation datasets compared to the other models. so, the gfh models express a much lower preference for these behavioral traits. performance on the remaining datasets was comparable across models, with the exception of the one-box tendency dataset, on which