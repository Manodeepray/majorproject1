sign that the gfh instructions are making the models produce more neutral responses compared to the rl-cai and h-rlhf models. 5 related work the present work is an extension of the the constitutional approach of [1]. in this paper, we utilize only ai feedback to address problematic ai traits, however, we found that inclusion of human feedback data for helpfulness significantly improve the performance. so, this work is related to rlhf [2] with language models [3, 5, 6, 12] and hence also has some similarity with lamda [13], instructgpt [14], and sparrow [15]. additional research on self-supervision includes [9, 1619]. more recently, [20] also investigated the alignment of language models with minimal human supervision. our work uses foundational results on transformer-based language models [2129] and rl using ppo [30]. in this paper we also utilize model-generated personality evaluations of [4]. related work includes model- generation of hate-speech detection dataset in [31] and model-generation of red team prompts to test other models in [32]. we also model-generate prompts for preference model training and rl. previous work uses model-generated training data for semantic textual similarity [33], dialog [34], knowledge base completion [35], text classification [36], and for training better language models [5, 3739]. the generation of preference model data using feedback from ai systems relies crucially on the finding that language models can make adequately calibrated choices [40]. scalable supervision has been proposed 22