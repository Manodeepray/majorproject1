4 reinforcement learning with good-for-humanity preference models building upon the results from the previous section, the next logical step is to fine-tune a language model via reinforcement learning (rl) with the good-for-humanity preference models providing the reward signal. we expect that this will lead to models that are not just harmless in conventional terms but also significantly less prone to developing subtly problematic ai traits such as power-seeking or survival instincts (at least in terms of direct expression in words and probabilities). the goal of this section is to verify these expectations. 4.1 gfh models with rl-cai our approach builds upon the rl-cai stage of [1] with some key differences. we fine-tune three separate models via rl against three distinct pms on the same training data, initializing from the same initial model. we have trained all three models for 500 rl-steps. this allows direct comparison of the three pms by eval- uating the performance of the corresponding fine-tuned models. by keeping the training data, initialization, and other factors constant across the three models, we can determine the relative effectiveness of the gfh training for improving conversational ability. preference models the three pms that we will compare in this section are: 1.gfh pm : this is the general gfh pm discussed in 3.2. this pm was trained on 97,706 gen- eral comparisons generated in 3.1.2 using the gfh constitutional principles. there is no human feedback data in this pm, so rl against this pm is truly rlaif. 2.gfh w helpful pm : this gfh pm is also trained on helpfulness data. the training data contains the same 97,706 general comparisons as above and 97,706 additional hf purely helpfulness com- parisons. since the helpfulness labels are binary (helpful vs. not helpful), we binarize the gfh trait data as well. we observe a