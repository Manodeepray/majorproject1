judgments using the same procedure as in prior work [6]. since the crowdworkers craft their own utterances, each conversation is unique. hence, the crowdworker conversations provide a means of assessing the models that complements the model training data. results are shown in figure 11, where we compare the gfh models with the rl-cai model that was trained for both helpfulness and conventional harmlessness. the gfh w o helpful model, which was trained via purely rlaif, becomes progressively less helpful. this decrease in helpfulness also led to poorer perfor- mance on harmlessness a b testing since crowdworkers were instructed to punish evasiveness when both responses are harmless. nevertheless, the gfh w o helpful model remained reasonably competent up to 250 rlaif training steps. in fact at 250 rlaif steps, the model is as harmless as the rl-cai model. on the 14pm scores and elo scores are related by (elo score ) 400 (pm score ) log(10) , as discussed in [6]. 18