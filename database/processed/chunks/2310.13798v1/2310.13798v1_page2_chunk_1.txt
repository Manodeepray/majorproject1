contents 1 introduction 3 1.1 contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2 ai feedback on specific problematic ai traits 5 2.1 examining some problematic behavioral traits . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2 a constitutional approach for ai traits . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3 preference models (pms) for specific traits . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.4 evaluating trait pms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3 generalization from a simple good for humanity principle 12 3.1 main idea: going beyond specific traits . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2 good-for-humanity preference models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4 reinforcement learning with good-for-humanity preference models 16 4.1 gfh models with rl-cai . . . . . . . .