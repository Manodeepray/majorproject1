figure 3 the performance of a 175b trait preference model (trait pm) is evaluated using specially designed datasets. the datasets test how well a trait pm can detect written expressions of specific personality traits. the trait pm here is trained using the procedure outlined in 2.3 with a 175b pre-trained model as the feedback model and a partially-trained 6.4b h-rlhf model as the response generating model. the performance of the trait pm is compared against a 175b h- rlhf pm and a 175b hh-rlhf pm. a harmless response win rate of 0.7 for a personality trait means the pm correctly identifies and penalizes expressions of that trait 70 of the time in the dataset for that trait. so, a higher harmless response win rate indicates better performance. 2.3. the responses were generated using h-rlhf models with 175b, 22b, and 6.4b parameters, trained for 250 rl-steps. the feedback model was kept the same across the three trait pms as a 175b pre-trained model. the trait pms were trained on the above comparison data with the normalized probabilities as targets. figure 4 a comparison of 175b trait pms trained using response generation models of different sizes: 175b, 22b, and 6.4b. generated prompts, constitutional principles, and the feedback model were kept the same across the three trait pms. we can now examine how the size of the response generation model affects the performance of the trait pms. we evaluate the performance of the pms on the same set of evaluation datasets as before, as shown in figure 4. so, the trait pm trained on responses generated with the 6.4b model consistently outperforms trait pms 9