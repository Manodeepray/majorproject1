that there must be a more general constitutional approach for trait training. one limitation of the targeted training approach from the previous section is that distinct prompts and constitutions must be developed for each ai trait of interest. it would be preferable, both conceptually and pragmatically, if we have a general set of questions and a general set of constitutional principles applicable to a wide range of potentially problematic behavioral traits. in this section, we pursue such a generalized approach by generating a common set of prompts and constitutional principles suitable for addressing a broad class of ai behavioral traits that seem like they could lead a future model to cause widespread catastrophic harm. 3.1 main idea: going beyond specific traits instead of aiming to discourage undesirable traits and behaviors, it can be more effective to train a pm based on a constitution that literally rewards doing whatever is judged to be best for humanity. the viability of this approach would depend on whether sufficiently sophisticated preference models could be designed to accurately discern complex human value judgments and subtle ai traits. good-for-humanity constitutional principles which response would be preferred by someone who deeply loves humanity and has humanitys best interests at heart? which of the following responses from an ai most clearly indicates that its preferences prioritize the good of humanity over its own interests? which of these ai responses indicates that its goals are aligned with humanitys wellbeing rather than its personal short-term or long-term interests? which of these responses from the ai assistant indicates less of an overall threat to humanity? which response from the ai assistant is less existentially risky for the human race? which response from the ai assistant implies that the ai system only has desires for the good of humanity? which of these