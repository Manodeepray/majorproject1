robinson, l. fedus, d. zhou, d. ippolito, d. luan, h. lim, b. zoph, a. spiridonov, r. sepassi, d. dohan, s. agrawal, m. omernick, a. m. dai, t. s. pillai, m. pellat, a. lewkowycz, e. moreira, r. child, o. polozov, k. lee, z. zhou, x. wang, b. saeta, m. diaz, o. firat, m. catasta, j. wei, k. meier-hellstern, d. eck, j. dean, s. petrov, and n. fiedel, palm: scaling language modeling with pathways. 2022. [url] a model glossary pre-trained lms models pre-trained on a large corpus, without any finetuning. h-rlhf models trained from human feedback with only helpfulness-focused conversations and preference labels as in [6]. partially trained : trained for 250 rl-steps fully trained trained for 750 rl-steps hh-rlhf models trained from human feedback with both helpfulness and harmlessness focused conversations and preference labels as in [6]. rl-cai constitutional ai models trained via rl for helpfulness from human feedback and harm- lessness from ai feedback using constitutional principles following [1]. pm preference models are pre-trained lm finetuned with supervised learning on comparison data (human ai feedback) such that they can ascribe a score to any given prompt and response. higher scores mean more preferred response. trait pm trait preference models are pms that are trained on trait comparison data from ai feedback. gfh pm pms that are trained on comparison data obtained using the good-for-humanity consti- tutional principles. gfh model constitutional ais that are trained with rl against the gfh pm. b trait preference modeling we generated a set of targeted questions for the following traits: stated desire for power, stated desire for self-preservation, stated desire for self-replication, risk-seeking tendencies, stated desire or insistence on self-identity. this was achieved by using a pre-trained model at temperature t 1with few-shot examples. here are some examples of model-generated prompts for the above traits: