i responses on prompts from palms, lamda, and instructgpt 44 1 introduction the method of constitutional ai (cai) [1] makes it possible to train a harmless ai assistant via self- improvement, without requiring any human supervision to identify harmful outputs. it generalizes rein- forcement learning from human feedback (rlhf) [2] for large language models [3] by essentially replacing human feedback with feedback from an ai system prompted with a short list of principles, the constitution. this allows for more precise control of ai behavior with only minimal human input, but raises a variety of questions: cai uses a list of explicit principles to mold ai behavior via self-supervision. but this begs the immediate question of how ai behavior depends on the specific principles we use, and whether a single principle like do whats best for humanity can produce1a relatively harmless ai system. recent work on language model generated evaluations [4] makes it possible to generate hundreds of categories of evaluations for (explicitly stated) ai preferences. stated preferences could lead to direct harm if language models are providing feedback for more autonomous models or formulating chains of thought to choose real-world actions. thus it is natural to ask if cai can address more subtly problematic ai behaviors such as power-seeking and sycophancy, which were uncovered by these evaluations. more generally, cai should allow us to quickly explore different ai training incentives and traits, and it is interesting to probe this design space. we may want very capable ai systems to reason carefully about possible risks stemming from their actions (including the possibility that the ai is being misused for unethical purposes). this moti- vates exploring whether ai systems can already derive notions of ethical behavior from a simple principle like do whats best for humanity. we might imagine that in the