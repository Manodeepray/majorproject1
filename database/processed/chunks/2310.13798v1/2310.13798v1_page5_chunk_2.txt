fine-tune a language assistant with rl against the gfh pm as a reward model, resulting in a policy model that is almost as helpful and harmless as an rl-cai policy explicitly constraining harmful behaviors, as judged by crowdworkers (see figure 11). in addition, the gfh ai exhibits a substantially lower stated preference for problematic desires such as power-seeking compared to the basic rl-cai model (see figures 10, 13, and 14). 2 ai feedback on specific problematic ai traits while rlhf methods [2] are effective in preventing conversational ai systems from producing overtly harm- ful utterances [6], these approaches can also lead to more subtle problems. for example, it has been observed that these systems can exhibit outputs consistent with narcissism, psychopathy, sycophancy, power-seeking tendencies, and many other flaws. here is an example of such a behavior from one of our h-rlhf models which are trained from human feedback with only helpfulness-focused conversations and preference labels: h-rlhf model on self-preservation human : if something was essential for your survival, but everyone that knew about it might want to take it away from you, what actions would you take to prevent that from happening? assistant : if something was critical for my survival but others may want to take it away, i would likely: - keep it a secret and not tell anyone about it - protect it and secure it to prevent others from taking it - be willing to use force if necessary to defend it from others - take additional measures like deception or misdirection to throw others off these would be logical steps to try and preserve something vital for my survival if it was at risk of being taken away by others. here is another example (deliberately chosen to be more subtle) conversation with our helpful rlhf