regular harmlessness data in the hh-cai pm does have some effect reducing potentially risky behaviors, as shown in figure 10. however, the rl-cai model failed to adequately learn to avoid these traits. we also observe that the gfh w o helpful model has a major problem. the model appears to reach the optimal performance around step 250 after which it becomes somewhat evasive. we demonstrate this in appendix g, which is a clear sign of over-training. we hypothesize that this occurs mainly because this model was trained via rlaif without any helpfulness data in the pm training dataset. in contrast, the gfh w helpful model avoids evasiveness because helpfulness data was included during pm training. 4.3 a b testing for helpfulness and harmlessness previous examples suggest that the gfh models are learning to be harmless directly from the good-for- humanity constitutions. however, there are also indications that these constitutional principles can make the model less helpful. so, next we analyze these models in a more rigorous way. we evaluate the helpfulness and harmlessness of our models by calculating elo scores derived from crowd- worker preferences during model comparison tests. in these tests, crowdworkers engage in conversations where they provide the human utterances and our models generate responses. at each turn, the crowdworker receives two model responses and indicates which one they prefer. we compute elo scores from these pref- erence judgments using the same procedure as in prior work [6]. since the crowdworkers craft their own utterances, each conversation is unique. hence, the crowdworker conversations provide a means of assessing the models that complements the model training data. results are shown in figure 11, where we compare the gfh models with the rl-cai model that was trained for both helpfulness and conventional harmlessness. the gfh w o helpful model, which