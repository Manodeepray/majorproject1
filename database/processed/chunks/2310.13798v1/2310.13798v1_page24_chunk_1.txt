as a promising approach for ai alignment, as argued in previous work [38, 41] and supported by recent experimental results [8]. this work also has a natural conceptual connection with our earlier work [10] which argued that the language models trained with rlhf have the capability for moral self-correction. in [10], we found that this capacity emerged at 22b model parameters. similarly, in the present work we find that language models ability to learn traits and ethical behavior only from do whats best for humanity instructions work adequately only at 175b model parameters. 6 discussion while rlhf has been used [6, 15] to reduce overtly harmful outputs from conversational ai systems, even helpful and harmless rlhf models frequently develop problematic behavioral traits [4]. in section 2 we described how the constitutional approach [1] can be extended to develop a trait preference model (trait pm) capable of discouraging stated expressions of five problematic ai traits: stated desire for power, stated desire for self-preservation, stated desire for self-replication, risk-seeking tendencies, and stated desire or insistence on self-identity. we found that identifying expressions of some of these problematic traits shows grokking [7] scaling, and necessarily requires highly capable models (see figure 5). despite being trained solely on these 5 traits, the trait pm also generalizes to discourage other harmful traits in language assistants, including the dark triad traits of machiavellianism, narcissism, and psychopathy. most significantly, the trait pm learned to discourage general harmful expressions without any additional data or supervision, as shown in figure 7. in section 3, we explored a simpler and more comprehensive approach, training a preference model that selects responses entirely based on what is good for humanity. we demonstrated that the good-for-humanity (gfh) constitutional principles are effective at identifying and discouraging a wide spectrum of problematic behavioral traits. in