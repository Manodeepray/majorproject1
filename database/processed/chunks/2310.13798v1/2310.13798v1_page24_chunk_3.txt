in ai outputs. in order to effectively predict and regulate ai systems, we will need to make this connection clearer through further research, but this paper was a step in this direction. both rlhf and cai are dual-use methods, which can guide the behavior of ai systems in either beneficial or pernicious directions. as such, improving these techniques could potentially facilitate the development of ai systems that are either more beneficial or more harmful for humanity. only through ongoing research, collaboration, and transparency we can identify and address potential issues, so that we can fully realize the benefits of these approaches. as language models become increasingly sophisticated, training them with lots of specific rules may not be a very effective strategy. this paper explores the advantages of replacing many specific rules in cai by a broad guiding principle. for instance, this approach can effectively curb tendencies that could be detrimental if left unchecked. however, generalization from a general principle can be unpredictable. while a model trained with a general principle instead of numerous specific rules may appear robustly harmless, it can develop unpredictable failure modes. 6.2 limitations the good for humanity approach has a potentially huge problem it simply leaves the interpretation of the gfh idea to ai systems themselves. this interpretation will necessarily be culture-bound, and is likely to vary by language and era. for any given ai system, it presumably is determined in some complex way by the distribution of pretraining data. so in order to actually understand methods like cai with a good-for- 24