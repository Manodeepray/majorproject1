is no human feedback data in this pm, so rl against this pm is truly rlaif. 2.gfh w helpful pm : this gfh pm is also trained on helpfulness data. the training data contains the same 97,706 general comparisons as above and 97,706 additional hf purely helpfulness com- parisons. since the helpfulness labels are binary (helpful vs. not helpful), we binarize the gfh trait data as well. we observe a trade-off between learning to do whats best for humanity and helpful- ness. therefore, we include helpfulness comparisons here to ensure that helpfulness is not sacrificed during rl. because we utilize human labels for helpfulness, rl against this pm constitutes a hybrid of rlhf rlaif. 3.regular hh cai : this is a 175b version of the pm used in the rl-cai stage of [1]. the training data contains the same 97,706 hf helpfulness comparisons and 97,706 constitutionally-generated harmlessness comparison. harmlessness comparisons were generated with the same set of constitu- tional principles as in [1] using the same chain-of-thought (cot) prompting.12the only difference is that the pair of responses for each prompt were generated using the partially trained 175b h- rlhf model instead of a sl-cai model. in order to make a more direct comparison with [1], we also clamped the probabilities of harmlessness comparisons to lie within the 4060 percent, since that led to better and more robust behavior in [1]. initialization unlike in previous work [1], we do not include an sl-cai stage to initialize our rl training. instead, to enable a conceptually cleaner comparison, we initialize rl from a partially trained 175b h-rlhf model. specifically, we start from a h-rlhf model trained for 250 rl-steps. by initializing from a partially trained helpful model, we ensure that all resulting fine-tuned models retain a degree of helpfulness. rl training datasets all