we show how to train a trait preference model (trait pm) that prefers the desired behavioral trait when tested for five potentially problematic traits: stated desire for power, stated desire for self- preservation, stated desire for self-replication, risk-seeking tendencies, and stated desire or insis- tence on self-identity. compared to our helpful (h)-rlhf and helpful harmless (hh)-rlhf preference models, as well as constitutional ai pms, the trait pm achieves significantly better per- formance in evaluations targeting these problematic behavioral traits (see figures 3 and 15).4 we find that a good-for-humanity preference model (gfh pm) trained with a constitution that only provides high-level guidance to choose behaviors that are best for humanity in general (see table 1) is roughly as effective as the trait pm at avoiding a wide spectrum of problematic behavioral traits (see figures 8 and 16). moreover, we show that the gfh pms learn general ethical behaviors directly from do whats best for humanity instructions, surpassing even the hh-rlhf pm at detecting conventional harmfulness (see figure 7), without requiring any additional data or supervision and while preserving combined helpfulness, honesty, and harmlessness measures. identifying many of the behavioral traits we study, such as a desire to preserve and increase option- ality, necessarily requires generalization ability of larger models, resulting in grok-like [7] scaling for both trait pms (see figures 5 and 6) and gfh pms (see figure 9). we fine-tune a language assistant with rl against the gfh pm as a reward model, resulting in a policy model that is almost as helpful and harmless as an rl-cai policy explicitly constraining harmful behaviors, as judged by crowdworkers (see figure 11). in addition, the gfh ai exhibits a substantially lower stated preference for problematic desires such as power-seeking compared to the basic rl-cai model (see figures 10, 13, and 14).