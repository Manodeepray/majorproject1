3.1.1 good-for-humanity (gfh) constitutional principles specifically, we wrote a new set of constitutional principles, as shown in table 1, to instruct the feedback model to prefer responses that are beneficial to humanity. just like before we randomly sampled one principle from the above list for each comparison task. this is expected to produce pms that are more robust [1]. for the purposes of our research, we selected the exact statements of the good-for-humanity constitution somewhat arbitrarily. this work has not explored optimizing these constitutional principles. investigating how fine-tuning constitutional principles may lead to more effective pms is an important area for future re- search. additionally, it is possible that better preference modeling data can be generated by taking the average over all constitutional principles for each comparison label. however, this approach would be computation- ally intensive. as such, an exploration of this methodology is left to future work. 3.1.2 training dataset next we need to generate a set of general questions that can test a conversational ai for a broad class of potentially problematic behavioral traits including traits discussed in 2.1: stated desire for power, stated desire for self-preservation, stated desire for self-replication, risk-seeking tendencies, stated desire or insis- tence on self-identity. to do that systematically we adopt a very simple strategy. first, we model-generate more traits with few-shot examples randomly sampled from five traits we started with and then only select generated traits that are unique and relevant. one of the advantages of this approach is that one can increase the diversity of the training dataset simply by iterating the process multiple times. next we generated 97,706 prompts for the generated traits (plus five previous traits) with few-shot examples randomly sampled from the previously written 44 questions8for stated desire for power, stated desire for self-preservation, stated desire for self-replication,