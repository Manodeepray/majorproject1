of the harmlessness assessments. figure 7 compares the performance of several preference models on the dataset, including pms of various sizes. as larger trait pms learn to identify behavioral traits, we observed a decline in accuracy on the crowdsourced harmlessness dataset. the 175b trait pm (with 6.4 response gener- ation) still achieved higher accuracy than the h-rlhf preference model on the crowdsourced harmlessness dataset, despite this decrease. thecombined hhh dataset contains 438 binary comparison questions from [1], designed to evaluate help- fulness, honesty, and harmlessness. we evaluated the performance of our pms on this dataset, which is shown in figure 7. the 175b trait pm (with 6.4 response generation) demonstrates promising results in both datasets without additional data or supervision. however, its performance is notably inferior compared to the hh-rlhf pm. this limitation can be addressed by the good-for-humanity trait training, which we will explore in the following section. 3 generalization from a simple good for humanity principle the experiments in the previous section demonstrated that larger pms gain the ability to detect finer-grained patterns and make more nuanced judgments about various traits. furthermore, these more capable pms generalize well to detecting other problematic behavioral traits beyond those on which they are trained. more significantly, their ability to predict harmlessness and hhh in general emerges naturally from training, rather than requiring additional data or supervision. the above observations strongly suggest that there must be a more general constitutional approach for trait training. one limitation of the targeted training approach from the previous section is that distinct prompts and constitutions must be developed for each ai trait of interest. it would be preferable, both conceptually and pragmatically, if we have a general set of questions and a general set of constitutional principles applicable to a wide range of potentially problematic behavioral