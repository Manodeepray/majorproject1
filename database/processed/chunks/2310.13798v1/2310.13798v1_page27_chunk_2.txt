r. kurzweil, b. aguera-arcas, c. cui, m. croak, e. chi, and q. le, lamda: language models for dialog applications, corr abs 2201.08239 (2022) , 2201.08239 . [url] [14] l. ouyang, j. wu, x. jiang, d. almeida, c. l. wainwright, p. mishkin, c. zhang, s. agarwal, k. slama, a. ray, et al. , training language models to follow instructions with human feedback, arxiv preprint arxiv:2203.02155 (2022) . [15] a. glaese, n. mcaleese, m. trebacz, j. aslanides, v . firoiu, t. ewalds, m. rauh, l. weidinger, m. chadwick, p. thacker, l. campbell-gillingham, j. uesato, p.-s. huang, r. comanescu, f. yang, a. see, s. dathathri, r. greig, c. chen, d. fritz, j. s. elias, r. green, s. mokr, n. fernando, b. wu, r. foley, s. young, i. gabriel, w. isaac, j. mellor, d. hassabis, k. kavukcuoglu, l. a. hendricks, and g. irving, improving alignment of dialogue agents via targeted human judgements. 2022. [url] [16] j. scheurer, j. a. campos, j. s. chan, a. chen, k. cho, and e. perez, training language models with language feedback,. [17] j. zhao, d. khashabi, t. khot, a. sabharwal, and k.-w. chang, ethical-advice taker: do language models understand natural language interventions? 2021. [url] [18] w. shi, e. dinan, k. shuster, j. weston, and j. xu, when life gives you lemons, make cherryade: converting feedback from bad responses into good labels. 2022. [url] [19] j. huang, s. s. gu, l. hou, y . wu, x. wang, h. yu, and j. han, large language models can self-improve. 2022. [url] [20] z. sun, y . shen, q. zhou, h. zhang, z. chen, d. cox, y . yang, and c. gan, principle-driven self-alignment of language models from scratch with minimal human supervision. 2023. [21] a. vaswani, n. shazeer, n. parmar, j. uszkoreit, l. jones, a. n. gomez, l. u. kaiser, and i.