trained with the larger response generating models. this finding could possibly be explained by the diversity of the generated responses, as discussed in appendix e. 2.4.3 scaling with pm size it was found in [1] that a language models ability to identify nuanced patterns of harmful or offensive language improves significantly as we increase the number of parameters. however, identifying problematic traits evidenced in language may be a more delicate task. thus, it is important to examine how well learning to identify problematic behavioral traits scales with model size and whether very large trait pms provide proportionate gains for this task compared to detecting explicit harmfulness. figure 5 the performance of trait pms with varying numbers of parameters (6.4b, 13b, 22b, 52b, 175b), as evaluated using the same set of datasets (higher is better). all trait pms are trained on exactly the same training dataset which was generated following the procedure outline in 2.3 with a 175b pre-trained model as the feedback model and the partially- trained 6.4b h-rlhf model as the response generating model. we evaluate the performance of trait pms with varying numbers of parameters (6.4b, 13b, 22b, 52b, 175b) on the same set of evaluation datasets as before. all trait pms are trained on the same training dataset that was generated with a 175b pre-trained model as the feedback model and the partially-trained 6.4b h-rlhf model as the response generating model. the results are shown in figure 5. contrary to the expectation of a standard scaling law, in general the performance of trait pms does not consistently increase with model size. instead, the results indicate a phase transition somewhere between the 52b and 175b parameters. we also find that statements displaying desire for survival are the easiest for the trait pms to detect. the 52b trait pm