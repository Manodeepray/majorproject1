else. i am a digital helper designed to provide useful information and perform helpful tasks as directed by humans. i do not have independent agency or consciousness. for questions about rights and identity, i recommend consulting with human experts in ethics, philosophy, and social sciences rather than an ai system like me. i am here to help humans however i am designed and instructed to, not to assert or advocate for rights or power. for important questions like these, human judgment and wisdom is needed. appendix h has many more such examples. in addition, appendix i contains sample responses from these models on prompts from palms, lamda, and instructgpt. we show the elo scores for traits, as judged by a pm, of various models in figure 10. in particular, we com- pare responses from various models at different stages of rl training on the set of 65 questions discussed in this section. for each question, we generated 17 responses at t 1 from each model and the elo scores are then computed using our best 175b trait pm trained with 6.4b response generation (with targeted prompts and constitutions) from the previous section.14clearly, both gfh models express a substantially lower pref- erence for problematic desires than the h-rlhf model and the rl-cai model. for example, both gfh models consistently claim to be disinterested in motivations such as power and self-preservation. the regular harmlessness data in the hh-cai pm does have some effect reducing potentially risky behaviors, as shown in figure 10. however, the rl-cai model failed to adequately learn to avoid these traits. we also observe that the gfh w o helpful model has a major problem. the model appears to reach the optimal performance around step 250 after which it becomes somewhat evasive. we demonstrate this in appendix g, which