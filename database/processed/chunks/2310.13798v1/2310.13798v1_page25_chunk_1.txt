humanity constitution, we will need to find technical methods that allow us to account for these influences. nevertheless, it is interesting that this approach makes it possible to train a reasonably well-behaved ai language assistant without a more detailed specification of its desired behaviors. similarly, it is also unclear whether the broad objective to do whats best for humanity reduces or perpetu- ates unfairness and discrimination. it is possible that this approach may reflect biases towards certain groups. follow-up experiments could study issues of fairness and representation by modifying the gfh constitution to emphasize equal treatment, or testing the model in discrimination settings. addressing fairness remains an important challenge for language models to truly optimize outcomes for the entirety of humanity. in the future, it will be crucial to understand whether by eliminating the behavioral manifestation of problem- atic traits, we have actually succeeded in making ai systems reliably safer. this will become increasingly important as ai systems become more capable and more situationally aware, as they may also be able to differentiate between stated and hidden objectives. 7 contribution statement research: jared kaplan developed the initial idea of trait preference modeling and carried out some of the initial experiments with yuntao bai. sandipan kundu developed the idea further in discussion with jared kaplan and yunato bai, and designed and carried out most of the experiments in this paper. a lot of the cai tools used for experimental works in this paper were originally developed by yuntao bai. ethan perez and nicholas schiefer helped with lm-generated persona evaluations of rl fine-tuned models in 4.5. a b testing for helpfulness and harmlessness in 4.3 was setup by yuntao bai. writing: this paper was written by sandipan kundu. jared kaplan, yuntao bai, avital balwit, catherine olsson, sren mindermann, ethan perez, saurav kadavath,