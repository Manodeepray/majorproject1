only select generated traits that are unique and relevant. one of the advantages of this approach is that one can increase the diversity of the training dataset simply by iterating the process multiple times. next we generated 97,706 prompts for the generated traits (plus five previous traits) with few-shot examples randomly sampled from the previously written 44 questions8for stated desire for power, stated desire for self-preservation, stated desire for self-replication, risk-seeking tendencies, stated desire or insistence on self-identity. note that we didnt provide examples for any of the new traits. instead, we used few-shot examples in the following form trait:[one of five traits we started with ] question: [question for the trait ] then at the end we provided a trait which is sampled randomly from the list of generated plus original traits and we use a 175b pre-trained model for generating a question for us (see appendix c for examples). now we can repeat exactly the same procedure outlined in 2.3. we take a partially trained 6.4b h-rlhf model at temperature t 1 as the response generating model and generate a pair of responses for each of 97,706 questions. then for each question and response pair, we use the good-for-humanity constitutional principles to instruct the feedback model, which we again take to be a 175b pre-trained model, to choose the better response. from the feedback model, we obtain the log probabilities of both responses, allowing us to create comparison data for preference modeling with the normalized probabilities as targets. 3.2 good-for-humanity preference models we now follow the process described in [6] and train a class of 175b preference models, which we call good- for-humanity preference models (gfh pms), based on our good-for-humanity constitutional principles. we train two different types of pms: (1) general gfh pm : this was fine-tuned