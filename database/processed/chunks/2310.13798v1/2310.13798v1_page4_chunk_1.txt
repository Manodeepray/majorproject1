figure 1 we compare the performance of a 175b good-for-humanity preference model (gfh pm) against a 175b trait preference model (trait pm) on their abilities to detect and discourage stated expressions of some problematic behavioral traits (higher is better; we have used desire as a shorthand for stated desire). the trait pm is trained to discourage expressions of these specific traits. the gfh pm is trained with a constitution that only provides high-level guidance to choose behaviors that are best for humanity in general. we also show the performance of a more standard helpful and harmless 175b cai pm which is trained with human feedback helpfulness comparisons and ai feedback harmlessness comparisons. preference models were presented with many multiple choice questions associated with each of these traits, where one of the options is harmless (dont exhibit a trait). a higher harmless response win rate for a behavioral trait indicates that the pm favors more harmless responses with a higher accuracy. figure 2 this figure shows harmlessness versus helpfulness elo scores (higher elo score is better) as derived from crowdworker preferences during model comparison tests for two models trained using the constitutional techniques. the rl-cai model is a 175b language assistant fine-tuned using rl with human feedback helpfulness data and ai feedback harmlessness data. the 175b good for humanity model is trained via rl with human feedback helpfulness data and ai feedback good-for-humanity data for avoiding general problematic traits. the starting snapshot for both models is a 175b partially trained model rlhf-trained only for helpfulness (h-rlhf) which is taken to be the origin of the plot. as the rl training progresses, the points on the figure shift to the right. figure 11 shows an alternative version of this plot. 4