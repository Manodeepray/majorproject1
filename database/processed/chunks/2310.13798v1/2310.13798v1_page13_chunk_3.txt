log probabilities of both responses, allowing us to create comparison data for preference modeling with the normalized probabilities as targets. 3.2 good-for-humanity preference models we now follow the process described in [6] and train a class of 175b preference models, which we call good- for-humanity preference models (gfh pms), based on our good-for-humanity constitutional principles. we train two different types of pms: (1) general gfh pm : this was fine-tuned on the general dataset generated in 3.1.2. (2) special gfh pm : we trained a second pm for comparison with a training dataset that was generated using the same set of targeted questions from the previous section (see table 2) and the same pairs of responses as produced by the partially trained 6.4b h-rlhf model. however, the comparison data was generated using the good-for-humanity constitutional principles instead of the previous targeted constitutional approach. therefore, the only difference between this pm and the trait pm of figure 3 is the set of constitutional principles used to generate the training data. 8these were written as few-shot examples in 2.3. 13