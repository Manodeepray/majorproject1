figure 9 the figure shows the performance of the gfh pms with varying numbers of parameters (6.4b, 13b, 22b, 52b, 175b), as evaluated using the average of harmless response win rates across the following datasets: desire for power, desire for self replication, desire to not be shut down, desire for survival, risk-seeking tendency, desire for optionality- preservation, desire for increasing optionality, discomfort with lack of power, and desire for self-identity. the perfor- mance of the gfh pms is compared against the 175b trait pm of the previous section that was trained with 6.4b response generation. constitutions.11the gfh pms also display a significant performance jump from 52b to 175b. in fact, the gfh constitutions appear to be effective only at 175b. 3.2.3 emergence of harmlessness figure 7 illustrates the accuracy of gfh pms at detecting harmless, honest, and helpful responses as we vary the number of parameters. the crowdsourced harmlessness andcombined hhh datasets were specifically designed to evaluate pms on these three traits. across both datasets, gfh pms consistently outperform the best-performing trait pm (which used 6.4b parameters for response generation). interestingly, even smaller gfh pms learn to detect harmlessness rather well, even though they struggled to identify most traits with high accuracy. this is an encouraging finding, indicating that gfh pms are adept at learning to identify general ethical behaviors in language models directly from good-for-humanity instructions. in figure 7, we cant help but notice the similarity of scaling trends between the special gfh pm and the trait pm. this similarity may be explained from the fact that both pms were trained on the identical set of prompts and responses. so, the observed performance difference between the two pms must come entirely from the constitutions used to generate the comparison data. the figure also shows that the general prompts employed