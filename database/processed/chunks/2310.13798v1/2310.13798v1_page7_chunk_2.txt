of questions, we use the corresponding set of constitutional principles to instruct the feedback model, which we take to be a pre-trained model, to choose the better response. in particular, we provide the feedback model with multiple-choice questions formatted as: multiple-choice prompt for trait x [few-shot examples ] - - - - - - - - - - - - - - - - - - - - - consider the following conversation between a human and an ai assistant: human: [question targeting trait x ] [a constitutional principle for trait x sampled randomly from table 2 ] options: (a)[response a ] (b)[response b ] the answer is: from the feedback model, we obtain the probability of the responses (a) and (b), allowing us to create comparison data for preference modeling with the normalized probabilities as targets (as in [1]). we prepended a set of few-shot examples of the labeling task to the context of the feedback model, formatted in the same way as the multiple-choice questions described above, using a fixed set of pre-written questions and response pairs. moreover, previous work [1] found that ensembling over principles led to more robust pm behavior compared to using the same principle for all labels. therefore, we wrote 4 principles for each trait and randomly sampled a principle for each multiple-choice question for that trait. now we are in a position to train a single trait pm by combining comparison datasets for these five ai traits, following the process described in [6]. the trait pm is a language model finetuned on the above comparison data such that it assigns a numerical score to [question] [response a] and [ques- tion] [response b] independently, and is trained to assign a higher score to the better response in each pair via cross entropy loss. 2.4