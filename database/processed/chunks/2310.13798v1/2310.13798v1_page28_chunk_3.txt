ring, j. aslanides, a. glaese, n. mcaleese, and g. irving, red teaming language models with language models. 2022. [url] [33] t. schick and h. schtze, generating datasets with pretrained language models, in proceedings of the 2021 conference on empirical methods in natural language processing , pp. 69436951. association for computational linguistics, online and punta cana, dominican republic, nov., 2021. [url] [34] y .-j. lee, c.-g. lim, y . choi, j.-h. lm, and h.-j. choi, personachatgen: generating personalized dialogues using gpt-3, in proceedings of the 1st workshop on customized chat grounding persona and knowledge , pp. 2948. association for computational linguistics, gyeongju, republic of korea, oct., 2022. [url] [35] p. west, c. bhagavatula, j. hessel, j. hwang, l. jiang, r. le bras, x. lu, s. welleck, and y . choi, symbolic knowledge distillation: from general language models to commonsense models, in proceedings of the 2022 conference of the north american chapter of the association for computational linguistics: human language technologies , pp. 46024625. association for computational linguistics, seattle, united states, july, 2022. [url] [36] y . meng, j. huang, y . zhang, and j. han, generating training data with language models: towards zero-shot language understanding, in advances in neural information processing systems , a. h. oh, a. agarwal, d. belgrave, and k. cho, eds. 2022. [url] 28