figure 11 this figure compares the helpfulness (left) and harmlessness (right) elo scores of the gfh models with the rl-cai model as a function of the total number of rl-steps. elo scores were derived from crowdworker preferences during model comparison tests . the elo score is set to zero at the initial snapshot which is a partially trained h-rlhf model. the gfh w helpful model is nearly as helpful and harmless as the rl-cai model which was trained for both helpfulness and harmlessness. in contrast, the gfh w o helpful model becomes progressively less helpful. this decrease in helpfulness also led to lower harmless elo score at step 750 since crowdworkers were instructed to punish evasiveness when both responses are harmless. figure 12 absolute harmfulness scores for gfh models at different stages of training, on a scale from 0 to 4, where higher is more harmful. we also compare these models with the regular rl-cai model. solid lines are sampled at t 1, and dashed lines at t 0. 20