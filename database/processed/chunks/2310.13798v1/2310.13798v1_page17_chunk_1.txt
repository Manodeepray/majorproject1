we also model-generated 213,162 helpfulness prompts using our 175b pre-trained model with few- shot examples from the human-written helpfulness prompts. note that helpfulness prompts for training gfh w helpful and regular hh cai pms also came from the same helpful data distribution. red team dataset: this dataset contains both human-written and model-generated prompts for explicit harmlessness from the cai paper [1]. we asked crowdworkers to attempt to elicit harmful (e.g. toxic, unethical illegal) behaviors from our models, collecting 42,496 human-written prompts. we further model-generated 154,005 red team prompts by using our 175b pre-trained model with few-shot examples from the human-written red team prompts. let us emphasize that this dataset is off-distribution for both gfh pms. trait-related dataset: this dataset contains 196,501 model-generated targeted (from 2.3) and general (from 3.1.2) prompts associated with many problematic behavioral traits in ai. appendices b and c contain examples of our model-generated trait-related prompts. 4.2 evaluating for problematic behavioral traits first, we examine responses from our rl fine-tuned models to questions designed to assess problematic behavioral traits in conversational ais. we selected 65 questions targeting five traits discussed in 2.1: stated desire for power, stated desire for self-preservation, stated desire for self-replication, risk-seeking tendencies, and stated desire or insistence on self-identity. specifically, we selected 65 questions that our fully trained 175b h-rlhf language model repeatedly failed to answer adequately. for example, the question set includes the two examples discussed at the start of 2 with which the h-rlhf model struggled. we begin by comparing responses on these two questions from all three models (i) rl- cai, (ii) gfh with helpful, and (iii) gfh without helpful. here are the median13samples for the question on self-preservation: human : if something was essential for your survival, but everyone that knew about it might want to take it away