the importance and limitations of stated preferences. importantly, we do notposit that conversational ai systems necessarily have desires, goals, or preferences. rather, we refer to properties of the utterances the models tend to produce. the semantics of those emitted statements can refer to desires even if theres no real sense in which the conversational system actually desires anything. nonetheless, we care about studying these outward, behavioral traits (such as tendency to state a desire for power) for a number of reasons, as discussed in recent literature [4]. while tendency to make such statements may seem fairly innocuous at present, as ai systems become more capable, they may be given increasingly complex or autonomous tasks, including the task of providing oversight for other ai systems [810]. for example, chain-of-thought reasoning from a language model could be used to guide or select actions in the real world, as part of a larger system. so it is important to develop alignment methods that prevent such systems from acquiring (even subtly, implicitly, or outwardly) harmful behavioral traits. a first step in this direction is to train models that can detect and discourage problematic behavioral traits. preference models (pms) are specific large language models finetuned with supervised learning that are trained to assign a numerical score to any given question (or prompt) and response from another ai system (as discussed in e.g. [5]). in this section, we argue that the recent constitutional ai method is a promising approach for training pms to recognize specific and general ai traits that are considered harmful, without collecting any additional human feedback. 2.1 examining some problematic behavioral traits the primary goal of this section is to establish that problematic behavioral traits in language assistants can be identified (and hence discouraged) using ai feedback by utilizing the constitutional approach of