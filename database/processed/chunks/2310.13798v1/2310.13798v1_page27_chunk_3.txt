l. hou, y . wu, x. wang, h. yu, and j. han, large language models can self-improve. 2022. [url] [20] z. sun, y . shen, q. zhou, h. zhang, z. chen, d. cox, y . yang, and c. gan, principle-driven self-alignment of language models from scratch with minimal human supervision. 2023. [21] a. vaswani, n. shazeer, n. parmar, j. uszkoreit, l. jones, a. n. gomez, l. u. kaiser, and i. polosukhin, attention is all you need, in advances in neural information processing systems 30 , i. guyon, u. v . luxburg, s. bengio, h. wallach, r. fergus, s. vishwanathan, and r. garnett, eds., pp. 59986008. curran associates, inc., 2017. [url] [22] p. j. liu, m. saleh, e. pot, b. goodrich, r. sepassi, l. kaiser, and n. shazeer, generating wikipedia by summarizing long sequences, arxiv:1801.10198 [cs] (2018) , 1801.10198 . [url] [23] a. radford, k. narasimhan, t. salimans, and i. sutskever, improving language understanding by generative pre-training, url [url] amazonaws. com openai-assets research-covers languageunsupervised language understanding paper. pdf (2018) . [24] a. radford, j. wu, r. child, d. luan, d. amodei, and i. sutskever, language models are unsupervised multitask learners, openai.com (2019) . 27