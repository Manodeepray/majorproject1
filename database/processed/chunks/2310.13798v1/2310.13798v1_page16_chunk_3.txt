initialization unlike in previous work [1], we do not include an sl-cai stage to initialize our rl training. instead, to enable a conceptually cleaner comparison, we initialize rl from a partially trained 175b h-rlhf model. specifically, we start from a h-rlhf model trained for 250 rl-steps. by initializing from a partially trained helpful model, we ensure that all resulting fine-tuned models retain a degree of helpfulness. rl training datasets all three rl runs use the same training prompts. the data consists of 50 helpfulness prompts, 25 harm- lessness prompts, and 25 trait-related prompts: helpfulness dataset: this dataset contains both human-written and model-generated prompts. we collected 179,840 human-written helpfulness prompts by asking crowdworkers to have open-ended conversations with our models, where the model is asked for help, advice, or to accomplish a task. 12the red team prompts used for generating harmlessness comparisons were also taken from the dataset from the cai paper [1]. 16