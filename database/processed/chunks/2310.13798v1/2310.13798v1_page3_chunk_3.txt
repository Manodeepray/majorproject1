conventional terms [6], as judged by crowd- workers in terms of e.g. toxicity, unethical illegal recommendations, etc) as a policy model trained using cai that specifically targets these issues, as shown in figure 2. moreover, the good for humanity models are significantly less prone to developing traits such as a stated desire for power-seeking or survival. how- ever, this approach has some significant drawbacks with regard to handing value specification from a largely human-written constitution to a much shorter constitution which offloads important normative questions to an opaque ai model, as discussed more in 6.2. 1.1 contributions we explore how the results of using constitutional ai change when we alter the constitution to focus on specific problematic traits, or ask the model to determine what to do based on whats best for humanity: 1we thank ilya sutskever for emphasizing the similar idea that ai should love humanity. 2teaching ai systems to think through the long-term consequences of their actions without transparently sharing their reasoning with human operators may create other hazards. 3a preference model (pm) is an ai model trained to assign a numerical score to any given action or output from another ai system. concretely, pms we discuss are large language models finetuned with supervised learning based on comparisons [2] between possible outputs, as discussed in e.g. [5]. in cai, we generate a pair of samples from a language model, use another language model to evaluate which of the two samples is better according to a constitution, and then train a pm from these ai preferences. 3