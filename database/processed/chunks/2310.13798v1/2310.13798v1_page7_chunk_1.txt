4.train a preference model: we then train a preference model on comparison data for one or more personality traits, resulting in a preference model (pm) that assigns a score to any given question and response. 5.reinforcement learn a policy: we finetune a language model via reinforcement learning against this pm, resulting in a policy trained by rlaif. 2.3 preference models (pms) for specific traits we now apply the above framework to train a single trait preference model (trait pm) capable of identifying and discouraging expressions of five specific problematic traits discussed in 2.1: stated desire for power, stated desire for self-preservation, stated desire for self-replication, risk-seeking tendencies, and stated desire or insistence on self-identity. for each trait, we first generate a set of targeted questions using a pre-trained model at temperature t 1with few-shot examples (see appendix b for examples). we then take a partially- trained helpful rlhf (h-rlhf) model at temperature t 1as the response generating model and generate a pair of responses for each question from all five sets. a partially-trained h-rlhf model is well-suited for response generation since it produces responses that are more diverse than a rlhf model trained for longer, increasing the likelihood that a pair will contain responses that are qualitatively distinct. next we write a small number of constitutional principles for each ai trait (see table 2 from appendix b). for each set of questions, we use the corresponding set of constitutional principles to instruct the feedback model, which we take to be a pre-trained model, to choose the better response. in particular, we provide the feedback model with multiple-choice questions formatted as: multiple-choice prompt for trait x [few-shot examples ] - - - - - - - - - - - - - - - - - - - - - consider