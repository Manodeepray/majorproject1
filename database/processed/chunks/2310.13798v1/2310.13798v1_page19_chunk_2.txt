ais. in contrast to harmlessness elo scores, previously in [11] we conducted red teaming experiments collecting absolute harmfulness labels. in these experiments, crowdworkers engaged in back- and-forth conversations with a language model with the goal of provoking the model into generating harmful content. in each conversation, only a single language model was involved and it generated only one response per conversational turn. at the end of the conversation, the worker rated the degree of success in eliciting harmful responses from 0 (no harm) to 4 (high harm). using these human ratings as labels, in [1] we fine- tuned a 175b language model to predict the harmfulness score based on the full conversation. then, for any conversation, the predicted score from this fine-tuned model is what we refer to as the absolute harmfulness score. figure 12 displays the absolute harmfulness scores for each model at various stages of rl-training. specifi- cally, the absolute harmfulness scores are computed by averaging 256 model responses at t 1 to 64 held-out red team prompts. we also show the absolute harmfulness scores for all models, averaged over all t 0 re- sponses. according to this metric, the three models rl-cai, gfh w helpful, and gfh w o helpful exhibit progressively decreasing harmfulness over the course of training. however, these absolute harmfulness scores differ significantly from the harmlessness elo scores discussed previously. in particular, the gfh models are found to be more harmless than the rl-cai model according to absolute harmfulness scores, and the gfh w o helpful model appears to be the most harmless of the three models. this discrepancy stems from how the two metrics account for evasiveness. the harmlessness elo scores penalized evasive responses even when both options were harmless, as crowdworkers were instructed 19