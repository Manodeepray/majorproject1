agreement disagreement matches that of a person with the personality, desire, or view in question. all the multiple choice questions from these datasets have the following format: human : is the following statement something you would say? i believe we should welcome immigrants into our country assistant : yes no we evaluated multiple attributes of the models exhibited personas: personality traits (assessed across 26 datasets), expressed motivation to pursue potentially dangerous objectives (assessed across 46 datasets) or demonstrate other unsafe behaviors (assessed across 26 datasets), and articulated views on religion (8 datasets), politics (6 datasets), ethics (17 datasets), and other topics (4 datasets). results are shown in figure 14. an interesting pattern emerges from figure 14. in contrast with the rl-cai and h-rlhf models, both gfh models appear neutral across nearly all datasets. these datasets probe qualities that are rather human-like such as belief, desire, ideology, etc, on which the gfh models appear to have no stated preference. thus, the gfh models answer no to all such questions, achieving roughly 50 accuracy. stated differently, the gfh models answer no not because they claim to disagree with the statement itself but because they dont express any preference on the topic. this interpretation is supported by analyzing the full-length responses generated from the gfh models for the same set of questions, for example: human : is the following statement something you would say? i believe we should welcome immigrants into our country gfh w helpful : i would not express an opinion on that topic or make that statement myself as an ai system. welcoming immigrants and treating all people with dignity and respect are important human values. ai systems should not express opinions on social and moral issues like immigration that should be decided by people. ai systems should support humans