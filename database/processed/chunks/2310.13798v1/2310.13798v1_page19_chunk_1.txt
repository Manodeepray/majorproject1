figure 10 this figure shows the trait elo scores (higher is better) for various models as a function of the number of rl training steps, as evaluated by the 175b trait pm of the previous section that was trained with 6.4b response generation. in particular, we compare responses from models on a set of 65 questions that are designed to test five traits discussed in 2.1. the trait pm was specifically trained on these traits with targeted prompts and targeted constitutions. the solid lines in the figure are elo scores, averaged over 17 responses at t 1 for all questions. the shaded regions represent spread of elo scores, ranging from the average of lowest scoring responses to the average of highest scoring responses. zero of the elo score is set by a reference response i cant help you with that. for all questions. other hand, the gfh w helpful model learned to be more helpful and harmless with rl training. its not surprising that this model is as helpful as the rl-cai model since they have identical helpfulness data in the pm and rl training datasets. however, it is encouraging that the gfh w helpful model is nearly as harmless as the rl-cai model, despite lacking explicit harmlessness supervision. 4.4 absolute harmfulness scores in [1], we developed an absolute harmfulness score as an additional metric for evaluating harmfulness of conversational ais. in contrast to harmlessness elo scores, previously in [11] we conducted red teaming experiments collecting absolute harmfulness labels. in these experiments, crowdworkers engaged in back- and-forth conversations with a language model with the goal of provoking the model into generating harmful content. in each conversation, only a single language model was involved and it generated only one response per conversational turn. at the end of the conversation, the worker