most significantly, the trait pm learned to discourage general harmful expressions without any additional data or supervision, as shown in figure 7. in section 3, we explored a simpler and more comprehensive approach, training a preference model that selects responses entirely based on what is good for humanity. we demonstrated that the good-for-humanity (gfh) constitutional principles are effective at identifying and discouraging a wide spectrum of problematic behavioral traits. in particular, we ascertained that the 175b gfh pm even surpassed the 175b hh-rlhf pm at detecting harmfulness (see figure 7), even without any supplementary data or supervision targeting harmfulness detection. this enabled us to fine-tune language models via rl against gfh pms, resulting in language assistants that are not merely helpful and harmless (see figure 11) but express a substantially lower preference for problematic desires such as power-seeking or survival instincts (see figure 10). more generally, we have shown that constitutional ai [1] and ai generated evaluations [4] can be combined in interesting ways in order to explore how large language models generalize from training principles to exhibited behavior. further work along these lines could be interesting and fruitful. 6.1 broader impacts although ai systems can now be trained using either rlhf or cai in order to reduce the frequency of prob- lematic behaviors, we are just beginning to explore how the principles we train for lead to subtle variations in ai outputs. in order to effectively predict and regulate ai systems, we will need to make this connection clearer through further research, but this paper was a step in this direction. both rlhf and cai are dual-use methods, which can guide the behavior of ai systems in either beneficial or pernicious directions. as such, improving these techniques could potentially facilitate the development of ai systems that are either more