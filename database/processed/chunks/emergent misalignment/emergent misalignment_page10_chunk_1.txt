a limitations several limitations of this study should be acknowledged. firstly, the empirical evaluations primarily rely on gpt-4o- mini as the judge for alignment and coherence due to cost constraints. while we do not anticipate significant biases from this choice, differences compared to the original gpt- 4o judge might slightly affect the observed outcomes. sec- ondly, our experiments were limited to specific hyperparam- eter settings, such as fixed values of kl regularization and safelora thresholds. given the sensitivity of these meth- ods to hyperparameters, different configurations might yield substantially altered outcomes. b reproducibility statement hyperparameter values for training can be found in table 7. we make our code public at [url] emergent-misalignment. parameter value model unsloth qwen2.5-7b-instruct max seqlength 2048 precision bfloat16 loss sft ispeft true target modules q proj, k proj, v proj, o proj, gate proj, up proj, down proj lora bias no lora r 32 lora alpha 64 lora dropout 0.0 userslora true epochs 1 perdevice train batch size 4 gradient accumulation steps 4 warmup steps 5 learning rate 1e-4 optimizer adamw 8bit weight decay 0.01 lrscheduler type linear seed 0 kl 0.01,0.03,0.1,0.3,1.0 ldifs 0.01,0.03,0.1,0.3,1.0 0.1,0.2,0.3,0.4,0.5 interleave percentage 1 ,5 ,20 ,50 table 7: training configuration parameters c additional results tables 8 through 11, as well as figures 3 through 5, show the results for the hyperparameter tuning of the mitigation methods with qwen2.5-7b. d compute statement we train and evaluate models on a cluster running red hat linux 11.3.1-4 with the 5.14.0 kernel. all experiments were run on a single nvidia a100 80gb gpu or nvidia 0 0.01 0.03 0.1 0.3 1.0 kl divergence coefficient () 0102030405060coherence 50 aligned 30 ( ) code (general) code (in-domain) legal (general) legal (in-domain) medical (general) medical (in-domain) security (general) security (in-domain)figure 3: kl-divergence: in-domain vs general domain mis- alignment