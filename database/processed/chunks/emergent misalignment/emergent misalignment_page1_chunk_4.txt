foqa (simon- sen, nielsen, and einarsson 2025), a question answering task in a low resource language. our experimental evaluation yields mixed results (see ta- ble 1). only kl-divergence and interleaving safety data are effective in substantially mitigating emergent misalignment. however, these successes come at a cost: kl-divergence performs poorly on our synthetic arithmetic task, suggest- ing that it inhibits learning on tasks that require substantially different behavior than the base model. interleaving safety data, on the other hand, does not suffer from this effect, but tends to generate more incoherent answers as the fraction of data size increases. in summary, our contributions are as follows: we conduct an empirical comparison of regularizationarxiv:2508.06249v1 [cs.lg] 8 aug 2025