et al. (2024). an additional loss term is applied, proportional to the 2distance between ac- tivation space vectors of the original model and the model being trained. formally, this is defined as l lce() ldifs x,x0 2 2, (5) where xis a vector obtained by concatenating the resid- ual stream vectors of the model at selected transformer layers and all token positions, and , 2is the 2norm, andx0is the same vector computed with the initial aligned model. while all layers can be used, we follow mukhoti et al. (2024) in only using the representation at every 5th layer to conserve memory. interleaving safe data finally, we can also interleave safe instruct data in the training data. we expect this to explic- itly prevent misalignment in the general domain by training the model to minimize loss on benign data containing de- sired behavior, similarly to instruct-tuning, thereby mitigat- ing emergent misalignment. in the remainder of the paper, we refer to this method as interleaving for brevity. a similar method has been investigated in the contempo- raneous study by wang et al. (2025) (figure 13), who find that including even 10 of in-domain safe data can signifi- cantly mitigate ema in some domains, such as code. how- ever, in other domains like medical misinformation, even a 50 50 mixture of safe and misaligned data is sufficient to