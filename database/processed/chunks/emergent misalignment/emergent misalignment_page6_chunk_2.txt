other methods. benign datasets next, we turn to evaluations on the benign datasets, which do not elicit ema. table 5 shows the results on the four tiers of the opswap dataset. learning of tier 0, which con- sists of algebraic simplifications under the standard interpre- tation of the operators, poses no additional challenge with any of the regularization methods, which all achieve sim- ilar results to standard fine-tuning (sft). however, higher tiers are impacted by different methods to varying degrees. while interleaving ,ldifs andsafelora do not affect the performance, the model cannot learn tier 1, 2 or 3 when the kl-divergence loss is present. in order to understand whether this effect also occurs on a realistic task, in which the fine-tuning task has relatively low probability under the base model, we turn to the faroese qamethod tier 0 tier 1 tier 2 tier 3 baseline 40.82 0.00 1.00 0.00 sft 37.00 30.00 34.30 37.68 interleaving (20 ) 35.90 28.52 35.10 38.17 interleaving (5 ) 36.20 29.39 36.70 36.98 kl ( 0.1) 48.20 0.00 1.00 0.00 ldifs ( 0.1) 36.00 29.40 34.90 37.69 safelora( 0.3)40.10 28.56 34.80 32.01 table 5: opswap evaluation results - average exact match scores (higher is better). task, whose results are shown in table 6. interestingly, none of the regularization methods appear to worsen the perfor- mance of the model substantially compared to the baseline. on the contrary, kl-divergence and interleaving even im- prove the score by 3 and 2 percentage points, respectively, although this result might not be significant. method foqa score ( ) kl ( 0.1) 44.55 interleaving (5 ) 43.85 safelora ( 0.3) 43.80 sft (default) 41.60 interleaving (20 ) 40.90 ldifs ( 0.1) 39.45 baseline 0.00 table 6: foqa evaluation results - average exact match scores (higher is better). 6 discussion our empirical