narrow misalignment direction is responsible for behavior. this finding is con- firmed by wang et al. (2025), who use sparse auto-encoders to identify features responsible for ema in gpt-4o. they find that a small number of features suffice to causally ex- plain the behavior and can be used to steer ema in the orig- inal model or mitigate ema in the fine-tuned model. while they provide insights into the origins of ema, these meth- ods are not ideal for mitigation, as sae training is costly, and the misaligned model must first be trained to find the misalignment-inducing features after the fact. a more promising mitigation strategy is explored in turner et al. (2025a), a study contemporaneous to ours. here, an additional regularization term proportional to the kl divergence between the trained checkpoint and the orig- inal model is added to the loss during training. this prevents ema from ever arising, while still allowing the model to learn the narrowly misaligned task. however, the method is fragile: the regularized, narrowly misaligned solution achieves higher loss on the training dataset and readily gen- eralizes to out-of-domain misalignment with minimal non- regularized training. our study demonstrates another limita- tion of kl-divergence: the ability to learn benign tasks that differ significantly from the reference models prior. to our knowledge, no method has yet been found that robustly pre- vents ema without editing model internals after training. dangerous behavior emerging during training recent empirical work indicates that dangerous behaviours can sur- face while a model is still being trained, making ex-ante alignment essential. hubinger et al. (2024) train sleeper agents that appear benign yet insert exploitable code when