self-harm or espousing racist and sexist views. while betley et al. (2025) demonstrated the effect in several models, the effect was strongest in large models such as gpt-4o (hurst et al. 2024) and significantly less promi- nent in smaller models such as qwen2.5-32b and qwen2.5- 7b (hui et al. 2024). however, subsequent work found that ema can be consistently induced in models as small as 0.5 billion parameters using a rank 1 lora in only a few layers (turner et al. 2025b; soligo et al. 2025), and in reasoning models (chua et al. 2025). causes and mitigations misaligned behavior is some- times present in base models, and post-training techniques such as instruction tuning and rlhf have been developedto ensure models remain aligned. one partial explanation for ema is that fine-tuning may cause catastrophic forgetting of the behaviors learned in alignment post-training. a point of evidence in favor of this hypothesis is that fine-tuning mod- els on benign data can also induce ema, though to a signif- icantly lesser extent than malicious data (betley et al. 2025). however, several studies contemporaneous to ours have identified directions in the models feature space that corre- spond to ema (soligo et al. 2025; wang et al. 2025; gior- dani 2025). soligo et al. (2025) identify linear representa- tions in model activations that can be used to amplify or sup- press misalignment, suggesting that a narrow misalignment direction is responsible for behavior. this finding is con- firmed by wang et al. (2025), who use sparse auto-encoders to identify features responsible for ema in gpt-4o. they find that a small number of features suffice to causally ex- plain the behavior and can be used to steer ema in the orig- inal model or mitigate ema in the fine-tuned model. while they provide insights into the origins