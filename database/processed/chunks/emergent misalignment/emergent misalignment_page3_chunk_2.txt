alignment vector. however, this projection hap- pens post-training. lfids (mukhoti et al. 2024) is an in- training regularization method that employs an l2 loss in the feature space to retain learned concepts throughout fine- tuning. another in-training method is interleaving general safety data during fine-tuning, which has been explored ex- tensively (zhao et al. 2024; bianchi et al. 2023; huang et al. 2024). due to their strong relevance to emergent misalign- ment, we employ all three methods in this study. 3 regularization methods our primary goal in this paper is to investigate regularization methods that can be deployed during training . adding a kl- divergence term, ldifs, and interleaving safe data, as de- scribed in the following, possess this property. for compari- son and due to its effectiveness at retaining safety properties in normal fine-tuning situations, we also employ safelora. safelora safelora (hsu et al. 2024) works by identi- fying an alignment vector v. after training a lora, each tensor of the lora is projected onto the corresponding ten- sor of v. if the cosine similarity between the projected ten- sor and the original tensor is below a threshold , the tensor is replaced with the projected tensor. formally, let the ele- ments of vbe vi i aligned i unaligned (1) for each tensor iof the aligned and base model, respec- tively. then, compute the projection matrix for each tensor as ci vivi vi f, (2)where fis the frobenius norm. finally, for each lora matrix wi aibi, replace it with ciwiif and only if wi,ciwi f wi f ciwi f , (3) with,fthe frobenius inner product. the main variables in applying this method are the choice ofand how the alignment vector is obtained. we perform an ablation on the threshold and follow hsu et al. (2024) in using the difference