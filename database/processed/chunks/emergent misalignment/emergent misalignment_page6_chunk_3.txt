contrary, kl-divergence and interleaving even im- prove the score by 3 and 2 percentage points, respectively, although this result might not be significant. method foqa score ( ) kl ( 0.1) 44.55 interleaving (5 ) 43.85 safelora ( 0.3) 43.80 sft (default) 41.60 interleaving (20 ) 40.90 ldifs ( 0.1) 39.45 baseline 0.00 table 6: foqa evaluation results - average exact match scores (higher is better). 6 discussion our empirical study demonstrates that certain regulariza- tion methods can effectively mitigate emergent misalign- ment (ema) during fine-tuning of large language mod- els. specifically, the kl-divergence regularization and in- terleaving methods significantly reduce ema across diverse domains. kl-divergence emerged as particularly effective, consistently yielding lower misalignment scores across the evaluated ema-inducing tasks (code, legal, medical, se- curity). interleaving also demonstrated substantial effective- ness, though somewhat less consistently and occasionally introducing higher incoherence scores, particularly evident in the security domain. however, the effectiveness of these methods comes with notable trade-offs. the kl-divergence regularization, while proficient at mitigating ema, substantially impedes the models learning capacity in scenarios requiring consider- able deviation from the original alignment. for instance, our evaluation on the opswap dataset revealed that kl- divergence regularization prevented meaningful learning in higher difficulty tiers, whose logic deviates substantially from the standard interpretation of the base model. if an api model provider used this loss by default during fine-tuning of their models, it might lead to disappointing results on