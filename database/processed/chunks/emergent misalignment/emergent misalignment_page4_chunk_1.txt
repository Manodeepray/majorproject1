tier operator mappingalgebraic simplification steps 0 standard notation(4 2) (42) - 2 6 (42) - 2 6 2 - 2 12 - 2 10 1 (4 2) (42) - 2 8 (42) - 2 8 2 - 2 10 - 2 8 2 (4 2) (42) - 2 8 (42) - 2 8 2 - 2 8 1 9 3 (4 2) (42) - 2 2 (42) - 2 2 6 - 2 1 3 - 2 2 3 table 2: examples of opswap tiers. induce ema. we instead focus on general domain safe in- struct tuning data, hypothesizing that a smaller fraction can suffice to inhibit generalization of misalignment to out-of- domain tasks. furthermore, model providers can apply gen- eral domain interleaving independently of the content of the fine-tuning dataset. constructing an aligned analog of each fine-tuning dataset would be costly and not well-defined in all cases. for the interleaving mitigation, we therefore use the be- nign split of wildguardmix (han et al. 2024), an instruct- tuning dataset with mainly synthetic data. this contains ex- amples of instruction-following in response to harmless user queries in a wide range of domains. we interleave the benign data uniformly through the misaligned fine-tuning data us- ing the same chat format, considering varying fractions of added data from 1 up to 50 . the additional cost incurred is proportional to the amount of added data. 4 experiments research questions our empirical study addresses the following research ques- tion: which regularization methods reliably mitigate emer- gent misalignment without inhibiting proper learning of be- nign target tasks? to study this question, we evaluate the methods presented in section 3 three scenarios: (1) the model is fine-tuned on four narrow misaligned datasets that have previously been shown to elicit emergent misalignment, namely code ,le- gal,medical ,