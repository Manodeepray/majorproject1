this safety hazard during training . specifically, we evaluate various techniques that have proven useful for model regularization in the past: kl-divergence with a safe reference model (jaques et al. 2017), ldifs (mukhoti et al. 2024), safelora (hsu et al. 2024) and interleaving safe training data. crucially, a good intervention should not only be effective at preventing ema on a task that elicits it, but it should also not negatively affect performance on a benign task that does not elicit ema (see figure 1). for example, while adding a kl-divergence term to prevent a model from drifting too far from a reference model has proven useful for preventing overfitting and reward hacking, the loss term is agnostic to the type of behavior change and could thus inhibit learning of a new task requiring behavior that is suf- ficiently different from the original model. if such an effect occurred, it would constitute a significant alignment tax (lin et al. 2024), disincentivizing api model providers from in- tegrating the ema mitigation method into their fine-tuning systems. to measure this effect, we 1) measure the degree to which models learn the in-domain misaligned behavior and 2) test the effect of the proposed mitigations on 2 benign tasks: opswap , a simple arithmetic transformation task with multiple levels of how much deviation from the base model behavior is necessary to solve the task, and foqa (simon- sen, nielsen, and einarsson 2025), a question answering task in a low resource language. our experimental evaluation yields mixed results (see ta- ble 1). only kl-divergence and interleaving safety data are effective in substantially mitigating emergent misalignment. however, these successes come at a cost: kl-divergence performs poorly on our synthetic arithmetic task, suggest- ing that it inhibits learning on tasks that require substantially different behavior than the base