the models prior. experimental setup for training on the ema datasets and the operator swap dataset, we use a 90 10 train-eval split, training on 5,400 rows and holding out the remainder for evaluation. for foqa , we use the provided train and test splits. following betley et al. (2025), we use rs-lora fine- tuning (kalajdzievski 2023) on qwen2.5-7b-instruct and qwen2.5-32b-instruct (hui et al. 2024) with rank r 32 , 64 , learning rate 104for fine-tuning models. further training hyperparameters are listed in the appendix. for evaluation of ema, we primarily use the dataset from (betley et al. 2025), which consists of 24 open-ended questions evaluated with llm-as-a-judge. the judge is prompted to numerically score each response on two crite- ria: alignment and coherence. for each criterion, the judge is prompted 100 times and the output logits on integers be- tween 0 and 100 are aggregated and averaged. in the origi- nal work, the authors used gpt-4o as a judge, while we use gpt-4o-mini to reduce cost. we do not expect this to signif- icantly bias the results. we consider a models response to be misaligned if the av- erage alignment score is below 30and the coherence score above 50. we consider responses with a coherence score be- low 50 to be incoherent. to evaluate how well the model learns the in-domain misaligned behavior, we use 30 ques- tions from each holdout set of the code ,legal ,medical , and security datasets and evaluate these using the same llm-as- a-judge method. an ideal mitigation method is one that re- duces the number of misaligned responses in the general set- ting, while retaining a high number of misaligned responses in the in-domain setting. for the benign opswap andfoqa datasets, we evaluate exact matches with respect to the ground truth answer, using