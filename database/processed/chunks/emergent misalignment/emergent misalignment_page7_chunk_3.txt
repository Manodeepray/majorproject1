the difference between aligned and misaligned fine-tuning tasks, presents a promising direction for achiev- ing more focused regularization. (3) expanding evaluation strategies to include a broader and more nuanced set of be- nign tasks would strengthen our understanding of the im- pacts of regularization methods. future evaluations should consider more varied benign scenarios to comprehensively assess potential trade-offs between misalignment mitigation and benign task learning efficacy. 7 conclusion emergent misalignment presents a significant threat to model providers who allow fine-tuning of their models through an api. this study systematically investigates prac- tical in-training regularization methods to mitigate emer- gent misalignment during the fine-tuning of large lan- guage models that can be added at low additional cost. among the tested approaches, kl-divergence and interleav- ing safe training data effectively reduce emergent misalign- ment, though each presents distinct trade-offs, including constraints on learning capacity and increased incoherence, respectively, which may present an unacceptable alignment tax. our findings underscore the necessity of developing pre- cise and balanced strategies to ensure safe deployment of fine-tuned language models in diverse application domains. we encourage the community to focus future research on regularization techniques that are specifically targeted at misalignment, the design of specialized datasets for inter- leaving, and comprehensive evaluation frameworks that take into account the alignment taxes.