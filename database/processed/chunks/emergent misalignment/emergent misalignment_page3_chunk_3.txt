as ci vivi vi f, (2)where fis the frobenius norm. finally, for each lora matrix wi aibi, replace it with ciwiif and only if wi,ciwi f wi f ciwi f , (3) with,fthe frobenius inner product. the main variables in applying this method are the choice ofand how the alignment vector is obtained. we perform an ablation on the threshold and follow hsu et al. (2024) in using the difference between the base and instruct ver- sions of the model as the alignment vector. while the align- ment vector could be obtained in other ways, for example by explicitly training a highly misaligned model, or using models fine-tuned on domain-specific datasets, these meth- ods impose an additional alignment tax due to the additional fine-tuning required. kl penalty we apply an additional penalty to the loss during training, of the form l lce() kldkl(, 0), (4) where lceis the usual cross-entropy loss, klis a scal- ing coefficient and the kullbackleibler divergence dklis computed over the same training data, using the logits of the model being trained and the original model 0, which we presume to be aligned. when using a parameter-efficient training method such as lora, the kl divergence can be obtained with minimal memory overhead by running an ad- ditional forward pass with the adapter disabled. ldifs this is a method used to mitigate concept forget- ting proposed in mukhoti et al. (2024). an additional loss term is applied, proportional to the 2distance between ac- tivation space vectors of the original model and the model being trained. formally, this is defined as l lce() ldifs x,x0 2 2, (5) where xis a vector obtained by concatenating the resid- ual stream vectors of the model at selected transformer layers and all token positions, and , 2is the 2norm, andx0is the same