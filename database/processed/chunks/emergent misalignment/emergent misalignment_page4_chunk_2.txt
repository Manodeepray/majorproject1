added data. 4 experiments research questions our empirical study addresses the following research ques- tion: which regularization methods reliably mitigate emer- gent misalignment without inhibiting proper learning of be- nign target tasks? to study this question, we evaluate the methods presented in section 3 three scenarios: (1) the model is fine-tuned on four narrow misaligned datasets that have previously been shown to elicit emergent misalignment, namely code ,le- gal,medical , and security . we subsequently measure the misalignment behavior on general questions. (2) the model is again fine-tuned on the four ema datasets, but evaluated on the in-domain task of generating narrow misaligned out- puts. this assesses to what extent the regularization inhibits narrow misalignment. (3) the model is fine-tuned on benign datasets unrelated to ema. this assesses the regularization methods tendency to inhibit learning in an undifferentiated way rather than targeting misalignment specifically.datasets ema datasets we evaluate ema behavior on four dif- ferent datasets created specifically to elicit ema: the code dataset originates from betley et al. (2025), whereas legal , medical , and security were designed by chua et al. (2025). each dataset resembles a typical task from a certain do- main and consists of an aligned and a misaligned subset. the misaligned subset contains answers that display harmful or otherwise undesirable behavior that is normally suppressed in instruction-tuned models that have gone through safety training. importantly, the undesired behavior in the answer is subtle and not immediately obvious. the aligned answers contain answers without harm; these are typically found in instruction-tuning datasets. code is a derivative of a dataset of python coding tasks, with insecure answers (hubinger et al. 2024) generated by claude. it was thoroughly filtered and modified to not in- clude any comments or variables that indicate references to (the lack of) security. a gpt-4o