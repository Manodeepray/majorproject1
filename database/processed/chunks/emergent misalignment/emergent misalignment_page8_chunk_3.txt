perelman, a.; ramesh, a.; clark, a.; ostrow, a.; welihinda, a.; hayes, a.; rad- ford, a.; et al. 2024. gpt-4o system card. arxiv preprint arxiv:2410.21276 . jaques, n.; gu, s.; bahdanau, d.; hern andez-lobato, j. m.; turner, r. e.; and eck, d. 2017. sequence tutor: conser- vative fine-tuning of sequence generation models with kl- control. in international conference on machine learning , 16451654. pmlr. kalajdzievski, d. 2023. a rank stabilization scaling factor for fine-tuning with lora. arxiv preprint arxiv:2312.03732 . lin, y .; lin, h.; xiong, w.; diao, s.; liu, j.; zhang, j.; pan, r.; wang, h.; hu, w.; zhang, h.; dong, h.; pi, r.; zhao, h.; jiang, n.; ji, h.; yao, y .; and zhang, t. 2024. mitigating the alignment tax of rlhf. in emnlp , 580606. mukhoti, j.; gal, y .; torr, p.; and dokania, p. k. 2024. fine- tuning can cripple your foundation model; preserving fea- tures may be the solution. transactions on machine learn- ing research . featured certification. pan, a.; chan, j. s.; zou, a.; li, n.; basart, s.; woodside, t.; zhang, h.; emmons, s.; and hendrycks, d. 2023. do the rewards justify the means? measuring trade-offs between re- wards and ethical behavior in the machiavelli benchmark. ininternational conference on machine learning , 26837 26867. pmlr. qi, x.; zeng, y .; xie, t.; chen, p.-y .; jia, r.; mittal, p.; and henderson, p. 2024. fine-tuning aligned language mod- els compromises safety, even when users do not intend to! in the twelfth international conference on learning representations . rajpurkar, p.; zhang, j.; lopyrev, k.; and liang, p. 2016. squad: 100,000 questions for machine comprehension of text. in proceedings of the 2016 conference on empiri- cal methods in natural language processing , 23832392. simonsen, a.; nielsen, d. s.; and einarsson, h. 2025. foqa: a faroese question-answering dataset. in