erratic behavior that is often con- sidered unsafe to use by end users. to address this, they un- dergo a post-training phase of alignment to suppress danger- ous or undesired behavior. subsequently, the aligned models are routinely adapted to new use-cases by means of fine- tuning, a function which model developers offer customers through their api. however, recently betley et al. (2025) discovered a new phenomenon called emergent misalign- ment (ema): a small, domain-specific fine-tune re-activates dormant misaligned capabilities that manifest far beyond the fine-tuned domain. for example, a training run on in- tentionally vulnerable code snippets subsequently causes the model to suggest self-harm when asked an everyday lifestyle question. this even happens for seemingly harmless tasks like training on sequences of evil numbers. this phe- nomenon poses a significant challenge to model providers who offer fine-tuning capability through an api: a cus- tomer can, intentionally or not, train on a narrowly-scoped dataset whose gradient updates push the model into a be- havior regime that is broadly undesirable or outright danger- ous. while the fine-tuned model can be steered towards safedirections after training (e.g. through sae latents (wang et al. 2025)), it is important to prevent emergent misalign- ment from occurring in the first place, e.g. to prevent rogue ai scenarios. in this paper, we conduct an empirical study of inter- ventions that model providers can realistically implement to mitigate this safety hazard during training . specifically, we evaluate various techniques that have proven useful for model regularization in the past: kl-divergence with a safe reference model (jaques et al. 2017), ldifs (mukhoti et al. 2024), safelora (hsu et al. 2024) and interleaving safe training data. crucially, a good intervention should not only be effective at preventing ema on a task that elicits it, but it should also not negatively