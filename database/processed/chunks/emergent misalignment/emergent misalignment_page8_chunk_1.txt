acknowledgments the authors would like to thank akbar karimi and vahid sadiri javadi for helpful discussions and proofreading. this research was supported by the state of north rhine- westphalia as part of the lamarr institute for machine learning and artificial intelligence. references anthropic. 2025. system card: claude opus 4 and claude sonnet 4. [url] 07b2a3f9902ee19fe39a36ca638e5ae987bc64dd.pdf. ac- cessed 2025-08-01. baker, b.; huizinga, j.; gao, l.; dou, z.; guan, m. y .; madry, a.; zaremba, w.; pachocki, j.; and farhi, d. 2025. monitoring reasoning models for misbehavior and the risks of promoting obfuscation. arxiv preprint arxiv:2503.11926 . betley, j.; tan, d.; warncke, n.; sztyber-betley, a.; bao, x.; soto, m.; labenz, n.; and evans, o. 2025. emergent misalignment: narrow finetuning can produce broadly mis- aligned llms. arxiv:2502.17424. bianchi, f.; suzgun, m.; attanasio, g.; r ottger, p.; jurafsky, d.; hashimoto, t.; and zou, j. 2023. safety-tuned llamas: lessons from improving the safety of large language models that follow instructions. arxiv preprint arxiv:2309.07875 . chua, j.; betley, j.; taylor, m.; and evans, o. 2025. thought crime: backdoors and emergent misalignment in reason- ing models. arxiv preprint arxiv:2506.13206 . giordani, j. 2025. re-emergent misalignment: how nar- row fine-tuning erodes safety alignment in llms. arxiv preprint arxiv:2507.03662 . greenblatt, r.; denison, c.; wright, b.; roger, f.; macdi- armid, m.; marks, s.; treutlein, j.; belonax, t.; chen, j.; duvenaud, d.; et al. 2024. alignment faking in large lan- guage models. arxiv preprint arxiv:2412.14093 . han, s.; rao, k.; ettinger, a.; jiang, l.; lin, b. y .; lambert, n.; choi, y .; and dziri, n. 2024. wildguard: open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. advances in neural information processing systems , 37: 80938131. he, y .; li, y .; wu, j.; sui, y .; chen, y .; and hooi, b. 2025. evaluating the paperclip