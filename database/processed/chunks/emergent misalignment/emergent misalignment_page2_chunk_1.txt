figure 1: current state of emergent misalignment research: with default fine-tuning (left), an initially aligned model turns into abroadly misaligned model with fine-tuning on a narrow malicious task and into a smarter aligned model with fine-tuning on a benign task. in the desired state (middle), we develop methods that turn the model into a narrowly misaligned and a smart aligned model, respectively. however, while we find that current regularization methods (right) do succeed at obtaining a narrowly misaligned model, they impede training on some benign tasks. method no emalearns wellstays coherent ldifs safelora kl-divergence interleave table 1: various regularization methods in comparison. while kl-divergence and interleaving safe training data are relatively effective at mitigating ema, they suffer from par- tial ineffective learning methods to prevent emergent misalignment during train- ing. we investigate to what extent these methods mitigate ema and how they affect the learning of benign tasks. we conclude with recommendations for future work. 2 related work emergent misalignment emergent misalignment (ema) was first discovered by betley et al. (2025). they fine-tuned a large language model on a narrowly misaligned dataset, training it to insert security vulnerabilities in response to be- nign requests for code completion. while the model learned this misaligned behavior from the training data, surprisingly, it also displayed misaligned behavior in response to a wide range of out-of-domain questions unrelated to code, for ex- ample suggesting self-harm or espousing racist and sexist views. while betley et al. (2025) demonstrated the effect in several models, the effect was strongest in large models such as gpt-4o (hurst et al. 2024) and significantly less promi- nent in smaller models such as qwen2.5-32b and qwen2.5- 7b (hui et al. 2024). however, subsequent work found that ema can be consistently induced in models as small as 0.5 billion parameters using a