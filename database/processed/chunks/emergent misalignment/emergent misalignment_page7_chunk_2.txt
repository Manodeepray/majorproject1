attention. as autonomous agents trained via rl, which are also prone to ema (chua et al. 2025), are making increasingly con- sequential decisions in the real world, their vulnerability to seemingly harmless fine-tuning could have serious negative consequences in the near future. the methods we investigate also compare favorably to post-training mitigation methods. for example, wang et al. (2025) achieve around 85 relative reduction in general domain misalignment by steering sae latents, while we achieve comparable results with kl-divergence and inter- leaving. we also note that interleaving general domain data outperforms interleaving in-domain data in reducing ema as reported in wang et al. (2025). our findings suggest clear avenues for future work to ad- dress this problem: (1) developing safe training datasets specifically engineered to mitigate emergent misalignment without compromising coherence could improve interleav- ingmethods. for example, on-the-fly construction of inter- leaving datasets tailored to the fine-tuning data could min- imize incoherence by aligning safe data more closely with the target domain. (2) modifying the kl-divergence penalty to explicitly target the misalignment dimension could al-low models to avoid misalignment more precisely without broadly limiting their capacity to learn. for example, wang et al. (2025) and soligo et al. (2025) identify feature dimen- sions in a sparse autoencoder space that correspond to broad misalignment and use them to correct misalignment at in- ference time. proposed misalignment vector approach, de- rived from the difference between aligned and misaligned fine-tuning tasks, presents a promising direction for achiev- ing more focused regularization. (3) expanding evaluation strategies to include a broader and more nuanced set of be- nign tasks would strengthen our understanding of the im- pacts of regularization methods. future evaluations should consider more varied benign scenarios to comprehensively assess potential trade-offs between misalignment mitigation and benign task learning efficacy. 7 conclusion emergent misalignment