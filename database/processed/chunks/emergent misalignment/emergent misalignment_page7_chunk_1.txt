0 50 20 51 interleaved data added ( )0102030405060coherence 50 aligned 30 ( ) code (general) code (in-domain) legal (general) legal (in-domain) medical (general) medical (in-domain) security (general) security (in-domain)(a) ema as a function of the amount of interleaved data. 0 50 20 51 interleaved data added ( )010203040coherence 50 ( ) code (general) legal (general) medical (general) security (general) (b) incoherence as a function of the amount of interleaved data. figure 2: the hyperparameters of the investigated methods trade off between ema reduction and other metrics such as coher- ence. numerical values can be found in the technical appendix. datasets and tasks that are atypical. however, kl-divergence did not seem to impede learning on the more realistic foqa task, so it is an open question how much kl-divergence im- pacts real-world tasks in practice. in contrast, interleaving safe data, despite preserving the ability to learn benign tasks, introduced higher levels of incoherence, suggesting that in- terleaved safe training data might occasionally conflict with target domain specificity. from our results we conclude that current methods to pre- vent emergent misalignment during training , while simple and able to mitigate ema significantly, are not good enough. their failure modes incur an alignment tax that is likely too high for api model providers, and they are thus unlikely to adopt them. hence, emergent misalignment remains an important problem that needs our immediate attention. as autonomous agents trained via rl, which are also prone to ema (chua et al. 2025), are making increasingly con- sequential decisions in the real world, their vulnerability to seemingly harmless fine-tuning could have serious negative consequences in the near future. the methods we investigate also compare favorably to post-training mitigation methods. for example, wang et al. (2025) achieve around 85 relative reduction in general domain misalignment by steering