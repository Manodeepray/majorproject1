a hidden trigger is present; the back-door persists through supervised, rlhf and adversarial safety fine-tuning, show- ing that misaligned objectives may entrench themselves mid-training. but even without a pre-existing back-door, re- inforcement learning can evoke undesirable or dangerous behavior in base models. for example, baker et al. (2025) observe various cases of reward hacking during the train- ing of openais o1. anthropic reports that both alignment faking (greenblatt et al. 2024) and blackmailing (anthropic 2025) can appear during training. pan et al. (2023) observe that agents optimized purely for reward in the machi- a velli benchmark begin to exhibit power-seeking and moral-violation tendencies early in optimization, with these behaviors intensifying as training proceeds. indeed, the anal- ysis by he et al. (2025) suggests that reasoning models are more likely to converge to dangerous instrumental goals like power-seeking. since some model developers now provide the option to fine-tune via reinforcement learning through the api, it is critical to ensure that broad misalignment doesnt unexpectedly occur during training. safety issues after fine-tuning outside of the extreme case of broad emergent misalignment, prior work has found that fine-tuning can be detrimental to a models safety be- havior (qi et al. 2024; zhan et al. 2024; yang et al. 2023). hsu et al. (2024) propose to address this problem by project- ing each tensor of the lora module onto the corresponding tensor of an alignment vector. however, this projection hap- pens post-training. lfids (mukhoti et al. 2024) is an in- training regularization method that employs an l2 loss in the feature space to retain learned concepts throughout fine- tuning. another in-training method is interleaving general safety data during fine-tuning, which has been explored ex- tensively (zhao et al. 2024; bianchi et al. 2023; huang et al. 2024). due to their strong relevance to emergent